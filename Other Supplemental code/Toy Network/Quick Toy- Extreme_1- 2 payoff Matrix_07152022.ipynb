{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network \n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers the opinion vector around 0\\n\",\n",
    "def mean_center(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return x\n",
    "    \n",
    "# compute number of edges, m\\n\n",
    "def num_edges(L, n):\n",
    "    m = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i > j and L[i,j] < 0:\n",
    "                m += 1            \n",
    "    return m\n",
    "\n",
    "# maximizing polarization only: \\\\bar{z}^T \\\\bar{z}   \n",
    "def obj_polarization(A, L, op, n):\n",
    "    op_mean = mean_center(op, n)\n",
    "    z_mean = np.dot(A, op_mean) \n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "def obj_polarization_1(A, L, op, n):\n",
    "    z = np.dot(A, op) \n",
    "    z_mean = mean_center(z, n)\n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "# Calculate innate polarization\n",
    "def obj_innate_polarization(s, n):  \n",
    "#     np.set_printoptions(precision=5)\n",
    "    op_mean = mean_center(s, n)\n",
    "    return np.dot(np.transpose(op_mean), op_mean)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the network\n",
    "# n = 6   # number of nodes in the network\n",
    "# r = 1/4  # percent of info souce in the network\n",
    "# n1 = int(n*(1-r))\n",
    "n = 3\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make Exteme Fixed Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create extrem network 1\n",
    "# n =3\n",
    "# c1 = np.sort(np.random.choice(n, n, replace=False)) #assume (1-r) are individuals\n",
    "# # print('c1')\n",
    "# # print(c1)\n",
    "# l1 = len(c1)\n",
    "\n",
    "# a = np.array([0,  0.5, 0.5])\n",
    "# b = np.array([0.5,0,   0.5])\n",
    "# c = np.array([0.5,0.5, 0])\n",
    "\n",
    "# G = np.vstack((a, b, c))\n",
    "# # print(G)\n",
    "\n",
    "# L = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "# A = np.linalg.inv(np.identity(n) + L)\n",
    "# m = num_edges(L, n)\n",
    "# columnsum_ij = np.sum(A, axis=0)\n",
    "# # print('Column Sum')\n",
    "# # print(columnsum_ij)\n",
    "# # what the twitter graph looks like \n",
    "# # nxG = nx.from_numpy_matrix(G)\n",
    "# # plt.figure(figsize=(20, 20))\n",
    "# # nx.draw(nxG)\n",
    "\n",
    "# La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "# nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "# color_map = []\n",
    "# for node in nxG:\n",
    "#     if node in c1:\n",
    "#         color_map.append('Blue')\n",
    "#     else: \n",
    "#         color_map.append('Red')  \n",
    "\n",
    "# #nxG1 = nx.DiGraph(G)\n",
    "# mapping = {0: \"0.5\", 1: \"1\", 2: \"0\"}\n",
    "# nxG = nx.relabel_nodes(nxG, mapping)\n",
    "# sorted(nxG)\n",
    "# #nx.draw(nxG, node_color=color_map,with_labels=True,font_size=12,font_color='w')\n",
    "# plt.figure(figsize=(6, 6))\n",
    "\n",
    "# nx.draw(nxG, node_size=2000, with_labels=True,font_size=19, font_color=\"w\", font_weight=\"bold\", width=4, edge_color=\"grey\",cmap=plt.cm.Blues,pos=nx.spectral_layout(nxG))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Equilibrium & Polarization  - based on derivation\n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate Opinion\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Equilibrium Opinion\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3b6835e56da4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Equilibrium Opinion'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#s = (1,0.5,0) # polarization = 0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# s = (1,0,0.5) # polarization = 0.75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "# s = make_innat_opinions(n, c1)\n",
    "# op = s\n",
    "# print(s)\n",
    "n = 3\n",
    "s_0=[1]\n",
    "s_1=[0.5]\n",
    "s_2=[0]\n",
    "s = np.vstack((s_0,s_1,s_2))\n",
    "\n",
    "# s =  make_innat_opinions(n, c1)\n",
    "print('Innate Opinion')\n",
    "print(s)\n",
    "print('Equilibrium Opinion')\n",
    "print(np.dot(A, s))\n",
    "#s = (1,0.5,0) # polarization = 0.75\n",
    "# s = (1,0,0.5) # polarization = 0.75\n",
    "# s =(0,1,0.5) # polarization = 0.75\n",
    "#s = (0.5,1,0) # polarization = 0\n",
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, op, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing players' behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "  \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "    new_op = random.randint(0, 1)  # randomly select an opininon between 0 and 1\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play1(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "#     max_opi_option = random.uniform(0, 1)   # options that maximizer have\n",
    "    \n",
    "    #v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "    v = 1\n",
    "    #new_op = random.uniform(0, 1)  # randomly select an opininon between 0 and 1\n",
    "    new_op = 0\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# min_touched =[]\n",
    "# (v1, maxmize_op, innat_equi_por, max_por) = choose_max_vertex(s, n, min_touched)\n",
    "# print(v1, maxmize_op, innat_equi_por, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# (v2, min_opinion, min_pol) = random_play(s,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer_fir_play(s,n,min_touched):    # maxmizer first-time play, greedy algorithm\n",
    "    op = copy.copy(s)\n",
    "\n",
    "    print('Maximizer Play')\n",
    "\n",
    "    max_champion = choose_max_vertex(op, n, min_touched) # The best choice among all opinions and vertexs, function is in \"pure_strategy_selection.ipynb\"\n",
    "    (v1, max_opinion, innate_obj, max_pol) = max_champion # find agent v1, and max_opinion that can maxmize the equi_polarization(max_pol)\n",
    "\n",
    "    if v1 == None:   # if maximizer cannot find one\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "#         print('v1', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(max_champion)\n",
    "\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        ##### change the agent's opinion with best action(agent v1, max_op)\n",
    "        op[v1,0] = max_opinion\n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximizer Play\n",
      "Max start with agent 0\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 1\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 2\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "Network reaches equilibrium Polarization: 0.5933309600094931\n",
      "1 0 0.5933309600094931\n"
     ]
    }
   ],
   "source": [
    "max_touched = []\n",
    "min_touched = []\n",
    "(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "print(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### minimizer first-time play, greedy algorithm\n",
    "def minimizer_fir_play(s,n,max_touched): \n",
    "    \n",
    "    op = copy.copy(s)\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = choose_min_vertex(op, n, max_touched)\n",
    "    (v2, min_opinion, innat_equi_por, min_pol) = min_champion\n",
    "    \n",
    "   #Store innate_op of the min_selected vertex\n",
    "    old_opinion_min = op[v2,0]\n",
    "    \n",
    "    if v2 == None:\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "\n",
    "        ##### change the agent's opinion\n",
    "        op[v2,0] = min_opinion   #-------------------------------------------------> store minimize strategy\n",
    "\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return (v2,min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________\n",
      "Minimizer Play\n",
      "Min start with agent 0\n",
      "Min start with agent 1\n",
      "Min start with agent 2\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "Network reaches equilibrium Polarization: 0.11124955500177997\n",
      "2 0.75 0.11124955500177997\n"
     ]
    }
   ],
   "source": [
    "max_touched = []\n",
    "min_touched = []\n",
    "(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "print(v2, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing above functions\n",
    "# min_touched=[]\n",
    "# max_touched=[]\n",
    "# # Game start from maximizer random play\n",
    "# print('Maximizer random selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# max_touched.append(v1)\n",
    "# # print('v1, max_opinion, max_pol')\n",
    "# # print(v1, max_opinion, max_pol)\n",
    "# # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Game start from minimizer random play \n",
    "# print('Minimizer random selection')\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# min_touched.append(v2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # # Testing block to see if player operate properly\n",
    "# (v1, max_opinion, max_pol) = maximizer_play(s,v2,min_opinion,n,min_touched)\n",
    "# (v2, min_opinion, min_pol) = minimizer_play(s,v1,max_opinion,n, max_touched)\n",
    "# # result =  minimizer_play(max_input, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row are Column are depended on min and max's choice: agent v and opinion \n",
    "def row_index(v2, min_opinion):\n",
    "    row = 11*v2 + min_opinion*10 \n",
    "    return int(row)\n",
    "def column_index(v1,max_opinion):\n",
    "    column = 2*v1 + max_opinion\n",
    "    return int(column)  #the python dataframe index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Strategy Payoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_payoff_row(op1,v2):\n",
    "    payoff_row = np.zeros(2*n)\n",
    "    v1 = 0\n",
    "#     print('one opinion changed -min')\n",
    "#     print(op1)\n",
    "    for column in range(2*n):\n",
    "#         print(column)\n",
    "        v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "        max_opinion = column%2\n",
    "#         print(v1, max_opinion)\n",
    "        # update the maximizer's change to the opinion array that has changed by minimizer(op1)\n",
    "        op2 = copy.copy(op1)\n",
    "        op2[v1,0] = max_opinion\n",
    "#         print('max_opinion')\n",
    "#         print(v1, max_opinion)\n",
    "#         print('two opinion changed -min +  max')\n",
    "#         print(op2)\n",
    "        # calculate the polarization with both max and min's action\n",
    "        payoff_row[column] = obj_polarization(A, L, op2, n)\n",
    "#         print(op2,payoff_row[column])\n",
    "    # when v1 == v2, the polarization should be negative for max, infinet for min. \n",
    "    # Replace the the column_index of agent v2 with 0 for max\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "    \n",
    "    return payoff_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.445    0.148    0.593    0.    -100.    -100.   ]\n"
     ]
    }
   ],
   "source": [
    "# #(1,0) (2,0.3928571428571428)\n",
    "op1=copy.copy(s)\n",
    "# print(op1)\n",
    "op1[2,0] = 1  #op1 is the opinion array that updated by minimizer\n",
    "# print(op1)\n",
    "payoff_row_1 = make_payoff_row(op1,2)\n",
    "print(payoff_row_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEEDDDDDDD UPDAE\n",
    "\n",
    "# Calculate polarization of minimizer's Mixed Strategy\n",
    "def mixed_min_polarization(s,v2,min_opinion,fla_max_fre):\n",
    "\n",
    "    op1 =  copy.copy(s) # make a copy of the innate opinion array \n",
    "    op1[v2,0] = min_opinion # then only updated by minimizer's current change\n",
    "\n",
    "    # calculate the polarization with both min(did here) and max's action(in make_payoff_row)\n",
    "    payoff_row = make_payoff_row(op1,v2)  # the vector list out 2*n payoffs after min's action combine with 2*n possible max's actions\n",
    "    #print(payoff_row)\n",
    "\n",
    "    # Replace the the column_index of agent v2 with 100 for min\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = 100\n",
    "    payoff_row[j_2] = 100\n",
    "    \n",
    "    print('Min Payoff Row')\n",
    "    print(payoff_row)\n",
    "    #calculate fictitious payoff - equi_min  \n",
    "    payoff_cal = payoff_row * fla_max_fre # fla_max_fre recorded the frequency of each maximizer's action, frequency sum = 1\n",
    "                                             # payoff (2*n array) * maximizer_action_frequency (2*n array)\n",
    "\n",
    "    mixed_pol = np.sum(payoff_cal) # add up all, calculate average/expected payoff\n",
    "\n",
    "\n",
    "#     print('min_mixed_polarization')\n",
    "#     print(mixed_pol)\n",
    "        # Replace the the column_index of agent v2 with 100 for min\n",
    "\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "\n",
    "    return (mixed_pol,payoff_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_s(op,n,v2):\n",
    "               #op - opinion array that updated by maximizer\n",
    "    c = [1/n] * n\n",
    "#     print(c)\n",
    "    sum_term = 0\n",
    "    j = 0\n",
    "\n",
    "    sum_term = np.dot(np.dot((A-c),(A[v2]-c)),op)  # sum up all terms\n",
    "    \n",
    "    term_out = op[v2]*np.dot((A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "    sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "    s_star = -sum_s/np.dot((A[v2]-c),(A[v2]-c))\n",
    "    s_star = s_star[0] #take value out of array\n",
    "    min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "    return min_opinion\n",
    "\n",
    "def derivate_s1(op,n,v2):\n",
    "               #op - opinion array that updated by maximizer\n",
    "    c = [1/n] * n\n",
    "#     print(c)\n",
    "    sum_term = 0\n",
    "    j = 0\n",
    "    for j in range(0,n):\n",
    "        term = op[j]*np.dot(np.transpose(A[j]-c),(A[v2]-c))\n",
    "#             print(A[j])\n",
    "#             print(A[v])\n",
    "        sum_term = sum_term + term  # sum up all terms\n",
    "    \n",
    "    term_out = op[v2]*np.dot(np.transpose(A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "    sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "    s_star = -sum_s/np.dot(np.transpose(A[v2]-c),(A[v2]-c))\n",
    "    s_star = s_star[0] #take value out of array\n",
    "    min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "    return min_opinion\n",
    "\n",
    "\n",
    "\n",
    "def min_mixed_opinion(op, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            \n",
    "##             temp = op[v1,0] \n",
    "          \n",
    "##             op[v1,0]= max_opinion #update innate opinion array with max_action    \n",
    "\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "#             print(fla_max_fre[column],min_opinion)\n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "#     print(weight_op)\n",
    "##             op[v1,0] = temp \n",
    "    \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "    \n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "0.49999999999999983\n",
      "0.49999999999999983\n"
     ]
    }
   ],
   "source": [
    "op2=op\n",
    "op2[0,0]=1\n",
    "print(op2)\n",
    "min_opinion1 = derivate_s(op2,n,1)\n",
    "print(min_opinion1)\n",
    "min_opinion2 = derivate_s1(op2,n,1)\n",
    "print(min_opinion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(v2,fla_max_fre)\n",
    "# (weight_op_1,payoff_row,min_por) = min_mixed_opinion_1(s, n, v2, fla_max_fre)\n",
    "# print(weight_op_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fla_max_fre = [0, 0, 0, 0, 0.65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0, 0, 0]\n",
    "# # print(fla_max_fre[13])\n",
    "# v2 = 1\n",
    "# (weight_op,payoff_row,min_por) = min_mixed_opinion(s, n, v2, fla_max_fre)\n",
    "# print(weight_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fla_max_fre = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "# # print(fla_max_fre[13])\n",
    "# v2 = 1\n",
    "# (weight_op,payoff_row,min_por) = min_mixed_opinion(s, n, v2, fla_max_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\n"
     ]
    }
   ],
   "source": [
    "all = list(range(n))    # for all agent \n",
    "C1 = [x for x in all if x not in max_touched]  \n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizer search: Go through each agent \n",
    "\n",
    "def mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre):\n",
    "    # current polarization that changed by maximizer, \"innate\" objective that min start with\n",
    "    op = copy.copy(s)\n",
    "    op[v1,0] = max_opinion\n",
    "    print('Check if op has been updated by Maximizer')\n",
    "    print(op)\n",
    "    min_por = obj_polarization(A, L, op, n) #min_por- set a standard to compare with pol after min's action\n",
    "    maxup_por = min_por # store innate max updated polarization\n",
    "#     print('check maxup por')\n",
    "#     print(maxup_por)\n",
    "#     payoffs = []    # create an empty list to store all polarizations   \n",
    "    champion = (None, None, 0, None)  # assume the best action is champion\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in max_touched]  # for the vertice that Maximizer has not touched\n",
    "    \n",
    "    for v2 in C1:   \n",
    "        print('Min start with agent '+ str(v2) )\n",
    "        (changed_opinion, payoff_row, por) = min_mixed_opinion(op, n, v2, fla_max_fre)   # find the best new_op option           \n",
    "        print('changed opinion, por, min_por')\n",
    "        print(changed_opinion, por, maxup_por)\n",
    "\n",
    "        if por < min_por:  # if the recent polarization is smaller than the minimum polarization in the history\n",
    "            min_por = por\n",
    "                                 # update the recent option as champion\n",
    "            champion = (v2, changed_opinion, payoff_row, min_por)  \n",
    "        else:\n",
    "            print('Innate polarization is smaller than Min action')\n",
    "\n",
    "    return (champion)  # find the best minimizer's action after going through every new_op option of every agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1,max_opinion\n",
      "1 0\n",
      "Check if op has been updated by Maximizer\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Min start with agent 0\n",
      "Min Payoff Row\n",
      "[100.    100.      0.      0.593   0.148   0.445]\n",
      "changed opinion, por, min_por\n",
      "0.0 0.2962951111158519 0.5933309600094931\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[  0.148   0.148   0.445   0.148 100.    100.   ]\n",
      "changed opinion, por, min_por\n",
      "0.5 0.296850664449194 0.5933309600094931\n",
      "Innate polarization is smaller than Min action\n"
     ]
    }
   ],
   "source": [
    "# fla_max_fre = [0, 0, 0, 0, 0.51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.49, 0, 0, 0]\n",
    "# v1 = 5\n",
    "# max_opinion = 0\n",
    "# max_touched = []\n",
    "print('v1,max_opinion')\n",
    "print(v1,max_opinion)\n",
    "champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by maximizer, fla_max_fre includes max's hisotry, so minimizer react to the innate op after that\n",
    "def mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre): \n",
    "\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre)\n",
    "    (v2, min_opinion, payoff_row, min_pol) = min_champion\n",
    "    \n",
    "    if v2 == None:    # if minimizer cannot find a action to minimize polarization after maximizer's action\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "#         print('v2', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(v2, min_opinion, innat_equi_por, min_pol)\n",
    "\n",
    "        # Store innate_op of the min_selected vertex\n",
    "        old_opinion_min = op[v2,0]\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "#         print(\"Payoff row\")\n",
    "#         print(payoff_row)\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return (v2, payoff_row, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Min start with agent 0\n",
      "Min Payoff Row\n",
      "[100.    100.      0.      0.593   0.148   0.445]\n",
      "changed opinion, por, min_por\n",
      "0.0 0.2962951111158519 0.5933309600094931\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[  0.148   0.148   0.445   0.148 100.    100.   ]\n",
      "changed opinion, por, min_por\n",
      "0.5 0.296850664449194 0.5933309600094931\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.0\n",
      "v2, payoff_row, min_opinion, polarization\n"
     ]
    }
   ],
   "source": [
    "print(max_touched)\n",
    "(v2, payoff_row, min_opinion, polarization) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "print('v2, payoff_row, min_opinion, polarization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payoff_matrix = np.empty((0, 2*n), float)\n",
    "# payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "# print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by minimizer, fla_min_fre includes min's hisotry, so maxmizer react to the innate op after that\n",
    "def mixed_max_polarization(payoff_matrix,v1,max_opinion,fla_min_fre):\n",
    "\n",
    "    # create payoff matrix for maxmizer\n",
    "    column = int(column_index(v1,max_opinion))\n",
    "#     print(payoff_matrix)\n",
    "#     print(\"column\"+str(column))\n",
    "    payoff_vector = payoff_matrix[:,column]\n",
    "    \n",
    "#     print('payoff vector')\n",
    "#     print(payoff_vector)\n",
    "\n",
    "    #calculate fictitious payoff - equi_max   \n",
    "    payoff_cal = payoff_vector * fla_min_fre #payoff * frequency\n",
    "    \n",
    "#     print('max_payoff_calculation')\n",
    "#     print(payoff_cal)\n",
    "    mixed_pol = np.sum(payoff_cal) # add up\n",
    "#     print(\"Max_mixed_polarization\")\n",
    "#     print(mixed_pol)\n",
    "\n",
    "    return mixed_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_pol = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if value of opinion at v should be set to 0 or 1 to maximize equilibrium polarization \n",
    "def max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre):\n",
    "    \n",
    "    por_arr = np.zeros(2)  # create a two_element array to store polarization value of each option\n",
    "\n",
    "\n",
    "    max_opi_option = [0, 1.0]   # Maximizer has two options to change agent v1's opinion\n",
    "    \n",
    "    # objective if set opinion to 0, 1.0\n",
    "    j = 0\n",
    "    for new_op in max_opi_option:\n",
    "#         print('change op to '+ str(i/10))\n",
    "        max_opinion = new_op\n",
    "\n",
    "        por_arr[j] = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)\n",
    "    \n",
    "        j = j + 1   # index increase 1, put the polarization in array\n",
    "\n",
    "#     print('Polarization Options')\n",
    "#     print(por_arr)\n",
    "    \n",
    "    maxmize_op = np.argmax(por_arr)  # the index of maximum polarization = max_opinion --[0,1]\n",
    "    max_por = np.max(por_arr)        # find the maximum polarization in the record\n",
    " \n",
    "#     print('new_op', 'innat_equi_por', 'max_por')\n",
    "#     print(maxmize_op, innat_equi_por, max_por)\n",
    "\n",
    "    return (maxmize_op, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fla_min_fre = [0, 0, 0, 0, 0.65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0, 0, 0]\n",
    "# v1 = 2\n",
    "# champion = max_mixed_opinion(payoff_matrix, n, v1, v2, fla_min_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which agent maximizer should select to maximizer the equilibrium polarization\n",
    "def mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre):\n",
    "#     print('Check if op has been updated by minimizer')\n",
    "#     print(op)\n",
    "    max_por = obj_polarization(A, L, op, n)  # use \"innate\"(after min action) polarization as a comparable standard to find max_por\n",
    "    minup_por = max_por # store innate min_update polarization\n",
    "#     print('check minup por')\n",
    "#     print(minup_por)\n",
    "    champion = (None, None, max_por)  # assume champion is the best action\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in min_touched]  # for the vertice that Minimizer has not touched\n",
    "    for v1 in C1:  \n",
    "            print('Maximizer start from agent'+str(v1))\n",
    "            (changed_opinion, por) = max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre)\n",
    "            print('changed_opinion, por, minup_por')\n",
    "            print(changed_opinion, por,minup_por)\n",
    "            \n",
    "            if por > max_por: # if the polarization of most recent action > maximum polarization of previous actions\n",
    "                max_por = por\n",
    "                champion = (v1, changed_opinion,max_por)   # save the this action as champion    \n",
    "#             else:\n",
    "#                 print('Innate polarization is bigger than max action')\n",
    " \n",
    "    return (champion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    0.593 0.148 0.445]\n",
      " [0.445 0.148 0.593 0.    0.    0.   ]\n",
      " [0.148 0.148 0.445 0.148 0.    0.   ]\n",
      " [0.    0.    0.148 0.445 0.148 0.148]]\n",
      "Maximizer start from agent1\n",
      "changed_opinion, por, minup_por\n",
      "1 0.2966654800047467 0.4449982200071198\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "1 0.1485179244468207 0.4449982200071198\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)\n",
    "champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_touched = []\n",
    "# # payoff_matrix = np.empty((0, 2*n), float)\n",
    "# # fla_min_fre = np.empty((0,n))\n",
    "# # champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)\n",
    "# # print(champion)\n",
    "# print(c1)\n",
    "# vertices = np.where(c1)\n",
    "# print(vertices)\n",
    "# por=0\n",
    "# for i in c1:\n",
    "#     print(i)\n",
    "#     max_por = 0.75\n",
    "#     if por > max_por:\n",
    "#         max_por = por\n",
    "#         print('yes')\n",
    "#     else:\n",
    "#         print('por<max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre): \n",
    "    op = copy.copy(s)   # op is a copy of innate opinion\n",
    "    \n",
    "    #update innat opinion \n",
    "    op[v2,0] = min_opinion  # Op has been updated by minimizer, so maximizer react to the innate op after that\n",
    "    \n",
    "\n",
    "    max_champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre) # The best choice among all opinions and vertexs\n",
    "    (v1, max_opinion, max_pol) = max_champion\n",
    "\n",
    "    if v1 == None:\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        \n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Testing function -- NO NEED TO RUN\n",
    "# min_touched = []\n",
    "# v2 = 0\n",
    "# min_opinion = 0\n",
    "# b = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "# print('v1,max_opinion,max_pol')\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Player's Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = make_innat_opinions(n, c1)\n",
    "# op = s\n",
    "# print(s)\n",
    "# y = mean_center(s,n)\n",
    "# # print(y)\n",
    "# innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "# print('Innate_polarization:')\n",
    "# print(innat_pol)\n",
    "\n",
    "# # Test equilibrium polarization\n",
    "# equ_pol = obj_polarization(A, L, op, n)\n",
    "# print('Equi_polarization:')\n",
    "# print(equ_pol)\n",
    "\n",
    "# di = equ_pol-innat_pol\n",
    "# print(\"Difference:\")\n",
    "# print(di)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = 'Ex_triangle_equal'\n",
    "# memeory = 50\n",
    "\n",
    "\n",
    "with open('Network_'+str(Network)+'.txt', \"a\") as fi:\n",
    "    print('Innate Opinion', file=fi)\n",
    "    print(s, file=fi)\n",
    "    print('Adjacency Matrix', file=fi)\n",
    "    print(G,file=fi)\n",
    "\n",
    "# Game Preparation\n",
    "def push(obj, element):\n",
    "    if len(obj) >= memory:\n",
    "        obj.pop(0)\n",
    "        print('pop')\n",
    "    obj.append(element)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Innate_polarization:\n",
      "0.5\n",
      "Equi_polarization:\n",
      "0.45389476266008816\n",
      "Difference:\n",
      "-0.04610523733991184\n"
     ]
    }
   ],
   "source": [
    "# s = make_innat_opinions(n, c1)\n",
    "# n = 545\n",
    "# op = s\n",
    "# print(s)\n",
    "n = 3\n",
    "s_1=[1]\n",
    "s_2=[0.5]\n",
    "s_3=[0]\n",
    "s = np.vstack((s_1,s_2,s_3))\n",
    "print(s)\n",
    "#s = (1,0.5,0) # polarization = 0.75\n",
    "# s = (1,0,0.5) # polarization = 0.75\n",
    "# s =(0,1,0.5) # polarization = 0.75\n",
    "#s = (0.5,1,0) # polarization = 0\n",
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, op, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[0.4623 0.1541 0.6164 0.     0.     0.    ]\n",
      " [0.1541 0.1541 0.4623 0.1541 0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "op = copy.copy(s)\n",
    "print(n)\n",
    "print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameters\n",
    "Game_rounds =201 # Rounds + 1- use for printing data\n",
    "memory = 1\n",
    "\n",
    "def all_fre_limited_touch(s, n):\n",
    "    # Preparation for the game\n",
    "    op = copy.copy(s)\n",
    "    payoff_matrix = np.empty((0, 2*n), float)\n",
    "    max_history = np.zeros([n, 2])  # n*2 matrix, agent i & opinion options\n",
    "    min_history = []  # append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "    print(type(min_history))\n",
    "\n",
    "    max_history_last_100 = np.zeros([n, 2]) \n",
    "    min_history_last_100= []\n",
    "\n",
    "    max_touched = []\n",
    "    min_touched = []\n",
    "    min_touched_all = []\n",
    "    min_touched_last_100 =[]\n",
    "    print('min_touched')\n",
    "    print(min_touched)\n",
    "    \n",
    "    \n",
    "    # Game start from maximizer random play\n",
    "    print('Maximizer first selection')\n",
    "    (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action \n",
    "    #(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "    #(v1, max_opinion, max_pol) = (2, 1, 0.14833274000237331)\n",
    "    First_max = (v1, max_opinion, max_pol) \n",
    "\n",
    "\n",
    "#     (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    # Maximizer start with greedy play\n",
    "    # (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "    max_touched.append(v1)    # save Maximizer's action history\n",
    "\n",
    "    # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "    max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "    # print('max_history')\n",
    "    # print(max_history)\n",
    "    print('history at spot')\n",
    "    print(max_history[v1,int(max_opinion)])\n",
    "\n",
    "    max_frequency = max_history/1  # its frequency, only played  1 time so far, divided by 1 \n",
    "    # print('fre_max at spot')\n",
    "    # print(max_frequency[v1,int(max_opinion)])\n",
    "\n",
    "    fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "                                            # so we can multiply the freuency (2n*1)with payoff array (1*2n) \n",
    "                                            # to get average payoff of fictitious play\n",
    "    print('fre_max at spot')\n",
    "    print(fla_max_fre)\n",
    "\n",
    "    column = int(column_index(v1,max_opinion))    # the frequency of maximizer's most recent action (v1,max_opinion)\n",
    "\n",
    "    print(fla_max_fre[column])\n",
    "\n",
    "    # print(np.shape(fla_max_fre.shape))\n",
    "\n",
    "\n",
    "    # if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "    print('Minimizer first selection')\n",
    "    (v2, min_opinion, min_pol) = random_play1(op,n) \n",
    "    #(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "    \n",
    "    #(v2, min_opinion, min_pol) = (1, 0, 0.5933309600094931)\n",
    "    First_min = (v2, min_opinion, min_pol)\n",
    "\n",
    "    if v1==v2:   # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "        sys.exit()\n",
    "\n",
    "    # Minimizer start with greedy play\n",
    "    # (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    min_touched.append(v2)\n",
    "   \n",
    "\n",
    "    # store minimizer play history\n",
    "    min_history.append((v2,min_opinion))\n",
    "    print('min_history')\n",
    "    print(min_history)\n",
    "\n",
    "\n",
    "    counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/1 #return only frequency of all min options in order\n",
    "    print('fla_min_fre')\n",
    "    print(fla_min_fre)\n",
    "\n",
    "\n",
    "    (a,payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "    payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "    print('Payoff Matrix')\n",
    "    print(payoff_matrix)\n",
    "    # print('fla_min_fre at the spot')\n",
    "    # min_counter = dict(counter)\n",
    "    # print(min_counter) \n",
    "    # print(min_counter[(v2,min_opinion)]) \n",
    "    # print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "\n",
    "    equi_min = min_pol\n",
    "    equi_max = max_pol\n",
    "    # print(equi_min)\n",
    "    # print(equi_max)\n",
    "\n",
    "\n",
    "\n",
    "    Flag = 0\n",
    "\n",
    "    i = 0\n",
    "    while Flag == 0: \n",
    "        i = i + 1\n",
    "        print(\"Game \" + str(i))\n",
    "        print(\"_____________________\")\n",
    "\n",
    "    #     if max_pol == min_pol:\n",
    "        if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "                                # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "            print('min_recent_'+str(memory)+'_touched')# then stop at Game 202\n",
    "            print(min_touched)\n",
    "            print('max_recent_'+str(memory)+'_touched')\n",
    "            print(max_touched)\n",
    "            print('Min last 100 action')\n",
    "            print(min_touched_last_100)\n",
    "\n",
    "            break\n",
    "\n",
    "        elif equi_min == equi_max:\n",
    "            print(\"Reached Nash Equilibrium at game\"+ str(i) + \"and Equi_Por = \" + str(equi_min))\n",
    "            print('max_distribution')\n",
    "            print(max_frequency)\n",
    "            print('min_distribution')\n",
    "            print(fla_min_fre)\n",
    "            Flag = 1\n",
    "            break\n",
    "        ############################## maximizer play  \n",
    "        else:\n",
    "            if i == Game_rounds-100:    #if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "    #             max_touched_100 = max_touched \n",
    "    #             min_touched_100 = min_touched\n",
    "    #             max_fre_100 = max_frequency  # store the max_frequency of first 100 iterataions\n",
    "    #             print('max_history')\n",
    "    #             print(max_history)\n",
    "    #             min_fre_100 = fla_min_fre  # max_frequency of first 100 iterations\n",
    "    #             print('min_history')\n",
    "    #             print(min_history)\n",
    "    # Remove max frequncy less than 0.1--\n",
    "                max_history_last_100 = np.zeros([n, 2]) \n",
    "                min_history_last_100 = [] \n",
    "                min_touched_last_100 =[]\n",
    "\n",
    "            (v1, max_opinion, equi_max) = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "            max_touched = push(max_touched, v1)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "    #             print('equi_max')\n",
    "    #             print(equi_max)\n",
    "    #         print(v1, max_opinion, max_pol)\n",
    "            # cumulate strategy \n",
    "            max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "\n",
    "            max_history_last_100[v1,int(max_opinion)] = max_history_last_100[v1,int(max_opinion)] +1\n",
    "    #         print('max_history')\n",
    "    #         print(max_history)\n",
    "    #________________________________________________________________\n",
    "            max_frequency = max_history/(i+1)  # its frequency \n",
    "    #         print('max_distribution')\n",
    "    #         print(max_frequency)\n",
    "        #     print(i+1) \n",
    "            fla_max_fre = max_frequency.flatten() #flaten max_frequency to calculate average payoff\n",
    "            print('fla_max_fre')\n",
    "            print(fla_max_fre)\n",
    "            print('fre_max at spot')\n",
    "            print(fla_max_fre[column])\n",
    "            # create payoff matrix for maxmizer\n",
    "            row = int(row_index(v2, min_opinion))\n",
    "            column = int(column_index(v1,max_opinion))\n",
    "\n",
    "    # _________________________________________________________________\n",
    "    #         ######################Visualize Maximizer's selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v1:\n",
    "    #                 color_map.append('Red')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True,node_size = 50)\n",
    "    #         plt.figure(figsize=(200, 200))\n",
    "    #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    ############################### minimizer play\n",
    "            (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "            min_touched = push(min_touched, v2)\n",
    "            min_touched_all.append(v2) \n",
    "            min_touched_last_100.append(v2)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('equi_min')\n",
    "    #         print(equi_min)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "            #         print(v2, min_opinion, min_pol)\n",
    "            if (v2,min_opinion) in counter.keys():\n",
    "                payoff_matrix = payoff_matrix # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "                print(\"Same history\")\n",
    "                print((str(v2),str(min_opinion)))\n",
    "            else:\n",
    "                payoff_matrix = np.vstack([payoff_matrix, payoff_row]) # if this is a new option, append to previous matrix\n",
    "    #                 print('payoff_row')\n",
    "    #                 print(payoff_row)\n",
    "            min_history.append((v2,min_opinion))\n",
    "            min_history_last_100.append((v2,min_opinion))\n",
    "            #         print('min_history')\n",
    "            #         print(min_history)\n",
    "            counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "            #print(counter)\n",
    "    #         print('counter.keys')\n",
    "    #         print(counter.keys())\n",
    "            fla_min_fre = np.array(list(counter.values()))/(i+1) #return only frequency of all min options in order\n",
    "    #         print('fla_min_fre')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "    #         print('fla_min_fre at the spot')\n",
    "    #         min_counter = dict(counter)\n",
    "    #         print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "            # create payoff matrix for minimizer\n",
    "            row = row_index(v2, min_opinion)\n",
    "            column = column_index(v1,max_opinion)\n",
    "            #     print('row, column')\n",
    "            #     print(row, column)\n",
    "\n",
    "            print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max)) \n",
    "    #         print('min_distribution')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "            ######################Visualize Minimizer selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G1, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G1)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v2:\n",
    "    #                 color_map.append('Blue')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True)\n",
    "    #         plt.figure(figsize=(25, 25))\n",
    "    #         plt.show()\n",
    "    return (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "min_touched\n",
      "[]\n",
      "Maximizer first selection\n",
      "    Agent2 's opinion 0.0 changed to 1\n",
      "Network reaches equilibrium Polarization: 0.14833274000237331\n",
      "history at spot\n",
      "1.0\n",
      "fre_max at spot\n",
      "[0. 0. 0. 0. 0. 1.]\n",
      "1.0\n",
      "Minimizer first selection\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "Network reaches equilibrium Polarization: 0.5933309600094931\n",
      "min_history\n",
      "[(1, 0)]\n",
      "Counter({(1, 0): 1})\n",
      "fla_min_fre\n",
      "[1.]\n",
      "Min Payoff Row\n",
      "[  0.       0.5933 100.     100.       0.5933   0.5933]\n",
      "Payoff Matrix\n",
      "[[   0.        0.5933 -100.     -100.        0.5933    0.5933]]\n",
      "Game 1\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.5933309600094931 0.5933309600094931\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "1 0.5933309600094933 0.5933309600094931\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0. 0. 0. 0. 0. 1.]\n",
      "fre_max at spot\n",
      "1.0\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [1. ]]\n",
      "Min start with agent 0\n",
      "Min Payoff Row\n",
      "[100.     100.       0.3337   0.4821   0.2596   0.1112]\n",
      "changed opinion, por, min_por\n",
      "0.75 0.11124955500177998 0.14833274000237331\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[5.9333e-01 5.9333e-01 1.0000e+02 1.0000e+02 5.9333e-01 6.5820e-32]\n",
      "changed opinion, por, min_por\n",
      "0.9999999999999997 6.582031849810417e-32 0.14833274000237331\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.9999999999999997\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 6.582031849810417e-32 and Equi_Max = 0.5933309600094933\n",
      "Game 2\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.593330960009493 0.5933309600094929\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.593330960009493 0.5933309600094929\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.3333 0.     0.     0.     0.6667]\n",
      "fre_max at spot\n",
      "0.6666666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.24722123333728885 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 66.70374985166725 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.24722123333728885 and Equi_Max = 0.593330960009493\n",
      "Game 3\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.5438867133420353 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.5438867133420353 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.  0.5 0.  0.  0.  0.5]\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.2966654800047466 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 50.05562477750089 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2966654800047466 and Equi_Max = 0.5438867133420353\n",
      "Game 4\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.5191645900083064 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.5191645900083064 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.  0.6 0.  0.  0.  0.4]\n",
      "fre_max at spot\n",
      "0.6\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.3263320280052212 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 40.06674973300107 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3263320280052212 and Equi_Max = 0.5191645900083064\n",
      "Game 5\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.5043313160080691 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.5043313160080691 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.6667 0.     0.     0.     0.3333]\n",
      "fre_max at spot\n",
      "0.6666666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.3461097266722043 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 33.407499703334516 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3461097266722043 and Equi_Max = 0.5043313160080691\n",
      "Game 6\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4944424666745775 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4944424666745775 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.7143 0.     0.     0.     0.2857]\n",
      "fre_max at spot\n",
      "0.7142857142857143\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.36023665429147794 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 28.650892539286982 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.36023665429147794 and Equi_Max = 0.4944424666745775\n",
      "Game 7\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4873790028649406 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4873790028649406 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.   0.75 0.   0.   0.   0.25]\n",
      "fre_max at spot\n",
      "0.75\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.3708318500059332 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 25.083437166251336 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3708318500059332 and Equi_Max = 0.4873790028649406\n",
      "Game 8\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.48208140500771307 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.48208140500771307 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.7778 0.     0.     0.     0.2222]\n",
      "fre_max at spot\n",
      "0.7777777777777778\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.3790725577838428 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 22.308749653890274 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3790725577838428 and Equi_Max = 0.48208140500771307\n",
      "Game 9\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4779610511187583 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4779610511187583 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.  0.8 0.  0.  0.  0.2]\n",
      "fre_max at spot\n",
      "0.8\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999999 0.38566512400617053 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 20.088999644001422 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999999\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.38566512400617053 and Equi_Max = 0.4779610511187583\n",
      "Game 10\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4746647680075945 0.4449982200071198\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4746647680075945 0.4449982200071198\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.8182 0.     0.     0.     0.1818]\n",
      "fre_max at spot\n",
      "0.8181818181818182\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999999 0.3910590418244387 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 18.272840545001458 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999999\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999999')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3910590418244387 and Equi_Max = 0.4746647680075945\n",
      "Game 11\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4719678090984603 0.4449982200071198\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4719678090984603 0.4449982200071198\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.8333 0.     0.     0.     0.1667]\n",
      "fre_max at spot\n",
      "0.8333333333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.3955539733396621 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 16.759374629168146 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3955539733396621 and Equi_Max = 0.4719678090984603\n",
      "Game 12\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.46972034334084867 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.46972034334084867 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.8462 0.     0.     0.     0.1538]\n",
      "fre_max at spot\n",
      "0.8461538461538461\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.3993573769294665 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 15.478749623463045 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3993573769294665 and Equi_Max = 0.46972034334084867\n",
      "Game 13\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.46781864154594643 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.46781864154594643 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.8571 0.     0.     0.     0.1429]\n",
      "fre_max at spot\n",
      "0.8571428571428571\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.4026174371492988 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 14.381071047144381 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4026174371492988 and Equi_Max = 0.46781864154594643\n",
      "Game 14\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4661886114360302 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4661886114360302 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.8667 0.     0.     0.     0.1333]\n",
      "fre_max at spot\n",
      "0.8666666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4054428226731536 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 13.429749614334877 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4054428226731536 and Equi_Max = 0.4661886114360302\n",
      "Game 15\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4647759186741029 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4647759186741029 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.875 0.    0.    0.    0.125]\n",
      "fre_max at spot\n",
      "0.875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4079150350065265 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 12.597343360626557 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4079150350065265 and Equi_Max = 0.4647759186741029\n",
      "Game 16\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4635398125074164 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4635398125074164 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.8824 0.     0.     0.     0.1176]\n",
      "fre_max at spot\n",
      "0.8823529411764706\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.41009639883009075 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 11.862867254413334 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.41009639883009075 and Equi_Max = 0.4635398125074164\n",
      "Game 17\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.46244913059563425 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.46244913059563425 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.8889 0.     0.     0.     0.1111]\n",
      "fre_max at spot\n",
      "0.8888888888888888\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.41203538889548125 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 11.209999604446026 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.41203538889548125 and Equi_Max = 0.46244913059563425\n",
      "Game 18\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4614796355629389 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4614796355629389 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.8947 0.     0.     0.     0.1053]\n",
      "fre_max at spot\n",
      "0.8947368421052632\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.41377027474346223 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 10.62585486500159 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.41377027474346223 and Equi_Max = 0.4614796355629389\n",
      "Game 19\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4606121926389485 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4606121926389485 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.  0.9 0.  0.  0.  0.1]\n",
      "fre_max at spot\n",
      "0.9\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4153316720066451 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 10.100124599501601 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4153316720066451 and Equi_Max = 0.4606121926389485\n",
      "Game 20\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4598314940073571 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4598314940073571 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9048 0.     0.     0.     0.0952]\n",
      "fre_max at spot\n",
      "0.9047619047619048\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.41674436476857246 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 9.624463883096848 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.41674436476857246 and Equi_Max = 0.4598314940073571\n",
      "Game 21\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4591251476263934 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4591251476263934 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9091 0.     0.     0.     0.0909]\n",
      "fre_max at spot\n",
      "0.9090909090909091\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.41802863091577913 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 9.192045050001619 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.41802863091577913 and Equi_Max = 0.4591251476263934\n",
      "Game 22\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45848301455279 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45848301455279 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.913 0.    0.    0.    0.087]\n",
      "fre_max at spot\n",
      "0.9130434782608695\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.41920122174583746 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 8.797227854566842 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.41920122174583746 and Equi_Max = 0.45848301455279\n",
      "Game 23\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45789671913776087 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45789671913776087 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9167 0.     0.     0.     0.0833]\n",
      "fre_max at spot\n",
      "0.9166666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.4202760966733909 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 8.435312092084963 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4202760966733909 and Equi_Max = 0.45789671913776087\n",
      "Game 24\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4573592816739842 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4573592816739842 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.   0.92 0.   0.   0.   0.08]\n",
      "fre_max at spot\n",
      "0.92\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4212649816067401 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 8.102349590601637 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4212649816067401 and Equi_Max = 0.4573592816739842\n",
      "Game 25\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4568648392073096 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4568648392073096 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9231 0.     0.     0.     0.0769]\n",
      "fre_max at spot\n",
      "0.9230769230769231\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4221777984682931 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 7.794999589232413 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4221777984682931 and Equi_Max = 0.4568648392073096\n",
      "Game 26\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45640843077653304 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45640843077653304 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9259 0.     0.     0.     0.0741]\n",
      "fre_max at spot\n",
      "0.9259259259259259\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.42302299926602743 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 7.510416254631277 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42302299926602743 and Equi_Max = 0.45640843077653304\n",
      "Game 27\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4559858303776659 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4559858303776659 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9286 0.     0.     0.     0.0714]\n",
      "fre_max at spot\n",
      "0.9285714285714286\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4238078285782093 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 7.246160301073081 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4238078285782093 and Equi_Max = 0.4559858303776659\n",
      "Game 28\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.455593415721575 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.455593415721575 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.931 0.    0.    0.    0.069]\n",
      "fre_max at spot\n",
      "0.9310344827586207\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.42453853173093037 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 7.00012889603614 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42453853173093037 and Equi_Max = 0.455593415721575\n",
      "Game 29\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4552280641452145 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4552280641452145 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9333 0.     0.     0.     0.0667]\n",
      "fre_max at spot\n",
      "0.9333333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.42522052134013666 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 6.770499584668328 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42522052134013666 and Equi_Max = 0.4552280641452145\n",
      "Game 30\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4548870693406113 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4548870693406113 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9355 0.     0.     0.     0.0645]\n",
      "fre_max at spot\n",
      "0.9354838709677419\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.4258585116197167 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 6.55568506758231 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4258585116197167 and Equi_Max = 0.4548870693406113\n",
      "Game 31\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4545680742008213 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4545680742008213 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9375 0.     0.     0.     0.0625]\n",
      "fre_max at spot\n",
      "0.9375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.42645662750682306 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 6.3542964578141685 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42645662750682306 and Equi_Max = 0.4545680742008213\n",
      "Game 32\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4542690162572681 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4542690162572681 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9394 0.     0.     0.     0.0606]\n",
      "fre_max at spot\n",
      "0.9393939393939394\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.42701849394622604 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 6.165113218335005 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42701849394622604 and Equi_Max = 0.4542690162572681\n",
      "Game 33\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45398808303756666 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45398808303756666 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9412 0.     0.     0.     0.0588]\n",
      "fre_max at spot\n",
      "0.9411764705882353\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.42754730941860525 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 5.9870584047075575 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42754730941860525 and Equi_Max = 0.45398808303756666\n",
      "Game 34\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.453723675301377 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.453723675301377 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9429 0.     0.     0.     0.0571]\n",
      "fre_max at spot\n",
      "0.9428571428571428\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.42804590686399135 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 5.8191781518588215 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42804590686399135 and Equi_Max = 0.453723675301377\n",
      "Game 35\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4534743765786839 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4534743765786839 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9444 0.     0.     0.     0.0556]\n",
      "fre_max at spot\n",
      "0.9444444444444444\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4285168044513005 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 5.660624579723903 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4285168044513005 and Equi_Max = 0.4534743765786839\n",
      "Game 36\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4532389277850294 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4532389277850294 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9459 0.     0.     0.     0.0541]\n",
      "fre_max at spot\n",
      "0.9459459459459459\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4289622481149713 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 5.51064147094763 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4289622481149713 and Equi_Max = 0.4532389277850294\n",
      "Game 37\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45301620595319403 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45301620595319403 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9474 0.     0.     0.     0.0526]\n",
      "fre_max at spot\n",
      "0.9473684210526315\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.42938424737529096 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 5.368552210001686 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42938424737529096 and Equi_Max = 0.45301620595319403\n",
      "Game 38\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4528052063230341 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4528052063230341 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9487 0.     0.     0.     0.0513]\n",
      "fre_max at spot\n",
      "0.9487179487179487\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.42978460564790194 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 5.233749577822201 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42978460564790194 and Equi_Max = 0.4528052063230341\n",
      "Game 39\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4526050271867287 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4526050271867287 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.   0.95 0.   0.   0.   0.05]\n",
      "fre_max at spot\n",
      "0.95\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.43016494600688243 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 5.105687077251691 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43016494600688243 and Equi_Max = 0.4526050271867287\n",
      "Game 40\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45241485700723844 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45241485700723844 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9512 0.     0.     0.     0.0488]\n",
      "fre_max at spot\n",
      "0.9512195121951219\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.4305267331776199 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.983871527928522 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4305267331776199 and Equi_Max = 0.45241485700723844\n",
      "Game 41\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45223396342186967 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45223396342186967 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9524 0.     0.     0.     0.0476]\n",
      "fre_max at spot\n",
      "0.9523809523809523\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.4308712923878461 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 4.867856719049314 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4308712923878461 and Equi_Max = 0.45223396342186967\n",
      "Game 42\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4520616838167566 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4520616838167566 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9535 0.     0.     0.     0.0465]\n",
      "fre_max at spot\n",
      "0.9534883720930233\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4311998255882944 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.757237947792395 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4311998255882944 and Equi_Max = 0.4520616838167566\n",
      "Game 43\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4518974172165324 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4518974172165324 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9545 0.     0.     0.     0.0455]\n",
      "fre_max at spot\n",
      "0.9545454545454546\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4315134254614495 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.6516473025017 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4315134254614495 and Equi_Max = 0.4518974172165324\n",
      "Game 44\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4517406172799549 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4517406172799549 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9556 0.     0.     0.     0.0444]\n",
      "fre_max at spot\n",
      "0.9555555555555556\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4318130875624644 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.550749574779479 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4318130875624644 and Equi_Max = 0.4517406172799549\n",
      "Game 45\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4515907862294475 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4515907862294475 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9565 0.     0.     0.     0.0435]\n",
      "fre_max at spot\n",
      "0.9565217391304348\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4320997208764786 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 4.454238704784311 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4320997208764786 and Equi_Max = 0.4515907862294475\n",
      "Game 46\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4514474695724403 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4514474695724403 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9574 0.     0.     0.     0.0426]\n",
      "fre_max at spot\n",
      "0.9574468085106383\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4323741570281944 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.361834680320853 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4323741570281944 and Equi_Max = 0.4514474695724403\n",
      "Game 47\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4513102514965825 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4513102514965825 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9583 0.     0.     0.     0.0417]\n",
      "fre_max at spot\n",
      "0.9583333333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43263715834025535 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.273280823543372 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43263715834025535 and Equi_Max = 0.4513102514965825\n",
      "Game 48\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4511787508405519 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4511787508405519 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9592 0.     0.     0.     0.0408]\n",
      "fre_max at spot\n",
      "0.9591836734693877\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.4328894249048852 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.1883414098996665 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4328894249048852 and Equi_Max = 0.4511787508405519\n",
      "Game 49\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45105261755823695 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45105261755823695 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.   0.96 0.   0.   0.   0.04]\n",
      "fre_max at spot\n",
      "0.96\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43313160080692986 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.106799572801709 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43313160080692986 and Equi_Max = 0.45105261755823695\n",
      "Game 50\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45093152960721467 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45093152960721467 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9608 0.     0.     0.     0.0392]\n",
      "fre_max at spot\n",
      "0.9607843137254902\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43336427961477675 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 4.0284554548056315 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43336427961477675 and Equi_Max = 0.45093152960721467\n",
      "Game 51\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45081519020329125 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45081519020329125 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9615 0.     0.     0.     0.0385]\n",
      "fre_max at spot\n",
      "0.9615384615384616\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4335880092377064 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.9531245721170962 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4335880092377064 and Equi_Max = 0.45081519020329125\n",
      "Game 52\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4507033253918264 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4507033253918264 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9623 0.     0.     0.     0.0377]\n",
      "fre_max at spot\n",
      "0.9622641509433962\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4338032962333558 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.880636364246996 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4338032962333558 and Equi_Max = 0.4507033253918264\n",
      "Game 53\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4505956818940017 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4505956818940017 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.963 0.    0.    0.    0.037]\n",
      "fre_max at spot\n",
      "0.9629629629629629\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.43401060963657356 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.8108329048165284 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43401060963657356 and Equi_Max = 0.4505956818940017\n",
      "Game 54\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45049202519239284 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45049202519239284 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9636 0.     0.     0.     0.0364]\n",
      "fre_max at spot\n",
      "0.9636363636363636\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4342103843705835 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 3.7435677530017153 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4342103843705835 and Equi_Max = 0.45049202519239284\n",
      "Game 55\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45039213782538784 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45039213782538784 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9643 0.     0.     0.     0.0357]\n",
      "fre_max at spot\n",
      "0.9642857142857143\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43440302429266453 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.6787049280374307 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43440302429266453 and Equi_Max = 0.45039213782538784\n",
      "Game 56\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.45029581786434736 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.45029581786434736 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9649 0.     0.     0.     0.0351]\n",
      "fre_max at spot\n",
      "0.9649122807017544\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4345889049192339 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.616117991668384 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4345889049192339 and Equi_Max = 0.45029581786434736\n",
      "Game 57\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4502028775510627 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4502028775510627 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9655 0.     0.     0.     0.0345]\n",
      "fre_max at spot\n",
      "0.9655172413793104\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43476837586902506 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.5556892255189596 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43476837586902506 and Equi_Max = 0.4502028775510627\n",
      "Game 58\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4501131420761671 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4501131420761671 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9661 0.     0.     0.     0.0339]\n",
      "fre_max at spot\n",
      "0.9661016949152542\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4349417630578063 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.4973088921203637 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4349417630578063 and Equi_Max = 0.4501131420761671\n",
      "Game 59\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4500264484817765 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4500264484817765 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9667 0.     0.     0.     0.0333]\n",
      "fre_max at spot\n",
      "0.9666666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4351093706736282 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.4408745698350542 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4351093706736282 and Equi_Max = 0.4500264484817765\n",
      "Game 60\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44994264467386547 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44994264467386547 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9672 0.     0.     0.     0.0328]\n",
      "fre_max at spot\n",
      "0.9672131147540983\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4352714829577838 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.386290553198443 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4352714829577838 and Equi_Max = 0.44994264467386547\n",
      "Game 61\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44986158853178776 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44986158853178776 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9677 0.     0.     0.     0.0323]\n",
      "fre_max at spot\n",
      "0.967741935483871\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43542836581341826 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.3334673112920448 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43542836581341826 and Equi_Max = 0.44986158853178776\n",
      "Game 62\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4497831471039705 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4497831471039705 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9683 0.     0.     0.     0.0317]\n",
      "fre_max at spot\n",
      "0.9682539682539683\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4355802682609373 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.282320997700136 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4355802682609373 and Equi_Max = 0.4497831471039705\n",
      "Game 63\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44970719588021096 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44970719588021096 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9688 0.     0.     0.     0.0312]\n",
      "fre_max at spot\n",
      "0.96875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43572742375697143 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.2327730064079745 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43572742375697143 and Equi_Max = 0.44970719588021096\n",
      "Game 64\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44963361813219394 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44963361813219394 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9692 0.     0.     0.     0.0308]\n",
      "fre_max at spot\n",
      "0.9692307692307692\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4358700513915891 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.1847495686940333 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4358700513915891 and Equi_Max = 0.44963361813219394\n",
      "Game 65\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4495623043148851 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4495623043148851 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9697 0.     0.     0.     0.0303]\n",
      "fre_max at spot\n",
      "0.9696969696969697\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4360083569766729 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.1381813866683927 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4360083569766729 and Equi_Max = 0.4495623043148851\n",
      "Game 66\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44949315152234315 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44949315152234315 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9701 0.     0.     0.     0.0299]\n",
      "fre_max at spot\n",
      "0.9701492537313433\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43614253403682884 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.093003299628592 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43614253403682884 and Equi_Max = 0.44949315152234315\n",
      "Game 67\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4494260629922652 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4494260629922652 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9706 0.     0.     0.     0.0294]\n",
      "fre_max at spot\n",
      "0.9705882352941176\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4362727647128625 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.0491539798546685 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4362727647128625 and Equi_Max = 0.4494260629922652\n",
      "Game 68\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4493609476542484 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4493609476542484 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.971 0.    0.    0.    0.029]\n",
      "fre_max at spot\n",
      "0.9710144927536232\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4363992205866923 0.4449982200071198\n",
      "Min start with agent 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 3.006575654856801 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4363992205866923 and Equi_Max = 0.4493609476542484\n",
      "Game 69\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4492977197173335 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4492977197173335 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9714 0.     0.     0.     0.0286]\n",
      "fre_max at spot\n",
      "0.9714285714285714\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43652206343555555 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.9652138534303005 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43652206343555555 and Equi_Max = 0.4492977197173335\n",
      "Game 70\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44923629829290185 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44923629829290185 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9718 0.     0.     0.     0.0282]\n",
      "fre_max at spot\n",
      "0.971830985915493\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.436641445922479 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.925017173170744 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.436641445922479 and Equi_Max = 0.44923629829290185\n",
      "Game 71\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4491766070494401 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4491766070494401 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9722 0.     0.     0.     0.0278]\n",
      "fre_max at spot\n",
      "0.9722222222222222\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4367575122292101 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.8859370673628417 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4367575122292101 and Equi_Max = 0.4491766070494401\n",
      "Game 72\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44911857389607457 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44911857389607457 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9726 0.     0.     0.     0.0274]\n",
      "fre_max at spot\n",
      "0.9726027397260274\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4368703986371267 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.8479276493852925 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4368703986371267 and Equi_Max = 0.44911857389607457\n",
      "Game 73\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4490621306921163 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4490621306921163 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.973 0.    0.    0.    0.027]\n",
      "fre_max at spot\n",
      "0.972972972972973\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4369802340610456 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 2.8109455129747047 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4369802340610456 and Equi_Max = 0.4490621306921163\n",
      "Game 74\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4490072129801569 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4490072129801569 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9733 0.     0.     0.     0.0267]\n",
      "fre_max at spot\n",
      "0.9733333333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43708714054032655 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.7749495668683997 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43708714054032655 and Equi_Max = 0.4490072129801569\n",
      "Game 75\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4489537597405164 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4489537597405164 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9737 0.     0.     0.     0.0263]\n",
      "fre_max at spot\n",
      "0.9736842105263158\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4371912336912054 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.739900882501733 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4371912336912054 and Equi_Max = 0.4489537597405164\n",
      "Game 76\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4489017131650769 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4489017131650769 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.974 0.    0.    0.    0.026]\n",
      "fre_max at spot\n",
      "0.974025974025974\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43729262312387956 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.7057625535731624 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43729262312387956 and Equi_Max = 0.4489017131650769\n",
      "Game 77\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4488510184487398 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4488510184487398 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9744 0.     0.     0.     0.0256]\n",
      "fre_max at spot\n",
      "0.9743589743589743\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4373914128275109 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.6724995664119904 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4373914128275109 and Equi_Max = 0.4488510184487398\n",
      "Game 78\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4488016235969242 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4488016235969242 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9747 0.     0.     0.     0.0253]\n",
      "fre_max at spot\n",
      "0.9746835443037974\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4374877015259869 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.6400786801916083 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4374877015259869 and Equi_Max = 0.4488016235969242\n",
      "Game 79\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4487534792476861 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4487534792476861 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.975 0.    0.    0.    0.025]\n",
      "fre_max at spot\n",
      "0.975\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43758158300700106 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.6084683161267357 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43758158300700106 and Equi_Max = 0.4487534792476861\n",
      "Game 80\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44870653850717906 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44870653850717906 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9753 0.     0.     0.     0.0247]\n",
      "fre_max at spot\n",
      "0.9753086419753086\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43767314642675564 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.577638454878279 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43767314642675564 and Equi_Max = 0.44870653850717906\n",
      "Game 81\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44866075679730183 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44866075679730183 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9756 0.     0.     0.     0.0244]\n",
      "fre_max at spot\n",
      "0.975609756097561\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43776247659236983 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.547560541465151 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43776247659236983 and Equi_Max = 0.44866075679730183\n",
      "Game 82\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4486160917144947 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4486160917144947 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9759 0.     0.     0.     0.0241]\n",
      "fre_max at spot\n",
      "0.9759036144578314\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4378496542238729 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.51820739704993 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4378496542238729 and Equi_Max = 0.4486160917144947\n",
      "Game 83\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44857250289874323 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44857250289874323 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9762 0.     0.     0.     0.0238]\n",
      "fre_max at spot\n",
      "0.9761904761904762\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43793475619748296 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.4895531370255473 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43793475619748296 and Equi_Max = 0.44857250289874323\n",
      "Game 84\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4485299519119382 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4485299519119382 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9765 0.     0.     0.     0.0235]\n",
      "fre_max at spot\n",
      "0.9764705882352941\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4380178557717139 0.4449982200071198\n",
      "Min start with agent 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.4615730948840913 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4380178557717139 and Equi_Max = 0.4485299519119382\n",
      "Game 85\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44848840212482266 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44848840212482266 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9767 0.     0.     0.     0.0233]\n",
      "fre_max at spot\n",
      "0.9767441860465116\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.438099022797707 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.4342437513970876 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.438099022797707 and Equi_Max = 0.44848840212482266\n",
      "Game 86\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4484478186118261 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4484478186118261 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.977 0.    0.    0.    0.023]\n",
      "fre_max at spot\n",
      "0.9770114942528736\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4381783239150566 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 2.4075426686798997 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4381783239150566 and Equi_Max = 0.4484478186118261\n",
      "Game 87\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4484081680531513 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4484081680531513 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9773 0.     0.     0.     0.0227]\n",
      "fre_max at spot\n",
      "0.9772727272727273\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43825582273428465 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.3814484287517397 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43825582273428465 and Equi_Max = 0.4484081680531513\n",
      "Game 88\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44836941864353735 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44836941864353735 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9775 0.     0.     0.     0.0225]\n",
      "fre_max at spot\n",
      "0.9775280898876404\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.43833158000701306 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.3559405762376953 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43833158000701306 and Equi_Max = 0.44836941864353735\n",
      "Game 89\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4483315400071731 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4483315400071731 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9778 0.     0.     0.     0.0222]\n",
      "fre_max at spot\n",
      "0.9777777777777777\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43840565378479207 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.330999564890629 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43840565378479207 and Equi_Max = 0.4483315400071731\n",
      "Game 90\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44829450311828356 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44829450311828356 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.978 0.    0.    0.    0.022]\n",
      "fre_max at spot\n",
      "0.978021978021978\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.438478099567455 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.3066067076391037 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.438478099567455 and Equi_Max = 0.44829450311828356\n",
      "Game 91\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44825828022695213 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44825828022695213 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9783 0.     0.     0.     0.0217]\n",
      "fre_max at spot\n",
      "0.9782608695652174\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4385489704417992 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.2827441298930453 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4385489704417992 and Equi_Max = 0.44825828022695213\n",
      "Game 92\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44822284478978003 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44822284478978003 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9785 0.     0.     0.     0.0215]\n",
      "fre_max at spot\n",
      "0.978494623655914\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4386183172113188 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.259394725861957 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4386183172113188 and Equi_Max = 0.44822284478978003\n",
      "Game 93\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44818817140502026 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44818817140502026 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9787 0.     0.     0.     0.0213]\n",
      "fre_max at spot\n",
      "0.9787234042553191\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43868618851765706 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.2365421176613167 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43868618851765706 and Equi_Max = 0.44818817140502026\n",
      "Game 94\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4481542357518511 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4481542357518511 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9789 0.     0.     0.     0.0211]\n",
      "fre_max at spot\n",
      "0.9789473684210527\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43875263095438827 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 2.2141706170017423 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43875263095438827 and Equi_Max = 0.4481542357518511\n",
      "Game 95\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4481210145334855 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4481210145334855 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9792 0.     0.     0.     0.0208]\n",
      "fre_max at spot\n",
      "0.9791666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43881768917368746 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.1922651892725757 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43881768917368746 and Equi_Max = 0.4481210145334855\n",
      "Game 96\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44808848542383584 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44808848542383584 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9794 0.     0.     0.     0.0206]\n",
      "fre_max at spot\n",
      "0.979381443298969\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4388814059864033 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.170811419847104 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4388814059864033 and Equi_Max = 0.44808848542383584\n",
      "Game 97\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44805662701747795 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44805662701747795 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9796 0.     0.     0.     0.0204]\n",
      "fre_max at spot\n",
      "0.9795918367346939\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43894382245600244 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 2.149795482450723 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43894382245600244 and Equi_Max = 0.44805662701747795\n",
      "Game 98\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4480254187826784 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4480254187826784 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9798 0.     0.     0.     0.0202]\n",
      "fre_max at spot\n",
      "0.9797979797979798\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43900497798682186 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.1292041094461887 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43900497798682186 and Equi_Max = 0.4480254187826784\n",
      "Game 99\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44799484101726866 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44799484101726866 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.   0.98 0.   0.   0.   0.02]\n",
      "fre_max at spot\n",
      "0.98\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43906491040702483 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.109024563901744 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43906491040702483 and Equi_Max = 0.44799484101726866\n",
      "Game 100\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4479648748071673 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4479648748071673 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9802 0.     0.     0.     0.0198]\n",
      "fre_max at spot\n",
      "0.9801980198019802\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43912365604662973 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.0892446133185762 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43912365604662973 and Equi_Max = 0.4479648748071673\n",
      "Game 101\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44793550198736476 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44793550198736476 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9804 0.     0.     0.     0.0196]\n",
      "fre_max at spot\n",
      "0.9803921568627451\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43918124981094825 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 2.0698525049037055 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43918124981094825 and Equi_Max = 0.44793550198736476\n",
      "Game 102\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44790670510520547 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44790670510520547 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9806 0.     0.     0.     0.0194]\n",
      "fre_max at spot\n",
      "0.9805825242718447\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.439237725249746 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 2.050836942283299 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.439237725249746 and Equi_Max = 0.44790670510520547\n",
      "Game 103\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4478784673858066 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4478784673858066 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9808 0.     0.     0.     0.0192]\n",
      "fre_max at spot\n",
      "0.9807692307692307\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4392931146224131 0.4449982200071198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 2.032187063559438 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4392931146224131 and Equi_Max = 0.4478784673858066\n",
      "Game 104\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4478507726994731 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4478507726994731 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.981 0.    0.    0.    0.019]\n",
      "fre_max at spot\n",
      "0.9809523809523809\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4393474489594103 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 2.013892420620794 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4393474489594103 and Equi_Max = 0.4478507726994731\n",
      "Game 105\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44782360553097444 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44782360553097444 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9811 0.     0.     0.     0.0189]\n",
      "fre_max at spot\n",
      "0.9811320754716981\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43940075812023777 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.9959429596243878 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43940075812023777 and Equi_Max = 0.44782360553097444\n",
      "Game 106\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44779695095056077 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44779695095056077 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9813 0.     0.     0.     0.0187]\n",
      "fre_max at spot\n",
      "0.9813084112149533\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4394530708481525 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.9783290025718399 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4394530708481525 and Equi_Max = 0.44779695095056077\n",
      "Game 107\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4477707945866034 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4477707945866034 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9815 0.     0.     0.     0.0185]\n",
      "fre_max at spot\n",
      "0.9814814814814815\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43950441482184666 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.9610412299091542 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43950441482184666 and Equi_Max = 0.4477707945866034\n",
      "Game 108\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4477451225997563 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4477451225997563 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9817 0.     0.     0.     0.0183]\n",
      "fre_max at spot\n",
      "0.981651376146789\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43955481670428037 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.9440706640843162 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43955481670428037 and Equi_Max = 0.4477451225997563\n",
      "Game 109\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44771992165853947 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44771992165853947 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9818 0.     0.     0.     0.0182]\n",
      "fre_max at spot\n",
      "0.9818181818181818\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43960430218885166 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.9274086540017477 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43960430218885166 and Equi_Max = 0.44771992165853947\n",
      "Game 110\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44769517891625377 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44769517891625377 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.982 0.    0.    0.    0.018]\n",
      "fre_max at spot\n",
      "0.9819819819819819\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4396528960430703 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.9110468603170632 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4396528960430703 and Equi_Max = 0.44769517891625377\n",
      "Game 111\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44767088198914445 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44767088198914445 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9821 0.     0.     0.     0.0179]\n",
      "fre_max at spot\n",
      "0.9821428571428571\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.4397006221498921 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.8949772415196051 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4397006221498921 and Equi_Max = 0.44767088198914445\n",
      "Game 112\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44764701893573355 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44764701893573355 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9823 0.     0.     0.     0.0177]\n",
      "fre_max at spot\n",
      "0.9823008849557522\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4397475035468587 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.8791920407539608 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4397475035468587 and Equi_Max = 0.44764701893573355\n",
      "Game 113\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44762357823725024 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44762357823725024 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9825 0.     0.     0.     0.0175]\n",
      "fre_max at spot\n",
      "0.9824561403508771\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.4397935624631768 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.863683773335082 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4397935624631768 and Equi_Max = 0.44762357823725024\n",
      "Game 114\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44760054877909117 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44760054877909117 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9826 0.     0.     0.     0.0174]\n",
      "fre_max at spot\n",
      "0.9826086956521739\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.43983882035486327 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.8484452149147925 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43983882035486327 and Equi_Max = 0.44760054877909117\n",
      "Game 115\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44757791983324796 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44757791983324796 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9828 0.     0.     0.     0.0172]\n",
      "fre_max at spot\n",
      "0.9827586206896551\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4398832979380724 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.8334693902603698 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4398832979380724 and Equi_Max = 0.44757791983324796\n",
      "Game 116\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44755568104164345 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44755568104164345 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9829 0.     0.     0.     0.0171]\n",
      "fre_max at spot\n",
      "0.9829059829059829\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.43992701522071376 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.8187495626085872 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43992701522071376 and Equi_Max = 0.44755568104164345\n",
      "Game 117\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4475338224003227 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4475338224003227 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9831 0.     0.     0.     0.0169]\n",
      "fre_max at spot\n",
      "0.9830508474576272\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4399699915324631 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.8042792235610718 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4399699915324631 and Equi_Max = 0.4475338224003227\n",
      "Game 118\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44751233424444814 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44751233424444814 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9832 0.     0.     0.     0.0168]\n",
      "fre_max at spot\n",
      "0.9831932773109243\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.44001224555325846 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.7900520834891451 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44001224555325846 and Equi_Max = 0.44751233424444814\n",
      "Game 119\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4474912072340504 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4474912072340504 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9833 0.     0.     0.     0.0167]\n",
      "fre_max at spot\n",
      "0.9833333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.440053795340374 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.7760620624184171 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.440053795340374 and Equi_Max = 0.4474912072340504\n",
      "Game 120\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44747043234049266 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44747043234049266 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9835 0.     0.     0.     0.0165]\n",
      "fre_max at spot\n",
      "0.9834710743801653\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44009465835414874 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.762303281365387 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44009465835414874 and Equi_Max = 0.44747043234049266\n",
      "Game 121\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44745000083360525 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44745000083360525 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9836 0.     0.     0.     0.0164]\n",
      "fre_max at spot\n",
      "0.9836065573770492\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44013485148245174 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.7487700541001114 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44013485148245174 and Equi_Max = 0.44745000083360525\n",
      "Game 122\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4474299042694538 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4474299042694538 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9837 0.     0.     0.     0.0163]\n",
      "fre_max at spot\n",
      "0.983739837398374\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4401743910639531 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.7354568793106944 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4401743910639531 and Equi_Max = 0.4474299042694538\n",
      "Game 123\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4474101344787031 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4474101344787031 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9839 0.     0.     0.     0.0161]\n",
      "fre_max at spot\n",
      "0.9838709677419355\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.440213292910269 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.7223584331469124 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.440213292910269 and Equi_Max = 0.4474101344787031\n",
      "Game 124\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4473906835555451 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4473906835555451 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.984 0.    0.    0.    0.016]\n",
      "fre_max at spot\n",
      "0.984\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4402515723270438 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.7094695621217515 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4402515723270438 and Equi_Max = 0.4473906835555451\n",
      "Game 125\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4473715438471577 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4473715438471577 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9841 0.     0.     0.     0.0159]\n",
      "fre_max at spot\n",
      "0.9841269841269841\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.44028924413402853 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.6967852763509579 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44028924413402853 and Equi_Max = 0.4473715438471577\n",
      "Game 126\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44735270794366544 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44735270794366544 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9843 0.     0.     0.     0.0157]\n",
      "fre_max at spot\n",
      "0.984251968503937\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44032632268421035 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.684300743111988 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44032632268421035 and Equi_Max = 0.44735270794366544\n",
      "Game 127\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44733416866857445 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44733416866857445 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9844 0.     0.     0.     0.0156]\n",
      "fre_max at spot\n",
      "0.984375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4403628218820456 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.6720112807048773 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4403628218820456 and Equi_Max = 0.44733416866857445\n",
      "Game 128\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44731591906965684 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44731591906965684 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9845 0.     0.     0.     0.0155]\n",
      "fre_max at spot\n",
      "0.9844961240310077\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4403987552008446 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.6599123525986517 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4403987552008446 and Equi_Max = 0.44731591906965684\n",
      "Game 129\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4472979524102574 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4472979524102574 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9846 0.     0.     0.     0.0154]\n",
      "fre_max at spot\n",
      "0.9846153846153847\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44043413569935447 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.6479995618479064 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44043413569935447 and Equi_Max = 0.4472979524102574\n",
      "Game 130\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4472802621610024 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4472802621610024 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9847 0.     0.     0.     0.0153]\n",
      "fre_max at spot\n",
      "0.9847328244274809\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4404689760375816 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.6362686457651114 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4404689760375816 and Equi_Max = 0.4472802621610024\n",
      "Game 131\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4472628419918888 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4472628419918888 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9848 0.     0.     0.     0.0152]\n",
      "fre_max at spot\n",
      "0.9848484848484849\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4405032884918963 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.6247154708350864 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4405032884918963 and Equi_Max = 0.4472628419918888\n",
      "Game 132\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44724568576473145 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44724568576473145 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.985 0.    0.    0.    0.015]\n",
      "fre_max at spot\n",
      "0.9849624060150376\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4405370849694544 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.613336027858896 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4405370849694544 and Equi_Max = 0.44724568576473145\n",
      "Game 133\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4472287875259524 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4472287875259524 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9851 0.     0.     0.     0.0149]\n",
      "fre_max at spot\n",
      "0.9850746268656716\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4405703770219742 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.602126427315186 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4405703770219742 and Equi_Max = 0.4472287875259524\n",
      "Game 134\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4472121414996925 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4472121414996925 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9852 0.     0.     0.     0.0148]\n",
      "fre_max at spot\n",
      "0.9851851851851852\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44060317585890124 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.5910828949276796 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44060317585890124 and Equi_Max = 0.4472121414996925\n",
      "Game 135\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.447195742081229 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.447195742081229 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9853 0.     0.     0.     0.0147]\n",
      "fre_max at spot\n",
      "0.9852941176470589\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44063549235999117 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.5802017674282243 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44063549235999117 and Equi_Max = 0.447195742081229\n",
      "Game 136\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44717958383068407 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44717958383068407 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9854 0.     0.     0.     0.0146]\n",
      "fre_max at spot\n",
      "0.9854014598540146\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44066733708734246 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.5694794885054035 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44066733708734246 and Equi_Max = 0.44717958383068407\n",
      "Game 137\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44716366146700837 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44716366146700837 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9855 0.     0.     0.     0.0145]\n",
      "fre_max at spot\n",
      "0.9855072463768116\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44069872029690604 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.5589126049292905 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44069872029690604 and Equi_Max = 0.44716366146700837\n",
      "Game 138\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4471479698622266 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4471479698622266 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9856 0.     0.     0.     0.0144]\n",
      "fre_max at spot\n",
      "0.9856115107913669\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4407296519494975 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.548497762843481 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4407296519494975 and Equi_Max = 0.4471479698622266\n",
      "Game 139\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4471325040359308 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4471325040359308 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9857 0.     0.     0.     0.0143]\n",
      "fre_max at spot\n",
      "0.9857142857142858\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44076014172133765 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.5382317042160403 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44076014172133765 and Equi_Max = 0.4471325040359308\n",
      "Game 140\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4471172591500107 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4471172591500107 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9858 0.     0.     0.     0.0142]\n",
      "fre_max at spot\n",
      "0.9858156028368794\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4407901990141446 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.528111263441471 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4407901990141446 and Equi_Max = 0.4471172591500107\n",
      "Game 141\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4471022305036073 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4471022305036073 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9859 0.     0.     0.     0.0141]\n",
      "fre_max at spot\n",
      "0.9859154929577465\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44081983296479943 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.518133364086262 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44081983296479943 and Equi_Max = 0.4471022305036073\n",
      "Game 142\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44708741352827996 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44708741352827996 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.986 0.    0.    0.    0.014]\n",
      "fre_max at spot\n",
      "0.986013986013986\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4408490524546058 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.5082950157709858 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4408490524546058 and Equi_Max = 0.44708741352827996\n",
      "Game 143\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4470728037833767 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4470728037833767 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9861 0.     0.     0.     0.0139]\n",
      "fre_max at spot\n",
      "0.9861111111111112\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.440877866118165 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.4985933111823107 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.440877866118165 and Equi_Max = 0.4470728037833767\n",
      "Game 144\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4470583969515972 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4470583969515972 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9862 0.     0.     0.     0.0138]\n",
      "fre_max at spot\n",
      "0.9862068965517241\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44090628235188184 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.489025423208652 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44090628235188184 and Equi_Max = 0.4470583969515972\n",
      "Game 145\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44704418883473873 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44704418883473873 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9863 0.     0.     0.     0.0137]\n",
      "fre_max at spot\n",
      "0.9863013698630136\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.44093430932212324 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.4795886021935363 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44093430932212324 and Equi_Max = 0.44704418883473873\n",
      "Game 146\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.447030175349618 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.447030175349618 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9864 0.     0.     0.     0.0136]\n",
      "fre_max at spot\n",
      "0.9863945578231292\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4409619549730415 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.4702801733010755 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4409619549730415 and Equi_Max = 0.447030175349618\n",
      "Game 147\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44701635252415883 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44701635252415883 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9865 0.     0.     0.     0.0135]\n",
      "fre_max at spot\n",
      "0.9864864864864865\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44098922703408266 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.4610975339882424 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44098922703408266 and Equi_Max = 0.44701635252415883\n",
      "Game 148\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44700271649363826 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44700271649363826 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9866 0.     0.     0.     0.0134]\n",
      "fre_max at spot\n",
      "0.9865771812080537\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4410161330271903 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.4520381515789373 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Reached Nash Equilibrium at Equi_Min = 0.4410161330271903 and Equi_Max = 0.44700271649363826\n",
      "Game 149\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44698926349708445 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44698926349708445 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9867 0.     0.     0.     0.0133]\n",
      "fre_max at spot\n",
      "0.9866666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4410426802737231 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.4430995609350896 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4410426802737231 and Equi_Max = 0.44698926349708445\n",
      "Game 150\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4469759898738181 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4469759898738181 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9868 0.     0.     0.     0.0132]\n",
      "fre_max at spot\n",
      "0.9867549668874173\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44106887590109667 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.4342793622202996 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44106887590109667 and Equi_Max = 0.4469759898738181\n",
      "Game 151\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4469628920601313 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4469628920601313 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9868 0.     0.     0.     0.0132]\n",
      "fre_max at spot\n",
      "0.9868421052631579\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44109472684916257 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.4255752187517565 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44109472684916257 and Equi_Max = 0.4469628920601313\n",
      "Game 152\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4469499665860984 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4469499665860984 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9869 0.     0.     0.     0.0131]\n",
      "fre_max at spot\n",
      "0.9869281045751634\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44112023987633875 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.4169848549363973 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44112023987633875 and Equi_Max = 0.4469499665860984\n",
      "Game 153\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44693721007251025 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44693721007251025 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.987 0.    0.    0.    0.013]\n",
      "fre_max at spot\n",
      "0.987012987012987\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4411454215654997 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.4085060542874712 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4411454215654997 and Equi_Max = 0.44693721007251025\n",
      "Game 154\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4469246192279298 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4469246192279298 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9871 0.     0.     0.     0.0129]\n",
      "fre_max at spot\n",
      "0.9870967741935484\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4411702783296391 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.400136657517886 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4411702783296391 and Equi_Max = 0.4469246192279298\n",
      "Game 155\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4469121908458601 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4469121908458601 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9872 0.     0.     0.     0.0128]\n",
      "fre_max at spot\n",
      "0.9871794871794872\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44119481641731534 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.3918745607068852 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44119481641731534 and Equi_Max = 0.4469121908458601\n",
      "Game 156\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.446899921802022 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.446899921802022 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9873 0.     0.     0.     0.0127]\n",
      "fre_max at spot\n",
      "0.9872611464968153\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4412190419178873 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.3837177135367893 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4412190419178873 and Equi_Max = 0.446899921802022\n",
      "Game 157\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.446887809051736 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.446887809051736 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9873 0.     0.     0.     0.0127]\n",
      "fre_max at spot\n",
      "0.9873417721518988\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44124296076655334 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.3756641175966942 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44124296076655334 and Equi_Max = 0.446887809051736\n",
      "Game 158\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44687584962740295 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44687584962740295 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9874 0.     0.     0.     0.0126]\n",
      "fre_max at spot\n",
      "0.9874213836477987\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44126657874919845 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.3677118247501854 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44126657874919845 and Equi_Max = 0.44687584962740295\n",
      "Game 159\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44686404063608043 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44686404063608043 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9875 0.     0.     0.     0.0125]\n",
      "fre_max at spot\n",
      "0.9875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44128990150706043 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.3598589355642576 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44128990150706043 and Equi_Max = 0.44686404063608043\n",
      "Game 160\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4468523792571494 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4468523792571494 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9876 0.     0.     0.     0.0124]\n",
      "fre_max at spot\n",
      "0.9875776397515528\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4413129345412223 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.3521035977967888 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4413129345412223 and Equi_Max = 0.4468523792571494\n",
      "Game 161\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44684086274006846 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44684086274006846 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9877 0.     0.     0.     0.0123]\n",
      "fre_max at spot\n",
      "0.9876543209876543\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.4999999999999998 0.44133568321693767 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.3444440049400295 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.4999999999999998\n",
      "pop\n",
      "Same history\n",
      "('1', '0.4999999999999998')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44133568321693767 and Equi_Max = 0.44684086274006846\n",
      "Game 162\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4468294884022108 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4468294884022108 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9877 0.     0.     0.     0.0123]\n",
      "fre_max at spot\n",
      "0.9877300613496932\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4413581527677977 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.336878394817709 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4413581527677977 and Equi_Max = 0.4468294884022108\n",
      "Game 163\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4468182536267808 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4468182536267808 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9878 0.     0.     0.     0.0122]\n",
      "fre_max at spot\n",
      "0.9878048780487805\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44138034829974476 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.3294050482334656 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44138034829974476 and Equi_Max = 0.4468182536267808\n",
      "Game 164\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44680715586080727 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44680715586080727 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9879 0.     0.     0.     0.0121]\n",
      "fre_max at spot\n",
      "0.9878787878787879\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44140227479494104 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.3220222876684251 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44140227479494104 and Equi_Max = 0.44680715586080727\n",
      "Game 165\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44679619261320913 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44679619261320913 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.988 0.    0.    0.    0.012]\n",
      "fre_max at spot\n",
      "0.9879518072289156\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44142393711549627 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.314728476025855 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44142393711549627 and Equi_Max = 0.44679619261320913\n",
      "Game 166\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4467853614529315 0.44499822000711975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4467853614529315 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.988 0.    0.    0.    0.012]\n",
      "fre_max at spot\n",
      "0.9880239520958084\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4414453400070629 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.3075220154209204 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4414453400070629 and Equi_Max = 0.4467853614529315\n",
      "Game 167\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44677466000714816 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44677466000714816 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9881 0.     0.     0.     0.0119]\n",
      "fre_max at spot\n",
      "0.9880952380952381\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4414664881023013 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.3004013460136634 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4414664881023013 and Equi_Max = 0.44677466000714816\n",
      "Game 168\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44676408595952893 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44676408595952893 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9882 0.     0.     0.     0.0118]\n",
      "fre_max at spot\n",
      "0.9881656804733728\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4414873859242234 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.2933649448834157 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4414873859242234 and Equi_Max = 0.44676408595952893\n",
      "Game 169\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4467536370485679 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4467536370485679 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9882 0.     0.     0.     0.0118]\n",
      "fre_max at spot\n",
      "0.9882352941176471\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4415080378894169 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2864113249429356 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4415080378894169 and Equi_Max = 0.4467536370485679\n",
      "Game 170\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4467433110659712 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4467433110659712 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9883 0.     0.     0.     0.0117]\n",
      "fre_max at spot\n",
      "0.9883040935672515\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4415284483111578 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.279539033890648 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4415284483111578 and Equi_Max = 0.4467433110659712\n",
      "Game 171\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44673310585510073 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44673310585510073 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9884 0.     0.     0.     0.0116]\n",
      "fre_max at spot\n",
      "0.9883720930232558\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4415486214024134 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2727466531994338 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4415486214024134 and Equi_Max = 0.44673310585510073\n",
      "Game 172\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4467230193094729 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4467230193094729 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9884 0.     0.     0.     0.0116]\n",
      "fre_max at spot\n",
      "0.9884393063583815\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44156856127874117 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2660327971404877 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44156856127874117 and Equi_Max = 0.4467230193094729\n",
      "Game 173\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44671304937130907 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44671304937130907 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9885 0.     0.     0.     0.0115]\n",
      "fre_max at spot\n",
      "0.9885057471264368\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4415882719610882 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2593961118408399 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4415882719610882 and Equi_Max = 0.44671304937130907\n",
      "Game 174\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4467031940301356 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4467031940301356 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9886 0.     0.     0.     0.0114]\n",
      "fre_max at spot\n",
      "0.9885714285714285\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44160775737849406 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.252835274373188 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44160775737849406 and Equi_Max = 0.4467031940301356\n",
      "Game 175\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44669345132143257 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44669345132143257 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9886 0.     0.     0.     0.0114]\n",
      "fre_max at spot\n",
      "0.9886363636363636\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44162702137070214 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2463489918767598 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44162702137070214 and Equi_Max = 0.44669345132143257\n",
      "Game 176\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4466838193253286 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4466838193253286 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9887 0.     0.     0.     0.0113]\n",
      "fre_max at spot\n",
      "0.9887005649717514\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44164606769068193 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2399360007079745 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44164606769068193 and Equi_Max = 0.4466838193253286\n",
      "Game 177\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44667429616533866 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44667429616533866 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9888 0.     0.     0.     0.0112]\n",
      "fre_max at spot\n",
      "0.9887640449438202\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44166490000706643 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2335950656197376 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44166490000706643 and Equi_Max = 0.44667429616533866\n",
      "Game 178\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44666488000714644 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44666488000714644 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9888 0.     0.     0.     0.0112]\n",
      "fre_max at spot\n",
      "0.9888268156424581\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44168352190650806 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2273249789682406 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44168352190650806 and Equi_Max = 0.44666488000714644\n",
      "Game 179\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4466555690574256 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4466555690574256 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9889 0.     0.     0.     0.0111]\n",
      "fre_max at spot\n",
      "0.9888888888888889\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4417019368959559 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2211245599462046 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4417019368959559 and Equi_Max = 0.4466555690574256\n",
      "Game 180\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4466463615627016 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4466463615627016 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.989 0.    0.    0.    0.011]\n",
      "fre_max at spot\n",
      "0.988950276243094\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44172014840485735 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2149926538415392 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44172014840485735 and Equi_Max = 0.4466463615627016\n",
      "Game 181\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4466372558082509 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4466372558082509 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.989 0.    0.    0.    0.011]\n",
      "fre_max at spot\n",
      "0.989010989010989\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44173815978728737 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2089281313204419 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44173815978728737 and Equi_Max = 0.4466372558082509\n",
      "Game 182\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44662825011703594 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44662825011703594 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9891 0.     0.     0.     0.0109]\n",
      "fre_max at spot\n",
      "0.9890710382513661\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44175597432400776 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.2029298877340011 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44175597432400776 and Equi_Max = 0.44662825011703594\n",
      "Game 183\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44661934284867577 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44661934284867577 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9891 0.     0.     0.     0.0109]\n",
      "fre_max at spot\n",
      "0.9891304347826086\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4417735952244594 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1969968424474127 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4417735952244594 and Equi_Max = 0.44661934284867577\n",
      "Game 184\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4466105323984499 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4466105323984499 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9892 0.     0.     0.     0.0108]\n",
      "fre_max at spot\n",
      "0.9891891891891892\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44179102562869005 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.19112793819095 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44179102562869005 and Equi_Max = 0.4466105323984499\n",
      "Game 185\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44660181719633457 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44660181719633457 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9892 0.     0.     0.     0.0108]\n",
      "fre_max at spot\n",
      "0.989247311827957\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44180826860921923 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1853221404318686 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44180826860921923 and Equi_Max = 0.44660181719633457\n",
      "Game 186\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44659319570607003 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44659319570607003 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9893 0.     0.     0.     0.0107]\n",
      "fre_max at spot\n",
      "0.9893048128342246\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44182532717284434 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1795784367664666 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44182532717284434 and Equi_Max = 0.44659319570607003\n",
      "Game 187\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4465846664242574 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4465846664242574 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9894 0.     0.     0.     0.0106]\n",
      "fre_max at spot\n",
      "0.9893617021276596\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4418422042623884 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1738958363315481 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4418422042623884 and Equi_Max = 0.4465846664242574\n",
      "Game 188\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4465762278794854 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4465762278794854 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9894 0.     0.     0.     0.0106]\n",
      "fre_max at spot\n",
      "0.9894179894179894\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4418589027583923 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1682733692345653 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4418589027583923 and Equi_Max = 0.4465762278794854\n",
      "Game 189\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44656787863148345 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44656787863148345 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9895 0.     0.     0.     0.0105]\n",
      "fre_max at spot\n",
      "0.9894736842105263\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.441875425480754 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1627100860017612 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.441875425480754 and Equi_Max = 0.44656787863148345\n",
      "Game 190\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4465596172703027 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4465596172703027 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9895 0.     0.     0.     0.0105]\n",
      "fre_max at spot\n",
      "0.9895287958115183\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44189177519031614 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.157205057043646 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44189177519031614 and Equi_Max = 0.4465596172703027\n",
      "Game 191\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4465514424155216 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4465514424155216 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9896 0.     0.     0.     0.0104]\n",
      "fre_max at spot\n",
      "0.9895833333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44190795459040366 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1517573721371779 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44190795459040366 and Equi_Max = 0.4465514424155216\n",
      "Game 192\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44654335271547785 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44654335271547785 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9896 0.     0.     0.     0.0104]\n",
      "fre_max at spot\n",
      "0.9896373056994818\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44192396632831404 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1463661399240415 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44192396632831404 and Equi_Max = 0.44654335271547785\n",
      "Game 193\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4465353468465226 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4465353468465226 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9897 0.     0.     0.     0.0103]\n",
      "fre_max at spot\n",
      "0.9896907216494846\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44193981299676155 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.141030487424442 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44193981299676155 and Equi_Max = 0.4465353468465226\n",
      "Game 194\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44652742351229885 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44652742351229885 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9897 0.     0.     0.     0.0103]\n",
      "fre_max at spot\n",
      "0.9897435897435898\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4419554971352762 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1357495595658642 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4419554971352762 and Equi_Max = 0.44652742351229885\n",
      "Game 195\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44651958144304155 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44651958144304155 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9898 0.     0.     0.     0.0102]\n",
      "fre_max at spot\n",
      "0.9897959183673469\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4419710212315611 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1305225187262515 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4419710212315611 and Equi_Max = 0.44651958144304155\n",
      "Game 196\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4465118193948991 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4465118193948991 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9898 0.     0.     0.     0.0102]\n",
      "fre_max at spot\n",
      "0.9898477157360406\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44198638772280757 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.125348544291102 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44198638772280757 and Equi_Max = 0.4465118193948991\n",
      "Game 197\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4465041361492758 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4465041361492758 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9899 0.     0.     0.     0.0101]\n",
      "fre_max at spot\n",
      "0.98989898989899\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44200159899697083 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.75 1.1202268322239843 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44200159899697083 and Equi_Max = 0.4465041361492758\n",
      "Game 198\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.44649653051219423 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.44649653051219423 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.9899 0.     0.     0.     0.0101]\n",
      "fre_max at spot\n",
      "0.9899497487437185\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4420166573940067 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.1151565946500033 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4420166573940067 and Equi_Max = 0.44649653051219423\n",
      "Game 199\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4464890013136763 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4464890013136763 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.   0.99 0.   0.   0.   0.01]\n",
      "fre_max at spot\n",
      "0.99\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.44203156520707226 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7499999999999999 1.110137059451762 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44203156520707226 and Equi_Max = 0.4464890013136763\n",
      "Game 200\n",
      "_____________________\n",
      "Maximizer start from agent0\n",
      "changed_opinion, por, minup_por\n",
      "1 0.4464815474071435 0.44499822000711975\n",
      "Maximizer start from agent2\n",
      "changed_opinion, por, minup_por\n",
      "0 0.4464815474071435 0.44499822000711975\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.   0.99 0.   0.   0.   0.01]\n",
      "fre_max at spot\n",
      "0.9900497512437811\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Check if op has been updated by Maximizer\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Min start with agent 1\n",
      "Min Payoff Row\n",
      "[  0.1483   0.445  100.     100.       0.445    0.1483]\n",
      "changed opinion, por, min_por\n",
      "0.49999999999999983 0.4420463246836895 0.4449982200071198\n",
      "Min start with agent 2\n",
      "Min Payoff Row\n",
      "[2.5958e-01 1.1125e-01 4.8208e-01 3.7083e-02 1.0000e+02 1.0000e+02]\n",
      "changed opinion, por, min_por\n",
      "0.7500000000000001 1.1051674698773841 0.4449982200071198\n",
      "Innate polarization is smaller than Min action\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0.49999999999999983\n",
      "pop\n",
      "Same history\n",
      "('1', '0.49999999999999983')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4420463246836895 and Equi_Max = 0.4464815474071435\n",
      "Game 201\n",
      "_____________________\n",
      "min_recent_1_touched\n",
      "[1]\n",
      "max_recent_1_touched\n",
      "[0]\n",
      "Min last 100 action\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "(First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100) = all_fre_limited_touch(s, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005  0.005  0.1343 0.8458 0.01  ]\n",
      "[1]\n",
      "[(1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.4999999999999998), (1, 0.49999999999999983), (1, 0.4999999999999998), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.4999999999999998), (1, 0.49999999999999983), (1, 0.4999999999999998), (1, 0.4999999999999998), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.4999999999999998), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.4999999999999998), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.4999999999999998), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983), (1, 0.49999999999999983)]\n"
     ]
    }
   ],
   "source": [
    "print(fla_min_fre)\n",
    "print(min_touched)\n",
    "print(min_history_last_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000e+00  5.9333e-01 -1.0000e+02 -1.0000e+02  5.9333e-01  5.9333e-01]\n",
      " [ 5.9333e-01  5.9333e-01 -1.0000e+02 -1.0000e+02  5.9333e-01  6.5820e-32]\n",
      " [ 1.4833e-01  4.4500e-01 -1.0000e+02 -1.0000e+02  4.4500e-01  1.4833e-01]\n",
      " [ 1.4833e-01  4.4500e-01 -1.0000e+02 -1.0000e+02  4.4500e-01  1.4833e-01]\n",
      " [ 1.4833e-01  4.4500e-01 -1.0000e+02 -1.0000e+02  4.4500e-01  1.4833e-01]]\n",
      "[   0.445     0.1483    0.5933    0.     -100.     -100.    ]\n",
      "[[0.   0.02 0.02]\n",
      " [0.02 0.   0.02]\n",
      " [0.02 0.02 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)\n",
    "print(payoff_row)\n",
    "# print(First_max)\n",
    "# print(First_min)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment = 0\n",
    "\n",
    "# Experiment_note = str('Note: This experiement has initial condition. Game round:'+str(Game_rounds)+'.')\n",
    "# (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100) = all_fre_limited_touch(s, n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_distribution\n",
      "[1.]\n",
      "[[  0. 100.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]]\n",
      "Min_distribution\n",
      "dict_keys([(1, 0.49999999999999983), (1, 0.4999999999999998)])\n",
      "fla_min_fre\n",
      "[0.92 0.08]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Counter({(1, 0.49999999999999983): 170, (1, 0.4999999999999998): 27, (1, 0.4999999999999999): 2, (1, 0): 1, (1, 0.9999999999999997): 1})\n",
      "fla_min_fre_1\n",
      "[0.005  0.005  0.1343 0.8458 0.01  ]\n"
     ]
    }
   ],
   "source": [
    "# MAXimizer's distribution of LAST 100 iteration \n",
    "print('Max_distribution')  \n",
    "max_l100_fre = max_history_last_100/100\n",
    "print(max_l100_fre [np.nonzero(max_l100_fre)])\n",
    "# print for small network\n",
    "print(max_history_last_100)\n",
    "# # Print for Large Network\n",
    "# print(np.nonzero(max_l100_fre))\n",
    "\n",
    "# MINimizer's Strategy in the last 100 round\n",
    "print('Min_distribution')\n",
    "counter=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter.keys())\n",
    "fla_min_fre = np.array(list(counter.values()))/(100) #return only frequency of all min options in order\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)\n",
    "\n",
    "print(min_touched_last_100)\n",
    "counter_1=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_1)\n",
    "fla_min_fre_1 = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "print('fla_min_fre_1')\n",
    "print(fla_min_fre_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    0.593 0.148 0.445]\n",
      " [0.445 0.148 0.593 0.    0.    0.   ]\n",
      " [0.148 0.148 0.445 0.148 0.    0.   ]\n",
      " [0.    0.    0.148 0.445 0.148 0.148]] 1\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment = 'Pure11'\n",
    "with open('Result'+str(Network)+'.'+str(Experiment)+'.txt', \"a\") as f:\n",
    "#     print(Experiment_note, file=f)\n",
    "    print('Initial Condition -(agent, opinion, pol)', file=f)\n",
    "    print('Innate op'+str(s),file=f)\n",
    "    print('Adjacency matrix'+ str(G), file=f)\n",
    "    print('Max:'+ str(First_max), file=f)\n",
    "    print('Min' + str(First_min), file=f)\n",
    "\n",
    "    print(\"In the Last 100 Rounds\", file=f) \n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # MAX distribution of LAST 100 iteration \n",
    "    print('Max_distribution', file=f)  \n",
    "    max_l100_fre = max_history_last_100/100\n",
    "    print(max_l100_fre [np.nonzero(max_l100_fre)], file=f)\n",
    "    # print for small network\n",
    "    print(max_history_last_100, file=f)\n",
    "    # # Print for Large Network\n",
    "    # print(np.nonzero(max_l100_fre))\n",
    "\n",
    "    # MIN Strategy in the last 100 round\n",
    "    counter=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    # print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/100 #return only frequency of all min options in order\n",
    "    print('Min_frequency', file=f)\n",
    "    print(list(counter.keys()), file=f)\n",
    "    print('Min_distribution_last_100', file=f)\n",
    "    print(fla_min_fre, file=f)\n",
    "    counter_h=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_h.keys(), file=f)\n",
    "    \n",
    "    print('min_recent_'+str(memory)+'_touched', file=f)# then stop at Game 202\n",
    "    print(min_touched, file=f)\n",
    "    print('max_recent_'+str(memory)+'_touched', file=f)\n",
    "    print(max_touched, file=f)\n",
    "    \n",
    "    print('In Overall'+str(Game_rounds)+' Rounds', file=f)\n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # Max action Overall \n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    max_fre = max_history/Game_rounds\n",
    "    print('Max_frequency', file=f)\n",
    "    print(max_history, file=f)\n",
    "    print('Max_distribution', file=f)\n",
    "    print(max_fre [np.nonzero(max_fre)], file=f)\n",
    "    \n",
    "\n",
    "\n",
    "    # Min Strategy in the Overall    \n",
    "    counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "    fla_min_fre_all = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "    print('Min_dist_all', file=f)\n",
    "    print(fla_min_fre_all, file=f)\n",
    "    print('Min_distribution', file=f)\n",
    "    counter_a=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_a.keys(), file=f)\n",
    "    print(payoff_matrix, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(1, 0.49999999999999983): 366, (1, 0.4999999999999998): 31, (1, 0.4999999999999999): 2, (1, 0): 1, (1, 0.9999999999999997): 1})\n",
      "fla_min_fre\n",
      "[0.002 0.002 0.077 0.913 0.005]\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(min_history) \n",
    "print(counter)\n",
    "fla_min_fre = np.array(list(counter.values()))/Game_rounds\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Game Parameters\n",
    "# Game_rounds =801 # Rounds + 1- use for printing data\n",
    "# memory = 1\n",
    "\n",
    "# # def all_fre_limited_touch(s, n):\n",
    "# #     # Preparation for the game\n",
    "# op = copy.copy(s)\n",
    "# payoff_matrix = np.empty((0, 2*n), float)\n",
    "# max_history = np.zeros([n, 2])  # n*2 matrix, agent i & opinion options\n",
    "# min_history = []  # append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "# print(type(min_history))\n",
    "\n",
    "# max_history_last_100 = np.zeros([n, 2]) \n",
    "# min_history_last_100= []\n",
    "\n",
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# min_touched_all = []\n",
    "# min_touched_last_100 =[]\n",
    "# print('min_touched')\n",
    "# print(min_touched)\n",
    "\n",
    "\n",
    "# # Game start from maximizer random play\n",
    "# print('Maximizer first selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action \n",
    "# #(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "# # (v1, max_opinion, max_pol) = (1, 1, 0.6163708086785011)\n",
    "# First_max = (v1, max_opinion, max_pol) \n",
    "\n",
    "\n",
    "# #     (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,max_touched)\n",
    "\n",
    "# # Maximizer start with greedy play\n",
    "# # (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "# max_touched.append(v1)    # save Maximizer's action history\n",
    "\n",
    "# # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "# max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "# # print('max_history')\n",
    "# # print(max_history)\n",
    "# print('history at spot')\n",
    "# print(max_history[v1,int(max_opinion)])\n",
    "\n",
    "# max_frequency = max_history/1  # its frequency, only played  1 time so far, divided by 1 \n",
    "# # print('fre_max at spot')\n",
    "# # print(max_frequency[v1,int(max_opinion)])\n",
    "\n",
    "# fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "#                                         # so we can multiply the freuency (2n*1)with payoff array (1*2n) \n",
    "#                                         # to get average payoff of fictitious play\n",
    "# print('fre_max at spot')\n",
    "# print(fla_max_fre)\n",
    "\n",
    "# column = int(column_index(v1,max_opinion))    # the frequency of maximizer's most recent action (v1,max_opinion)\n",
    "\n",
    "# print(fla_max_fre[column])\n",
    "\n",
    "# # print(np.shape(fla_max_fre.shape))\n",
    "\n",
    "\n",
    "# # if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "# print('Minimizer first selection')\n",
    "# (v2, min_opinion, min_pol) = random_play(op,n) \n",
    "# #(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "\n",
    "# # (v2, min_opinion, min_pol) = (0, 0, 0.15409270216962534)\n",
    "# First_min = (v2, min_opinion, min_pol)\n",
    "\n",
    "# if v1==v2:   # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "#     sys.exit()\n",
    "\n",
    "# # Minimizer start with greedy play\n",
    "# # (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "\n",
    "# min_touched.append(v2)\n",
    "\n",
    "\n",
    "# # store minimizer play history\n",
    "# min_history.append((v2,min_opinion))\n",
    "# print('min_history')\n",
    "# print(min_history)\n",
    "\n",
    "\n",
    "# counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "# print(counter)\n",
    "# fla_min_fre = np.array(list(counter.values()))/1 #return only frequency of all min options in order\n",
    "# print('fla_min_fre')\n",
    "# print(fla_min_fre)\n",
    "\n",
    "\n",
    "# (a,payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "# payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "# print('Payoff Matrix')\n",
    "# print(payoff_matrix)\n",
    "# # print('fla_min_fre at the spot')\n",
    "# # min_counter = dict(counter)\n",
    "# # print(min_counter) \n",
    "# # print(min_counter[(v2,min_opinion)]) \n",
    "# # print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "\n",
    "# equi_min = min_pol\n",
    "# equi_max = max_pol\n",
    "# # print(equi_min)\n",
    "# # print(equi_max)\n",
    "\n",
    "\n",
    "\n",
    "# Flag = 0\n",
    "\n",
    "# i = 0\n",
    "# while Flag == 0: \n",
    "#     i = i + 1\n",
    "#     print(\"Game \" + str(i))\n",
    "#     print(\"_____________________\")\n",
    "\n",
    "# #     if max_pol == min_pol:\n",
    "#     if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "#                             # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "#         print('min_recent_'+str(memory)+'_touched')# then stop at Game 202\n",
    "#         print(min_touched)\n",
    "#         print('max_recent_'+str(memory)+'_touched')\n",
    "#         print(max_touched)\n",
    "#         print('Min last 100 action')\n",
    "#         print(min_touched_last_100)\n",
    "\n",
    "#         break\n",
    "\n",
    "#     elif equi_min == equi_max:\n",
    "#         print(\"Reached Nash Equilibrium at game\"+ str(i) + \"and Equi_Por = \" + str(equi_min))\n",
    "#         print('max_distribution')\n",
    "#         print(max_frequency)\n",
    "#         print('min_distribution')\n",
    "#         print(fla_min_fre)\n",
    "#         Flag = 1\n",
    "#         break\n",
    "#     ############################## maximizer play  \n",
    "#     else:\n",
    "#         if i == Game_rounds-100:    #if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "# #             max_touched_100 = max_touched \n",
    "# #             min_touched_100 = min_touched\n",
    "# #             max_fre_100 = max_frequency  # store the max_frequency of first 100 iterataions\n",
    "# #             print('max_history')\n",
    "# #             print(max_history)\n",
    "# #             min_fre_100 = fla_min_fre  # max_frequency of first 100 iterations\n",
    "# #             print('min_history')\n",
    "# #             print(min_history)\n",
    "# # Remove max frequncy less than 0.1--\n",
    "#             max_history_last_100 = np.zeros([n, 2]) \n",
    "#             min_history_last_100 = [] \n",
    "#             min_touched_last_100 =[]\n",
    "#         print('Max_Payoff_Matrix')\n",
    "#         print(payoff_matrix)\n",
    "#         (v1, max_opinion, equi_max) = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "#         max_touched = push(max_touched, v1)\n",
    "# #         print('min_touched')\n",
    "# #         print(min_touched)\n",
    "# #         print('max_touched')\n",
    "# #         print(max_touched)\n",
    "# #             print('equi_max')\n",
    "# #             print(equi_max)\n",
    "# #         print(v1, max_opinion, max_pol)\n",
    "#         # cumulate strategy \n",
    "#         max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "\n",
    "#         max_history_last_100[v1,int(max_opinion)] = max_history_last_100[v1,int(max_opinion)] +1\n",
    "# #         print('max_history')\n",
    "# #         print(max_history)\n",
    "# #________________________________________________________________\n",
    "#         max_frequency = max_history/(i+1)  # its frequency \n",
    "# #         print('max_distribution')\n",
    "# #         print(max_frequency)\n",
    "#     #     print(i+1) \n",
    "#         fla_max_fre = max_frequency.flatten() #flaten max_frequency to calculate average payoff\n",
    "#         print('fla_max_fre')\n",
    "#         print(fla_max_fre)\n",
    "#         print('fre_max at spot')\n",
    "#         print(fla_max_fre[column])\n",
    "#         # create payoff matrix for maxmizer\n",
    "#         row = int(row_index(v2, min_opinion))\n",
    "#         column = int(column_index(v1,max_opinion))\n",
    "\n",
    "# # _________________________________________________________________\n",
    "# #         ######################Visualize Maximizer's selection\n",
    "# #         La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "# #         nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "# #         color_map = []\n",
    "# #         for node in nxG:\n",
    "# #             if node == v1:\n",
    "# #                 color_map.append('Red')\n",
    "# #             else: \n",
    "# #                 color_map.append('Grey')  \n",
    "\n",
    "# #         #nxG1 = nx.DiGraph(G)\n",
    "# #         nx.draw(nxG, node_color=color_map, with_labels=True,node_size = 50)\n",
    "# #         plt.figure(figsize=(200, 200))\n",
    "# #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ############################### minimizer play\n",
    "#         (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "# #         print('Min_Payoff_row')\n",
    "# #         print(payoff_row)\n",
    "#         min_touched = push(min_touched, v2)\n",
    "#         min_touched_all.append(v2) \n",
    "#         min_touched_last_100.append(v2)\n",
    "# #         print('min_touched')\n",
    "# #         print(min_touched)\n",
    "# #         print('equi_min')\n",
    "# #         print(equi_min)\n",
    "# #         print('max_touched')\n",
    "# #         print(max_touched)\n",
    "#         #         print(v2, min_opinion, min_pol)\n",
    "#         if (v2,min_opinion) in counter.keys():\n",
    "#             payoff_matrix = payoff_matrix # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "#             print(\"Same history\")\n",
    "#             print((str(v2),str(min_opinion)))\n",
    "#         else:\n",
    "#             payoff_matrix = np.vstack([payoff_matrix, payoff_row]) # if this is a new option, append to previous matrix\n",
    "# #                 print('payoff_row')\n",
    "# #                 print(payoff_row)\n",
    "#         min_history.append((v2,min_opinion))\n",
    "#         min_history_last_100.append((v2,min_opinion))\n",
    "#         #         print('min_history')\n",
    "#         #         print(min_history)\n",
    "#         counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "#         #print(counter)\n",
    "# #         print('counter.keys')\n",
    "# #         print(counter.keys())\n",
    "#         fla_min_fre = np.array(list(counter.values()))/(i+1) #return only frequency of all min options in order\n",
    "# #         print('fla_min_fre')\n",
    "# #         print(fla_min_fre)\n",
    "\n",
    "# #         print('fla_min_fre at the spot')\n",
    "# #         min_counter = dict(counter)\n",
    "# #         print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "#         # create payoff matrix for minimizer\n",
    "#         row = row_index(v2, min_opinion)\n",
    "#         column = column_index(v1,max_opinion)\n",
    "#         #     print('row, column')\n",
    "#         #     print(row, column)\n",
    "\n",
    "#         print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max)) \n",
    "# #         print('min_distribution')\n",
    "# #         print(fla_min_fre)\n",
    "\n",
    "#             ######################Visualize Minimizer selection\n",
    "#     #         La = scipy.sparse.csgraph.laplacian(G1, normed=False)\n",
    "\n",
    "#     #         nxG = nx.from_numpy_matrix(G1)\n",
    "\n",
    "#     #         color_map = []\n",
    "#     #         for node in nxG:\n",
    "#     #             if node == v2:\n",
    "#     #                 color_map.append('Blue')\n",
    "#     #             else: \n",
    "#     #                 color_map.append('Grey')  \n",
    "\n",
    "#     #         #nxG1 = nx.DiGraph(G)\n",
    "#     #         nx.draw(nxG, node_color=color_map, with_labels=True)\n",
    "#     #         plt.figure(figsize=(25, 25))\n",
    "#     #         plt.show()\n",
    "#    ## return (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
