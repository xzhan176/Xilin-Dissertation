{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network \n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers the opinion vector around 0\\n\",\n",
    "def mean_center(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return x\n",
    "    \n",
    "# compute number of edges, m\\n\n",
    "def num_edges(L, n):\n",
    "    m = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i > j and L[i,j] < 0:\n",
    "                m += 1            \n",
    "    return m\n",
    "\n",
    "# maximizing polarization only: \\\\bar{z}^T \\\\bar{z}   \n",
    "def obj_polarization(A, L, op, n):\n",
    "    op_mean = mean_center(op, n)\n",
    "    z_mean = np.dot(A, op_mean) \n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "def obj_polarization_1(A, L, op, n):\n",
    "    z = np.dot(A, op) \n",
    "    z_mean = mean_center(z, n)\n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "# Calculate innate polarization\n",
    "def obj_innate_polarization(s, n):  \n",
    "#     np.set_printoptions(precision=5)\n",
    "    op_mean = mean_center(s, n)\n",
    "    return np.dot(np.transpose(op_mean), op_mean)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the network\n",
    "# n = 6   # number of nodes in the network\n",
    "# r = 1/4  # percent of info souce in the network\n",
    "# n1 = int(n*(1-r))\n",
    "n = 3\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network\n",
    "### 1. Make Random Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1\n",
      "[0 1 2]\n",
      "weight1\n",
      "[0.0256 0.0173 0.007 ]\n",
      "G for agents completed!\n",
      "[[0.     0.0256 0.0173]\n",
      " [0.0256 0.     0.007 ]\n",
      " [0.0173 0.007  0.    ]]\n",
      "Column Sum\n",
      "[1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############################ Make Innate Opinion ################################\n",
    "##create two set of weights connected with density 1) inviduals  2) individual & informaton Source\n",
    "c1 = np.sort(np.random.choice(n, n, replace=False)) #assume (1-r) are individuals\n",
    "print('c1')\n",
    "print(c1)\n",
    "l1 = len(c1)\n",
    "\n",
    "def make_innat_opinions(n, c1): # Make opinion for agents only - no info source is involved\n",
    "    \n",
    "    # Make list of ind innate opinion to define info source opinion\n",
    "    innat_s = np.random.uniform(low=0.3, high=0.7, size=int(n))   #individual's innate opinion \n",
    "\n",
    "    s = np.zeros((n, 1))\n",
    "    \n",
    "    idx1 = 0\n",
    "    for i in range(len(s)):\n",
    "        s[i] = innat_s[idx1]  #set innate opinion for ind.\n",
    "        idx1 += 1\n",
    "\n",
    "\n",
    "#     # Make info souce opinion\n",
    "# #     h = np.random.uniform(low=max(s1), high=1.0, size=1)\n",
    "# #     g = np.random.uniform(low=0.0, high=min(s1), size=1)\n",
    "#     h = np.eye(1)\n",
    "#     g = np.zeros((1,1))\n",
    "\n",
    "#     # Make opinion list of both info source and agents   \n",
    "    \n",
    "#     s = np.zeros(((n+2), 1))\n",
    "#     idx1 = 0\n",
    "#     idx2 = 0\n",
    "\n",
    "#     for i in (len(c1)):\n",
    "#         s[i] = s1[idx1]\n",
    "#         idx1 += 1\n",
    "\n",
    "# #     print('s')\n",
    "# #     print(s)\n",
    "  \n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "##################################Creating Adjacency Matrix ########################\n",
    "np.set_printoptions(precision=4)\n",
    "### Prepare for create adjacent matrix\n",
    "p1 = 1 # density within ind.\n",
    "p2 = 0 # density of edges between Info Source and Indivisuals\n",
    "\n",
    "pre_weights1 = scipy.sparse.random(1, int(0.5*l1*(l1 - 1)), density=p1).A[0] \n",
    "weights1 = pre_weights1/25\n",
    "\n",
    "\n",
    "print(\"weight1\")\n",
    "print(weights1)\n",
    "weights1.shape\n",
    "\n",
    "# b = weights2.round()  #generate a binary array to indicate the connection between ind. and inf. source \n",
    "                          #without consider the innate opinions, just based on the edges between info source and ind.\n",
    "\n",
    "    \n",
    "    \n",
    "# create n x n adjacency matrix with existing init_G\n",
    "G = np.zeros((n, n))\n",
    "    \n",
    "## Assign edges between ind to ind \n",
    "idx = 0\n",
    "for i in c1:\n",
    "    for j in c1:\n",
    "            if i == j:\n",
    "                G[i][j] =0\n",
    "                continue\n",
    "            elif i < j:\n",
    "                G[i][j] = weights1[idx]\n",
    "                idx += 1\n",
    "#                 print(idx)\n",
    "#                 print (G1[i][j])\n",
    "            else:\n",
    "                G[i][j] = G[j][i]\n",
    "print(\"G for agents completed!\")\n",
    "print(G)\n",
    "\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "A = np.linalg.inv(np.identity(n) + L)\n",
    "m = num_edges(L, n)\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "print('Column Sum')\n",
    "print(columnsum_ij)\n",
    "# what the twitter graph looks like \n",
    "# nxG = nx.from_numpy_matrix(G)\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# nx.draw(nxG)\n",
    "\n",
    "La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "color_map = []\n",
    "for node in nxG:\n",
    "    if node in c1:\n",
    "        color_map.append('Blue')\n",
    "    else: \n",
    "        color_map.append('Red')  \n",
    "\n",
    "#nxG1 = nx.DiGraph(G)\n",
    "nx.draw(nxG, node_color=color_map, with_labels=False)\n",
    "plt.figure(figsize=(25, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Equilibrium & Polarization  - based on derivation\n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate Opinion\n",
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n",
      "Equilibrium Opinion\n",
      "[[0.9717]\n",
      " [0.5085]\n",
      " [0.0198]]\n",
      "Innate_polarization:\n",
      "0.5\n",
      "Equi_polarization:\n",
      "0.45313944339004775\n",
      "Difference:\n",
      "-0.04686055660995225\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "s_0=[1]\n",
    "s_1=[0.5]\n",
    "s_2=[0]\n",
    "s = np.vstack((s_0,s_1,s_2))\n",
    "\n",
    "\n",
    "# s =  make_innat_opinions(n, c1)\n",
    "print('Innate Opinion')\n",
    "print(s)\n",
    "print('Equilibrium Opinion')\n",
    "print(np.dot(A, s))\n",
    "#s = (1,0.5,0) # polarization = 0.75\n",
    "# s = (1,0,0.5) # polarization = 0.75\n",
    "# s =(0,1,0.5) # polarization = 0.75\n",
    "#s = (0.5,1,0) # polarization = 0\n",
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, op, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing players' behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "  \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "    new_op = random.randint(0, 1)  # randomly select an opininon between 0 and 1\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play1(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "#     max_opi_option = random.uniform(0, 1)   # options that maximizer have\n",
    "    \n",
    "    #v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "    v = 1\n",
    "    #new_op = random.uniform(0, 1)  # randomly select an opininon between 0 and 1\n",
    "    new_op = 0\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# min_touched =[]\n",
    "# (v1, maxmize_op, innat_equi_por, max_por) = choose_max_vertex(s, n, min_touched)\n",
    "# print(v1, maxmize_op, innat_equi_por, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# (v2, min_opinion, min_pol) = random_play(s,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer_fir_play(s,n,min_touched):    # maxmizer first-time play, greedy algorithm\n",
    "    op = copy.copy(s)\n",
    "\n",
    "    print('Maximizer Play')\n",
    "\n",
    "    max_champion = choose_max_vertex(op, n, min_touched) # The best choice among all opinions and vertexs, function is in \"pure_strategy_selection.ipynb\"\n",
    "    (v1, max_opinion, innate_obj, max_pol) = max_champion # find agent v1, and max_opinion that can maxmize the equi_polarization(max_pol)\n",
    "\n",
    "    if v1 == None:   # if maximizer cannot find one\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "#         print('v1', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(max_champion)\n",
    "\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        ##### change the agent's opinion with best action(agent v1, max_op)\n",
    "        op[v1,0] = max_opinion\n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximizer Play\n",
      "Max start with agent 0\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 1\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 2\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 3\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 4\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 5\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 6\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 7\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 8\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "Max start with agent 9\n",
      "change op to 0.0\n",
      "change op to 0.1\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent4 's opinion 0.4197046109204578 changed to 1\n",
      "Network reaches equilibrium Polarization: 0.2986802875362994\n",
      "4 1 0.2986802875362994\n"
     ]
    }
   ],
   "source": [
    "max_touched = []\n",
    "min_touched = []\n",
    "(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "print(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### minimizer first-time play, greedy algorithm\n",
    "def minimizer_fir_play(s,n,max_touched): \n",
    "    \n",
    "    op = copy.copy(s)\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = choose_min_vertex(op, n, max_touched)\n",
    "    (v2, min_opinion, innat_equi_por, min_pol) = min_champion\n",
    "    \n",
    "   #Store innate_op of the min_selected vertex\n",
    "    old_opinion_min = op[v2,0]\n",
    "    \n",
    "    if v2 == None:\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "\n",
    "        ##### change the agent's opinion\n",
    "        op[v2,0] = min_opinion   #-------------------------------------------------> store minimize strategy\n",
    "\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return (v2,min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________\n",
      "Minimizer Play\n",
      "Min start with agent 0\n",
      "Min start with agent 1\n",
      "Min start with agent 2\n",
      "Min start with agent 3\n",
      "Min start with agent 4\n",
      "Min start with agent 5\n",
      "Min start with agent 6\n",
      "Min start with agent 7\n",
      "Min start with agent 8\n",
      "Min start with agent 9\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 0.6592088074131972 changed to 0.40054039677251674\n",
      "Network reaches equilibrium Polarization: 0.043861701295137584\n",
      "0 0.40054039677251674 0.043861701295137584\n"
     ]
    }
   ],
   "source": [
    "max_touched = []\n",
    "min_touched = []\n",
    "(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "print(v2, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing above functions\n",
    "# min_touched=[]\n",
    "# max_touched=[]\n",
    "# # Game start from maximizer random play\n",
    "# print('Maximizer random selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# max_touched.append(v1)\n",
    "# # print('v1, max_opinion, max_pol')\n",
    "# # print(v1, max_opinion, max_pol)\n",
    "# # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Game start from minimizer random play \n",
    "# print('Minimizer random selection')\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# min_touched.append(v2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # # Testing block to see if player operate properly\n",
    "# (v1, max_opinion, max_pol) = maximizer_play(s,v2,min_opinion,n,min_touched)\n",
    "# (v2, min_opinion, min_pol) = minimizer_play(s,v1,max_opinion,n, max_touched)\n",
    "# # result =  minimizer_play(max_input, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row are Column are depended on min and max's choice: agent v and opinion \n",
    "def row_index(v2, min_opinion):\n",
    "    row = 11*v2 + min_opinion*10 \n",
    "    return int(row)\n",
    "def column_index(v1,max_opinion):\n",
    "    column = 2*v1 + max_opinion\n",
    "    return int(column)  #the python dataframe index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Strategy Payoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_payoff_row(op1,v2):\n",
    "    payoff_row = np.zeros(2*n)\n",
    "    v1 = 0\n",
    "#     print('one opinion changed -min')\n",
    "#     print(op1)\n",
    "    for column in range(2*n):\n",
    "#         print(column)\n",
    "        v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "        max_opinion = column%2\n",
    "#         print(v1, max_opinion)\n",
    "        # update the maximizer's change to the opinion array that has changed by minimizer(op1)\n",
    "        op2 = copy.copy(op1)\n",
    "        op2[v1,0] = max_opinion\n",
    "#         print('max_opinion')\n",
    "#         print(v1, max_opinion)\n",
    "#         print('two opinion changed -min +  max')\n",
    "#         print(op2)\n",
    "        # calculate the polarization with both max and min's action\n",
    "        payoff_row[column] = obj_polarization(A, L, op2, n)\n",
    "#         print(op2,payoff_row[column])\n",
    "    # when v1 == v2, the polarization should be negative for max, infinet for min. \n",
    "    # Replace the the column_index of agent v2 with 0 for max\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "    \n",
    "    return payoff_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.4124    0.4402    0.4301    0.3945 -100.     -100.        0.4341\n",
      "    0.4598    0.4528    0.449     0.4264    0.3871    0.4499    0.4537\n",
      "    0.4215    0.3994    0.4249    0.4351    0.4401    0.459 ]\n"
     ]
    }
   ],
   "source": [
    "# #(1,0) (2,0.3928571428571428)\n",
    "op1=copy.copy(s)\n",
    "# print(op1)\n",
    "op1[2,0] = 1  #op1 is the opinion array that updated by minimizer\n",
    "# print(op1)\n",
    "payoff_row_1 = make_payoff_row(op1,2)\n",
    "print(payoff_row_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEEDDDDDDD UPDAE\n",
    "\n",
    "# Calculate polarization of minimizer's Mixed Strategy\n",
    "def mixed_min_polarization(s,v2,min_opinion,fla_max_fre):\n",
    "\n",
    "    op1 =  copy.copy(s) # make a copy of the innate opinion array \n",
    "    op1[v2,0] = min_opinion # then only updated by minimizer's current change\n",
    "\n",
    "    # calculate the polarization with both min(did here) and max's action(in make_payoff_row)\n",
    "    payoff_row = make_payoff_row(op1,v2)  # the vector list out 2*n payoffs after min's action combine with 2*n possible max's actions\n",
    "    #print(payoff_row)\n",
    "\n",
    "    # Replace the the column_index of agent v2 with 100 for min\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = 100\n",
    "    payoff_row[j_2] = 100\n",
    "    \n",
    "#     print('Min Payoff Row')\n",
    "#     print(payoff_row)\n",
    "    #calculate fictitious payoff - equi_min  \n",
    "    payoff_cal = payoff_row * fla_max_fre # fla_max_fre recorded the frequency of each maximizer's action, frequency sum = 1\n",
    "                                             # payoff (2*n array) * maximizer_action_frequency (2*n array)\n",
    "\n",
    "    mixed_pol = np.sum(payoff_cal) # add up all, calculate average/expected payoff\n",
    "\n",
    "\n",
    "#     print('min_mixed_polarization')\n",
    "#     print(mixed_pol)\n",
    "        # Replace the the column_index of agent v2 with 100 for min\n",
    "\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "\n",
    "    return (mixed_pol,payoff_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_s(op,n,v2):\n",
    "               #op - opinion array that updated by maximizer\n",
    "    c = [1/n] * n\n",
    "#     print(c)\n",
    "    sum_term = 0\n",
    "    j = 0\n",
    "\n",
    "    sum_term = np.dot(np.dot((A-c),(A[v2]-c)),op)  # sum up all terms\n",
    "    \n",
    "    term_out = op[v2]*np.dot((A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "    sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "    s_star = -sum_s/np.dot((A[v2]-c),(A[v2]-c))\n",
    "    s_star = s_star[0] #take value out of array\n",
    "    min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "    return min_opinion\n",
    "\n",
    "def derivate_s1(op,n,v2):\n",
    "               #op - opinion array that updated by maximizer\n",
    "    c = [1/n] * n\n",
    "#     print(c)\n",
    "    sum_term = 0\n",
    "    j = 0\n",
    "    for j in range(0,n):\n",
    "        term = op[j]*np.dot(np.transpose(A[j]-c),(A[v2]-c))\n",
    "#             print(A[j])\n",
    "#             print(A[v])\n",
    "        sum_term = sum_term + term  # sum up all terms\n",
    "    \n",
    "    term_out = op[v2]*np.dot(np.transpose(A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "    sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "    s_star = -sum_s/np.dot(np.transpose(A[v2]-c),(A[v2]-c))\n",
    "    s_star = s_star[0] #take value out of array\n",
    "    min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "    return min_opinion\n",
    "\n",
    "\n",
    "\n",
    "def min_mixed_opinion(op, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            \n",
    "##             temp = op[v1,0] \n",
    "          \n",
    "##             op[v1,0]= max_opinion #update innate opinion array with max_action    \n",
    "\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "#             print(fla_max_fre[column],min_opinion)\n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "#     print(weight_op)\n",
    "##             op[v1,0] = temp \n",
    "    \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "    \n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    ]\n",
      " [0.3364]\n",
      " [0.3501]\n",
      " [0.5303]\n",
      " [0.4197]\n",
      " [0.3167]\n",
      " [0.5545]\n",
      " [0.317 ]\n",
      " [0.336 ]\n",
      " [0.3883]]\n",
      "0.46908599789418964\n",
      "0.46908599789418964\n"
     ]
    }
   ],
   "source": [
    "op2=op\n",
    "op2[0,0]=1\n",
    "print(op2)\n",
    "min_opinion1 = derivate_s(op2,n,1)\n",
    "print(min_opinion1)\n",
    "min_opinion2 = derivate_s1(op2,n,1)\n",
    "print(min_opinion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(v2,fla_max_fre)\n",
    "# (weight_op_1,payoff_row,min_por) = min_mixed_opinion_1(s, n, v2, fla_max_fre)\n",
    "# print(weight_op_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fla_max_fre = [0, 0, 0, 0, 0.65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0, 0, 0]\n",
    "# # print(fla_max_fre[13])\n",
    "# v2 = 1\n",
    "# (weight_op,payoff_row,min_por) = min_mixed_opinion(s, n, v2, fla_max_fre)\n",
    "# print(weight_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fla_max_fre = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "# # print(fla_max_fre[13])\n",
    "# v2 = 1\n",
    "# (weight_op,payoff_row,min_por) = min_mixed_opinion(s, n, v2, fla_max_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "all = list(range(n))    # for all agent \n",
    "C1 = [x for x in all if x not in max_touched]  \n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizer search: Go through each agent \n",
    "\n",
    "def mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre):\n",
    "    # current polarization that changed by maximizer, \"innate\" objective that min start with\n",
    "    op = copy.copy(s)\n",
    "    op[v1,0] = max_opinion\n",
    "#     print('Check if op has been updated by Maximizer')\n",
    "#     print(op)\n",
    "    min_por = obj_polarization(A, L, op, n) #min_por- set a standard to compare with pol after min's action\n",
    "    maxup_por = min_por # store innate max updated polarization\n",
    "#     print('check maxup por')\n",
    "#     print(maxup_por)\n",
    "#     payoffs = []    # create an empty list to store all polarizations   \n",
    "    champion = (None, None, 0, None)  # assume the best action is champion\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in max_touched]  # for the vertice that Maximizer has not touched\n",
    "    \n",
    "    for v2 in C1:   \n",
    "#         print('Min start with agent '+ str(v2) )\n",
    "        (changed_opinion, payoff_row, por) = min_mixed_opinion(op, n, v2, fla_max_fre)   # find the best new_op option           \n",
    "#         print('changed opinion, por, min_por')\n",
    "#         print(changed_opinion, por, maxup_por)\n",
    "\n",
    "        if por < min_por:  # if the recent polarization is smaller than the minimum polarization in the history\n",
    "            min_por = por\n",
    "                                 # update the recent option as champion\n",
    "            champion = (v2, changed_opinion, payoff_row, min_por)  \n",
    "#         else:\n",
    "#             print('Innate polarization is smaller than Min action')\n",
    "\n",
    "    return (champion)  # find the best minimizer's action after going through every new_op option of every agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1,max_opinion\n",
      "4 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fla_max_fre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-5694a4f5ce14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'v1,max_opinion'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_opinion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mchampion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_choose_min_vertex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_opinion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_touched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfla_max_fre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# print(champion)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fla_max_fre' is not defined"
     ]
    }
   ],
   "source": [
    "# fla_max_fre = [0, 0, 0, 0, 0.51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.49, 0, 0, 0]\n",
    "# v1 = 5\n",
    "# max_opinion = 0\n",
    "# max_touched = []\n",
    "print('v1,max_opinion')\n",
    "print(v1,max_opinion)\n",
    "champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by maximizer, fla_max_fre includes max's hisotry, so minimizer react to the innate op after that\n",
    "def mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre): \n",
    "\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre)\n",
    "    (v2, min_opinion, payoff_row, min_pol) = min_champion\n",
    "    \n",
    "    if v2 == None:    # if minimizer cannot find a action to minimize polarization after maximizer's action\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "#         print('v2', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(v2, min_opinion, innat_equi_por, min_pol)\n",
    "\n",
    "        # Store innate_op of the min_selected vertex\n",
    "        old_opinion_min = op[v2,0]\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "#         print(\"Payoff row\")\n",
    "#         print(payoff_row)\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return (v2, payoff_row, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fla_max_fre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-e9082caadfcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_touched\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayoff_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_opinion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolarization\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_min_play\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_opinion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_touched\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfla_max_fre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'v2, payoff_row, min_opinion, polarization'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fla_max_fre' is not defined"
     ]
    }
   ],
   "source": [
    "print(max_touched)\n",
    "(v2, payoff_row, min_opinion, polarization) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "print('v2, payoff_row, min_opinion, polarization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payoff_matrix = np.empty((0, 2*n), float)\n",
    "# payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "# print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by minimizer, fla_min_fre includes min's hisotry, so maxmizer react to the innate op after that\n",
    "def mixed_max_polarization(payoff_matrix,v1,max_opinion,fla_min_fre):\n",
    "\n",
    "    # create payoff matrix for maxmizer\n",
    "    column = int(column_index(v1,max_opinion))\n",
    "#     print(payoff_matrix)\n",
    "#     print(\"column\"+str(column))\n",
    "    payoff_vector = payoff_matrix[:,column]\n",
    "    \n",
    "#     print('payoff vector')\n",
    "#     print(payoff_vector)\n",
    "\n",
    "    #calculate fictitious payoff - equi_max   \n",
    "    payoff_cal = payoff_vector * fla_min_fre #payoff * frequency\n",
    "    \n",
    "#     print('max_payoff_calculation')\n",
    "#     print(payoff_cal)\n",
    "    mixed_pol = np.sum(payoff_cal) # add up\n",
    "#     print(\"Max_mixed_polarization\")\n",
    "#     print(mixed_pol)\n",
    "\n",
    "    return mixed_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_pol = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if value of opinion at v should be set to 0 or 1 to maximize equilibrium polarization \n",
    "def max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre):\n",
    "    \n",
    "    por_arr = np.zeros(2)  # create a two_element array to store polarization value of each option\n",
    "\n",
    "\n",
    "    max_opi_option = [0, 1.0]   # Maximizer has two options to change agent v1's opinion\n",
    "    \n",
    "    # objective if set opinion to 0, 1.0\n",
    "    j = 0\n",
    "    for new_op in max_opi_option:\n",
    "#         print('change op to '+ str(i/10))\n",
    "        max_opinion = new_op\n",
    "\n",
    "        por_arr[j] = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)\n",
    "    \n",
    "        j = j + 1   # index increase 1, put the polarization in array\n",
    "\n",
    "#     print('Polarization Options')\n",
    "#     print(por_arr)\n",
    "    \n",
    "    maxmize_op = np.argmax(por_arr)  # the index of maximum polarization = max_opinion --[0,1]\n",
    "    max_por = np.max(por_arr)        # find the maximum polarization in the record\n",
    " \n",
    "#     print('new_op', 'innat_equi_por', 'max_por')\n",
    "#     print(maxmize_op, innat_equi_por, max_por)\n",
    "\n",
    "    return (maxmize_op, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fla_min_fre = [0, 0, 0, 0, 0.65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0, 0, 0]\n",
    "# v1 = 2\n",
    "# champion = max_mixed_opinion(payoff_matrix, n, v1, v2, fla_min_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which agent maximizer should select to maximizer the equilibrium polarization\n",
    "def mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre):\n",
    "#     print('Check if op has been updated by minimizer')\n",
    "#     print(op)\n",
    "    max_por = obj_polarization(A, L, op, n)  # use \"innate\"(after min action) polarization as a comparable standard to find max_por\n",
    "    minup_por = max_por # store innate min_update polarization\n",
    "#     print('check minup por')\n",
    "#     print(minup_por)\n",
    "    champion = (None, None, max_por)  # assume champion is the best action\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in min_touched]  # for the vertice that Minimizer has not touched\n",
    "    for v1 in C1:  \n",
    "#             print('Maximizer start from agent'+str(v1))\n",
    "            (changed_opinion, por) = max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre)\n",
    "#             print('changed_opinion, por, minup_por')\n",
    "#             print(changed_opinion, por,minup_por)\n",
    "            \n",
    "            if por > max_por: # if the polarization of most recent action > maximum polarization of previous actions\n",
    "                max_por = por\n",
    "                champion = (v1, changed_opinion,max_por)   # save the this action as champion    \n",
    "#             else:\n",
    "#                 print('Innate polarization is bigger than max action')\n",
    " \n",
    "    return (champion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.3039    0.3676 -100.     -100.        0.2579    0.3884    0.2775\n",
      "     0.3808    0.3025    0.3854    0.2894    0.3358    0.2939    0.3752\n",
      "     0.2946    0.3544    0.3033    0.3687    0.3067    0.3851]\n",
      " [-100.     -100.        0.1876    0.1804    0.1829    0.2108    0.193\n",
      "     0.201     0.2065    0.2073    0.1734    0.1498    0.194     0.2083\n",
      "     0.186     0.1765    0.1951    0.1902    0.2031    0.2113]\n",
      " [-100.     -100.        0.19      0.1804    0.1859    0.2111    0.1963\n",
      "     0.2008    0.2093    0.2072    0.1755    0.1494    0.1967    0.2086\n",
      "     0.1882    0.1766    0.1974    0.1902    0.2059    0.2111]\n",
      " [-100.     -100.        0.19      0.1804    0.1859    0.2111    0.1963\n",
      "     0.2008    0.2093    0.2072    0.1755    0.1494    0.1967    0.2086\n",
      "     0.1882    0.1766    0.1974    0.1902    0.2059    0.2111]\n",
      " [-100.     -100.        0.19      0.1804    0.1859    0.2111    0.1963\n",
      "     0.2008    0.2093    0.2072    0.1755    0.1494    0.1967    0.2086\n",
      "     0.1882    0.1766    0.1974    0.1902    0.2059    0.2111]]\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)\n",
    "champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_touched = []\n",
    "# # payoff_matrix = np.empty((0, 2*n), float)\n",
    "# # fla_min_fre = np.empty((0,n))\n",
    "# # champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)\n",
    "# # print(champion)\n",
    "# print(c1)\n",
    "# vertices = np.where(c1)\n",
    "# print(vertices)\n",
    "# por=0\n",
    "# for i in c1:\n",
    "#     print(i)\n",
    "#     max_por = 0.75\n",
    "#     if por > max_por:\n",
    "#         max_por = por\n",
    "#         print('yes')\n",
    "#     else:\n",
    "#         print('por<max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre): \n",
    "    op = copy.copy(s)   # op is a copy of innate opinion\n",
    "    \n",
    "    #update innat opinion \n",
    "    op[v2,0] = min_opinion  # Op has been updated by minimizer, so maximizer react to the innate op after that\n",
    "    \n",
    "\n",
    "    max_champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre) # The best choice among all opinions and vertexs\n",
    "    (v1, max_opinion, max_pol) = max_champion\n",
    "\n",
    "    if v1 == None:\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        \n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Testing function -- NO NEED TO RUN\n",
    "# min_touched = []\n",
    "# v2 = 0\n",
    "# min_opinion = 0\n",
    "# b = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "# print('v1,max_opinion,max_pol')\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Player's Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = make_innat_opinions(n, c1)\n",
    "# op = s\n",
    "# print(s)\n",
    "# y = mean_center(s,n)\n",
    "# # print(y)\n",
    "# innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "# print('Innate_polarization:')\n",
    "# print(innat_pol)\n",
    "\n",
    "# # Test equilibrium polarization\n",
    "# equ_pol = obj_polarization(A, L, op, n)\n",
    "# print('Equi_polarization:')\n",
    "# print(equ_pol)\n",
    "\n",
    "# di = equ_pol-innat_pol\n",
    "# print(\"Difference:\")\n",
    "# print(di)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = 'Random 10 Agents'\n",
    "# memeory = 50\n",
    "\n",
    "\n",
    "with open('Network_'+str(Network)+'.txt', \"a\") as fi:\n",
    "    print('Innate Opinion', file=fi)\n",
    "    print(s, file=fi)\n",
    "    print('Adjacency Matrix', file=fi)\n",
    "    print(G,file=fi)\n",
    "\n",
    "# Game Preparation\n",
    "def push(obj, element):\n",
    "    if len(obj) >= memory:\n",
    "        obj.pop(0)\n",
    "        print('pop')\n",
    "    obj.append(element)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.494 ]\n",
      " [0.3931]\n",
      " [0.48  ]\n",
      " [0.4475]\n",
      " [0.4997]\n",
      " [0.379 ]\n",
      " [0.4192]\n",
      " [0.5566]\n",
      " [0.5905]\n",
      " [0.6184]]\n",
      "Innate_polarization:\n",
      "0.05973057074347636\n",
      "Equi_polarization:\n",
      "0.037980877723229445\n",
      "Difference:\n",
      "-0.021749693020246916\n"
     ]
    }
   ],
   "source": [
    "s = make_innat_opinions(n, c1)\n",
    "# n = 545\n",
    "# op = s\n",
    "# print(s)\n",
    "# n = 3\n",
    "# s_1=[1]\n",
    "# s_2=[0.5]\n",
    "# s_3=[0]\n",
    "# s = np.vstack((s_1,s_2,s_3))\n",
    "print(s)\n",
    "#s = (1,0.5,0) # polarization = 0.75\n",
    "# s = (1,0,0.5) # polarization = 0.75\n",
    "# s =(0,1,0.5) # polarization = 0.75\n",
    "#s = (0.5,1,0) # polarization = 0\n",
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, op, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[[   0.3039    0.3676 -100.     -100.        0.2579    0.3884    0.2775\n",
      "     0.3808    0.3025    0.3854    0.2894    0.3358    0.2939    0.3752\n",
      "     0.2946    0.3544    0.3033    0.3687    0.3067    0.3851]\n",
      " [-100.     -100.        0.1876    0.1804    0.1829    0.2108    0.193\n",
      "     0.201     0.2065    0.2073    0.1734    0.1498    0.194     0.2083\n",
      "     0.186     0.1765    0.1951    0.1902    0.2031    0.2113]\n",
      " [-100.     -100.        0.19      0.1804    0.1859    0.2111    0.1963\n",
      "     0.2008    0.2093    0.2072    0.1755    0.1494    0.1967    0.2086\n",
      "     0.1882    0.1766    0.1974    0.1902    0.2059    0.2111]\n",
      " [-100.     -100.        0.19      0.1804    0.1859    0.2111    0.1963\n",
      "     0.2008    0.2093    0.2072    0.1755    0.1494    0.1967    0.2086\n",
      "     0.1882    0.1766    0.1974    0.1902    0.2059    0.2111]\n",
      " [-100.     -100.        0.19      0.1804    0.1859    0.2111    0.1963\n",
      "     0.2008    0.2093    0.2072    0.1755    0.1494    0.1967    0.2086\n",
      "     0.1882    0.1766    0.1974    0.1902    0.2059    0.2111]]\n"
     ]
    }
   ],
   "source": [
    "op = copy.copy(s)\n",
    "print(n)\n",
    "print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameters\n",
    "Game_rounds =201 # Rounds + 1- use for printing data\n",
    "memory = 1\n",
    "\n",
    "def all_fre_limited_touch(s, n):\n",
    "    # Preparation for the game\n",
    "    op = copy.copy(s)\n",
    "    payoff_matrix = np.empty((0, 2*n), float)\n",
    "    max_history = np.zeros([n, 2])  # n*2 matrix, agent i & opinion options\n",
    "    min_history = []  # append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "    print(type(min_history))\n",
    "\n",
    "    max_history_last_100 = np.zeros([n, 2]) \n",
    "    min_history_last_100= []\n",
    "\n",
    "    max_touched = []\n",
    "    min_touched = []\n",
    "    min_touched_all = []\n",
    "    min_touched_last_100 =[]\n",
    "    print('min_touched')\n",
    "    print(min_touched)\n",
    "    \n",
    "    \n",
    "    # Game start from maximizer random play\n",
    "    print('Maximizer first selection')\n",
    "    (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action \n",
    "    #(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "    #(v1, max_opinion, max_pol) = (2, 1, 0.14833274000237331)\n",
    "    First_max = (v1, max_opinion, max_pol) \n",
    "\n",
    "\n",
    "#     (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    # Maximizer start with greedy play\n",
    "    # (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "    max_touched.append(v1)    # save Maximizer's action history\n",
    "\n",
    "    # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "    max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "    # print('max_history')\n",
    "    # print(max_history)\n",
    "    print('history at spot')\n",
    "    print(max_history[v1,int(max_opinion)])\n",
    "\n",
    "    max_frequency = max_history/1  # its frequency, only played  1 time so far, divided by 1 \n",
    "    # print('fre_max at spot')\n",
    "    # print(max_frequency[v1,int(max_opinion)])\n",
    "\n",
    "    fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "                                            # so we can multiply the freuency (2n*1)with payoff array (1*2n) \n",
    "                                            # to get average payoff of fictitious play\n",
    "    print('fre_max at spot')\n",
    "    print(fla_max_fre)\n",
    "\n",
    "    column = int(column_index(v1,max_opinion))    # the frequency of maximizer's most recent action (v1,max_opinion)\n",
    "\n",
    "    print(fla_max_fre[column])\n",
    "\n",
    "    # print(np.shape(fla_max_fre.shape))\n",
    "\n",
    "\n",
    "    # if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "    print('Minimizer first selection')\n",
    "    (v2, min_opinion, min_pol) = random_play1(op,n) \n",
    "    #(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "    \n",
    "    #(v2, min_opinion, min_pol) = (1, 0, 0.5933309600094931)\n",
    "    First_min = (v2, min_opinion, min_pol)\n",
    "\n",
    "    if v1==v2:   # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "        sys.exit()\n",
    "\n",
    "    # Minimizer start with greedy play\n",
    "    # (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    min_touched.append(v2)\n",
    "   \n",
    "\n",
    "    # store minimizer play history\n",
    "    min_history.append((v2,min_opinion))\n",
    "    print('min_history')\n",
    "    print(min_history)\n",
    "\n",
    "\n",
    "    counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/1 #return only frequency of all min options in order\n",
    "    print('fla_min_fre')\n",
    "    print(fla_min_fre)\n",
    "\n",
    "\n",
    "    (a,payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "    payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "    print('Payoff Matrix')\n",
    "    print(payoff_matrix)\n",
    "    # print('fla_min_fre at the spot')\n",
    "    # min_counter = dict(counter)\n",
    "    # print(min_counter) \n",
    "    # print(min_counter[(v2,min_opinion)]) \n",
    "    # print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "\n",
    "    equi_min = min_pol\n",
    "    equi_max = max_pol\n",
    "    # print(equi_min)\n",
    "    # print(equi_max)\n",
    "\n",
    "\n",
    "\n",
    "    Flag = 0\n",
    "\n",
    "    i = 0\n",
    "    while Flag == 0: \n",
    "        i = i + 1\n",
    "        print(\"Game \" + str(i))\n",
    "        print(\"_____________________\")\n",
    "\n",
    "    #     if max_pol == min_pol:\n",
    "        if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "                                # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "            print('min_recent_'+str(memory)+'_touched')# then stop at Game 202\n",
    "            print(min_touched)\n",
    "            print('max_recent_'+str(memory)+'_touched')\n",
    "            print(max_touched)\n",
    "            print('Min last 100 action')\n",
    "            print(min_touched_last_100)\n",
    "\n",
    "            break\n",
    "\n",
    "        elif equi_min == equi_max:\n",
    "            print(\"Reached Nash Equilibrium at game\"+ str(i) + \"and Equi_Por = \" + str(equi_min))\n",
    "            print('max_distribution')\n",
    "            print(max_frequency)\n",
    "            print('min_distribution')\n",
    "            print(fla_min_fre)\n",
    "            Flag = 1\n",
    "            break\n",
    "        ############################## maximizer play  \n",
    "        else:\n",
    "            if i == Game_rounds-100:    #if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "    #             max_touched_100 = max_touched \n",
    "    #             min_touched_100 = min_touched\n",
    "    #             max_fre_100 = max_frequency  # store the max_frequency of first 100 iterataions\n",
    "    #             print('max_history')\n",
    "    #             print(max_history)\n",
    "    #             min_fre_100 = fla_min_fre  # max_frequency of first 100 iterations\n",
    "    #             print('min_history')\n",
    "    #             print(min_history)\n",
    "    # Remove max frequncy less than 0.1--\n",
    "                max_history_last_100 = np.zeros([n, 2]) \n",
    "                min_history_last_100 = [] \n",
    "                min_touched_last_100 =[]\n",
    "\n",
    "            (v1, max_opinion, equi_max) = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "            max_touched = push(max_touched, v1)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "    #             print('equi_max')\n",
    "    #             print(equi_max)\n",
    "    #         print(v1, max_opinion, max_pol)\n",
    "            # cumulate strategy \n",
    "            max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "\n",
    "            max_history_last_100[v1,int(max_opinion)] = max_history_last_100[v1,int(max_opinion)] +1\n",
    "    #         print('max_history')\n",
    "    #         print(max_history)\n",
    "    #________________________________________________________________\n",
    "            max_frequency = max_history/(i+1)  # its frequency \n",
    "    #         print('max_distribution')\n",
    "    #         print(max_frequency)\n",
    "        #     print(i+1) \n",
    "            fla_max_fre = max_frequency.flatten() #flaten max_frequency to calculate average payoff\n",
    "            print('fla_max_fre')\n",
    "            print(fla_max_fre)\n",
    "            print('fre_max at spot')\n",
    "            print(fla_max_fre[column])\n",
    "            # create payoff matrix for maxmizer\n",
    "            row = int(row_index(v2, min_opinion))\n",
    "            column = int(column_index(v1,max_opinion))\n",
    "\n",
    "    # _________________________________________________________________\n",
    "    #         ######################Visualize Maximizer's selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v1:\n",
    "    #                 color_map.append('Red')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True,node_size = 50)\n",
    "    #         plt.figure(figsize=(200, 200))\n",
    "    #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    ############################### minimizer play\n",
    "            (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "            min_touched = push(min_touched, v2)\n",
    "            min_touched_all.append(v2) \n",
    "            min_touched_last_100.append(v2)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('equi_min')\n",
    "    #         print(equi_min)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "            #         print(v2, min_opinion, min_pol)\n",
    "            if (v2,min_opinion) in counter.keys():\n",
    "                payoff_matrix = payoff_matrix # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "                print(\"Same history\")\n",
    "                print((str(v2),str(min_opinion)))\n",
    "            else:\n",
    "                payoff_matrix = np.vstack([payoff_matrix, payoff_row]) # if this is a new option, append to previous matrix\n",
    "    #                 print('payoff_row')\n",
    "    #                 print(payoff_row)\n",
    "            min_history.append((v2,min_opinion))\n",
    "            min_history_last_100.append((v2,min_opinion))\n",
    "            #         print('min_history')\n",
    "            #         print(min_history)\n",
    "            counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "            #print(counter)\n",
    "    #         print('counter.keys')\n",
    "    #         print(counter.keys())\n",
    "            fla_min_fre = np.array(list(counter.values()))/(i+1) #return only frequency of all min options in order\n",
    "    #         print('fla_min_fre')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "    #         print('fla_min_fre at the spot')\n",
    "    #         min_counter = dict(counter)\n",
    "    #         print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "            # create payoff matrix for minimizer\n",
    "            row = row_index(v2, min_opinion)\n",
    "            column = column_index(v1,max_opinion)\n",
    "            #     print('row, column')\n",
    "            #     print(row, column)\n",
    "\n",
    "            print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max)) \n",
    "    #         print('min_distribution')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "            ######################Visualize Minimizer selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G1, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G1)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v2:\n",
    "    #                 color_map.append('Blue')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True)\n",
    "    #         plt.figure(figsize=(25, 25))\n",
    "    #         plt.show()\n",
    "    return (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "min_touched\n",
      "[]\n",
      "Maximizer first selection\n",
      "    Agent9 's opinion 0.6184271704065811 changed to 0\n",
      "Network reaches equilibrium Polarization: 0.17662099701522016\n",
      "history at spot\n",
      "1.0\n",
      "fre_max at spot\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "1.0\n",
      "Minimizer first selection\n",
      "    Agent1 's opinion 0.39307135706337315 changed to 0\n",
      "Network reaches equilibrium Polarization: 0.1683321795550994\n",
      "min_history\n",
      "[(1, 0)]\n",
      "Counter({(1, 0): 1})\n",
      "fla_min_fre\n",
      "[1.]\n",
      "Payoff Matrix\n",
      "[[   0.2883    0.3526 -100.     -100.        0.2847    0.3811    0.2901\n",
      "     0.3583    0.3022    0.3566    0.2815    0.3262    0.305     0.3496\n",
      "     0.2734    0.3308    0.2723    0.3422    0.2827    0.3437]]\n",
      "Game 1\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.5 0. ]\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent8 's opinion 0.5905188332805741 changed to 0.5230138273502922\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.18815796252342076 and Equi_Max = 0.38107574884876017\n",
      "Game 2\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.6667 0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.3333 0.    ]\n",
      "fre_max at spot\n",
      "0.6666666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.19126331774090222 and Equi_Max = 0.2968967628977617\n",
      "Game 3\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.   0.   0.   0.   0.   0.75 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.25 0.  ]\n",
      "fre_max at spot\n",
      "0.75\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380468')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.1917998718100338 and Equi_Max = 0.2624010199376506\n",
      "Game 4\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.  0.  0.  0.  0.  0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.2 0. ]\n",
      "fre_max at spot\n",
      "0.8\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380468')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.19212180425151276 and Equi_Max = 0.2451531484575951\n",
      "Game 5\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.8333 0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.1667 0.    ]\n",
      "fre_max at spot\n",
      "0.8333333333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380468')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.19233642587916538 and Equi_Max = 0.23480442556956177\n",
      "Game 6\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.8571 0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.1429 0.    ]\n",
      "fre_max at spot\n",
      "0.8571428571428571\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380468')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.1924897270417744 and Equi_Max = 0.22790527697753954\n",
      "Game 7\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.    0.    0.    0.    0.    0.875 0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.125 0.   ]\n",
      "fre_max at spot\n",
      "0.875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380468')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.19260470291373116 and Equi_Max = 0.2229773136975237\n",
      "Game 8\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.8889 0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.1111 0.    ]\n",
      "fre_max at spot\n",
      "0.8888888888888888\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380467\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.19269412859191973 and Equi_Max = 0.2192813412375118\n",
      "Game 9\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.  0.  0.  0.  0.  0.9 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.1 0. ]\n",
      "fre_max at spot\n",
      "0.9\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380468')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.1927656691344706 and Equi_Max = 0.21640669599083584\n",
      "Game 10\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.9091 0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.0909 0.    ]\n",
      "fre_max at spot\n",
      "0.9090909090909091\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380467\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380467')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.1928242023056486 and Equi_Max = 0.21410697979349513\n",
      "Game 11\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.9167 0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.0833 0.    ]\n",
      "fre_max at spot\n",
      "0.9166666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380468')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.1928729799482969 and Equi_Max = 0.21222539381385272\n",
      "Game 12\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.9231 0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.0769 0.    ]\n",
      "fre_max at spot\n",
      "0.9230769230769231\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380469\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.19291425333823012 and Equi_Max = 0.21065740549748402\n",
      "Game 13\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent2 's opinion 0.4799788814060366 changed to 1\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.9286 0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.0714 0.    ]\n",
      "fre_max at spot\n",
      "0.9285714285714286\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent5 's opinion 0.37896629569438406 changed to 0.5786518480380468\n",
      "pop\n",
      "Same history\n",
      "('5', '0.5786518480380468')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.19294963052960146 and Equi_Max = 0.20933064615286437\n",
      "Game 14\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent4 's opinion 0.4996857765716295 changed to 0\n",
      "pop\n",
      "fla_max_fre\n",
      "[0.     0.     0.     0.     0.     0.8667 0.     0.     0.0667 0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.0667 0.    ]\n",
      "fre_max at spot\n",
      "0.8666666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "Minimizer fail\n",
      "pop\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 20 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-ca4231009d8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mFirst_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFirst_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_touched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_touched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayoff_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfla_min_fre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_history_last_100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_touched_last_100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_touched_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_history_last_100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_fre_limited_touch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-dcd08388ed16>\u001b[0m in \u001b[0;36mall_fre_limited_touch\u001b[1;34m(s, n)\u001b[0m\n\u001b[0;32m    218\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_opinion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                 \u001b[0mpayoff_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpayoff_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayoff_row\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# if this is a new option, append to previous matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[1;31m#                 print('payoff_row')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;31m#                 print(payoff_row)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 20 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "(First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100) = all_fre_limited_touch(s, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005  0.005  0.1045 0.806  0.0796]\n",
      "[5]\n",
      "[(5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.558167125379164), (5, 0.5581671253791641), (5, 0.558167125379164), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.558167125379164), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.558167125379164), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.558167125379164), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791642), (5, 0.558167125379164), (5, 0.5581671253791642), (5, 0.5581671253791642), (5, 0.5581671253791641), (5, 0.558167125379164), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641), (5, 0.5581671253791641)]\n"
     ]
    }
   ],
   "source": [
    "print(fla_min_fre)\n",
    "print(min_touched)\n",
    "print(min_history_last_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.3039    0.3676 -100.     -100.        0.2579    0.3884    0.2775\n",
      "     0.3808    0.3025    0.3854    0.2894    0.3358    0.2939    0.3752\n",
      "     0.2946    0.3544    0.3033    0.3687    0.3067    0.3851]\n",
      " [   0.1889    0.163     0.1796    0.1644    0.1838    0.1911    0.1803\n",
      "     0.1899    0.1963    0.194  -100.     -100.        0.1933    0.1883\n",
      "     0.178     0.1596    0.1891    0.1707    0.1941    0.1966]\n",
      " [   0.1883    0.163     0.1789    0.1643    0.1829    0.1911    0.1796\n",
      "     0.1898    0.1957    0.1939 -100.     -100.        0.1925    0.1883\n",
      "     0.1774    0.1596    0.1885    0.1707    0.1935    0.1966]\n",
      " [   0.1883    0.163     0.1789    0.1643    0.1829    0.1911    0.1796\n",
      "     0.1898    0.1957    0.1939 -100.     -100.        0.1925    0.1883\n",
      "     0.1774    0.1596    0.1885    0.1707    0.1935    0.1966]\n",
      " [   0.1883    0.163     0.1789    0.1643    0.1829    0.1911    0.1796\n",
      "     0.1898    0.1957    0.1939 -100.     -100.        0.1925    0.1883\n",
      "     0.1774    0.1596    0.1885    0.1707    0.1935    0.1966]]\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment = 0\n",
    "\n",
    "# Experiment_note = str('Note: This experiement has initial condition. Game round:'+str(Game_rounds)+'.')\n",
    "# (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100) = all_fre_limited_touch(s, n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_distribution\n",
      "[1.]\n",
      "[[  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0.   0.]\n",
      " [  0. 100.]]\n",
      "Min_distribution\n",
      "dict_keys([(5, 0.5581671253791641), (5, 0.558167125379164), (5, 0.5581671253791642)])\n",
      "fla_min_fre\n",
      "[0.82 0.07 0.11]\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Counter({(5, 0.5581671253791641): 162, (5, 0.558167125379164): 21, (5, 0.5581671253791642): 16, (1, 0): 1, (5, 0.5635681697610936): 1})\n",
      "fla_min_fre_1\n",
      "[0.005  0.005  0.1045 0.806  0.0796]\n"
     ]
    }
   ],
   "source": [
    "# MAXimizer's distribution of LAST 100 iteration \n",
    "print('Max_distribution')  \n",
    "max_l100_fre = max_history_last_100/100\n",
    "print(max_l100_fre [np.nonzero(max_l100_fre)])\n",
    "# print for small network\n",
    "print(max_history_last_100)\n",
    "# # Print for Large Network\n",
    "# print(np.nonzero(max_l100_fre))\n",
    "\n",
    "# MINimizer's Strategy in the last 100 round\n",
    "print('Min_distribution')\n",
    "counter=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter.keys())\n",
    "fla_min_fre = np.array(list(counter.values()))/(100) #return only frequency of all min options in order\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)\n",
    "\n",
    "print(min_touched_last_100)\n",
    "counter_1=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_1)\n",
    "fla_min_fre_1 = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "print('fla_min_fre_1')\n",
    "print(fla_min_fre_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    0.593 0.148 0.445]\n",
      " [0.445 0.148 0.593 0.    0.    0.   ]\n",
      " [0.148 0.148 0.445 0.148 0.    0.   ]\n",
      " [0.    0.    0.148 0.445 0.148 0.148]] 1\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment = 'Pure11'\n",
    "with open('Result'+str(Network)+'.'+str(Experiment)+'.txt', \"a\") as f:\n",
    "#     print(Experiment_note, file=f)\n",
    "    print('Initial Condition -(agent, opinion, pol)', file=f)\n",
    "    print('Innate op'+str(s),file=f)\n",
    "    print('Adjacency matrix'+ str(G), file=f)\n",
    "    print('Max:'+ str(First_max), file=f)\n",
    "    print('Min' + str(First_min), file=f)\n",
    "\n",
    "    print(\"In the Last 100 Rounds\", file=f) \n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # MAX distribution of LAST 100 iteration \n",
    "    print('Max_distribution', file=f)  \n",
    "    max_l100_fre = max_history_last_100/100\n",
    "    print(max_l100_fre [np.nonzero(max_l100_fre)], file=f)\n",
    "    # print for small network\n",
    "    print(max_history_last_100, file=f)\n",
    "    # # Print for Large Network\n",
    "    # print(np.nonzero(max_l100_fre))\n",
    "\n",
    "    # MIN Strategy in the last 100 round\n",
    "    counter=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    # print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/100 #return only frequency of all min options in order\n",
    "    print('Min_frequency', file=f)\n",
    "    print(list(counter.keys()), file=f)\n",
    "    print('Min_distribution_last_100', file=f)\n",
    "    print(fla_min_fre, file=f)\n",
    "    counter_h=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_h.keys(), file=f)\n",
    "    \n",
    "    print('min_recent_'+str(memory)+'_touched', file=f)# then stop at Game 202\n",
    "    print(min_touched, file=f)\n",
    "    print('max_recent_'+str(memory)+'_touched', file=f)\n",
    "    print(max_touched, file=f)\n",
    "    \n",
    "    print('In Overall'+str(Game_rounds)+' Rounds', file=f)\n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # Max action Overall \n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    max_fre = max_history/Game_rounds\n",
    "    print('Max_frequency', file=f)\n",
    "    print(max_history, file=f)\n",
    "    print('Max_distribution', file=f)\n",
    "    print(max_fre [np.nonzero(max_fre)], file=f)\n",
    "    \n",
    "\n",
    "\n",
    "    # Min Strategy in the Overall    \n",
    "    counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "    fla_min_fre_all = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "    print('Min_dist_all', file=f)\n",
    "    print(fla_min_fre_all, file=f)\n",
    "    print('Min_distribution', file=f)\n",
    "    counter_a=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_a.keys(), file=f)\n",
    "    print(payoff_matrix, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(1, 0.49999999999999983): 366, (1, 0.4999999999999998): 31, (1, 0.4999999999999999): 2, (1, 0): 1, (1, 0.9999999999999997): 1})\n",
      "fla_min_fre\n",
      "[0.002 0.002 0.077 0.913 0.005]\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(min_history) \n",
    "print(counter)\n",
    "fla_min_fre = np.array(list(counter.values()))/Game_rounds\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Game Parameters\n",
    "# Game_rounds =801 # Rounds + 1- use for printing data\n",
    "# memory = 1\n",
    "\n",
    "# # def all_fre_limited_touch(s, n):\n",
    "# #     # Preparation for the game\n",
    "# op = copy.copy(s)\n",
    "# payoff_matrix = np.empty((0, 2*n), float)\n",
    "# max_history = np.zeros([n, 2])  # n*2 matrix, agent i & opinion options\n",
    "# min_history = []  # append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "# print(type(min_history))\n",
    "\n",
    "# max_history_last_100 = np.zeros([n, 2]) \n",
    "# min_history_last_100= []\n",
    "\n",
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# min_touched_all = []\n",
    "# min_touched_last_100 =[]\n",
    "# print('min_touched')\n",
    "# print(min_touched)\n",
    "\n",
    "\n",
    "# # Game start from maximizer random play\n",
    "# print('Maximizer first selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action \n",
    "# #(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "# # (v1, max_opinion, max_pol) = (1, 1, 0.6163708086785011)\n",
    "# First_max = (v1, max_opinion, max_pol) \n",
    "\n",
    "\n",
    "# #     (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,max_touched)\n",
    "\n",
    "# # Maximizer start with greedy play\n",
    "# # (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "# max_touched.append(v1)    # save Maximizer's action history\n",
    "\n",
    "# # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "# max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "# # print('max_history')\n",
    "# # print(max_history)\n",
    "# print('history at spot')\n",
    "# print(max_history[v1,int(max_opinion)])\n",
    "\n",
    "# max_frequency = max_history/1  # its frequency, only played  1 time so far, divided by 1 \n",
    "# # print('fre_max at spot')\n",
    "# # print(max_frequency[v1,int(max_opinion)])\n",
    "\n",
    "# fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "#                                         # so we can multiply the freuency (2n*1)with payoff array (1*2n) \n",
    "#                                         # to get average payoff of fictitious play\n",
    "# print('fre_max at spot')\n",
    "# print(fla_max_fre)\n",
    "\n",
    "# column = int(column_index(v1,max_opinion))    # the frequency of maximizer's most recent action (v1,max_opinion)\n",
    "\n",
    "# print(fla_max_fre[column])\n",
    "\n",
    "# # print(np.shape(fla_max_fre.shape))\n",
    "\n",
    "\n",
    "# # if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "# print('Minimizer first selection')\n",
    "# (v2, min_opinion, min_pol) = random_play(op,n) \n",
    "# #(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "\n",
    "# # (v2, min_opinion, min_pol) = (0, 0, 0.15409270216962534)\n",
    "# First_min = (v2, min_opinion, min_pol)\n",
    "\n",
    "# if v1==v2:   # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "#     sys.exit()\n",
    "\n",
    "# # Minimizer start with greedy play\n",
    "# # (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "\n",
    "# min_touched.append(v2)\n",
    "\n",
    "\n",
    "# # store minimizer play history\n",
    "# min_history.append((v2,min_opinion))\n",
    "# print('min_history')\n",
    "# print(min_history)\n",
    "\n",
    "\n",
    "# counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "# print(counter)\n",
    "# fla_min_fre = np.array(list(counter.values()))/1 #return only frequency of all min options in order\n",
    "# print('fla_min_fre')\n",
    "# print(fla_min_fre)\n",
    "\n",
    "\n",
    "# (a,payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "# payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "# print('Payoff Matrix')\n",
    "# print(payoff_matrix)\n",
    "# # print('fla_min_fre at the spot')\n",
    "# # min_counter = dict(counter)\n",
    "# # print(min_counter) \n",
    "# # print(min_counter[(v2,min_opinion)]) \n",
    "# # print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "\n",
    "# equi_min = min_pol\n",
    "# equi_max = max_pol\n",
    "# # print(equi_min)\n",
    "# # print(equi_max)\n",
    "\n",
    "\n",
    "\n",
    "# Flag = 0\n",
    "\n",
    "# i = 0\n",
    "# while Flag == 0: \n",
    "#     i = i + 1\n",
    "#     print(\"Game \" + str(i))\n",
    "#     print(\"_____________________\")\n",
    "\n",
    "# #     if max_pol == min_pol:\n",
    "#     if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "#                             # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "#         print('min_recent_'+str(memory)+'_touched')# then stop at Game 202\n",
    "#         print(min_touched)\n",
    "#         print('max_recent_'+str(memory)+'_touched')\n",
    "#         print(max_touched)\n",
    "#         print('Min last 100 action')\n",
    "#         print(min_touched_last_100)\n",
    "\n",
    "#         break\n",
    "\n",
    "#     elif equi_min == equi_max:\n",
    "#         print(\"Reached Nash Equilibrium at game\"+ str(i) + \"and Equi_Por = \" + str(equi_min))\n",
    "#         print('max_distribution')\n",
    "#         print(max_frequency)\n",
    "#         print('min_distribution')\n",
    "#         print(fla_min_fre)\n",
    "#         Flag = 1\n",
    "#         break\n",
    "#     ############################## maximizer play  \n",
    "#     else:\n",
    "#         if i == Game_rounds-100:    #if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "# #             max_touched_100 = max_touched \n",
    "# #             min_touched_100 = min_touched\n",
    "# #             max_fre_100 = max_frequency  # store the max_frequency of first 100 iterataions\n",
    "# #             print('max_history')\n",
    "# #             print(max_history)\n",
    "# #             min_fre_100 = fla_min_fre  # max_frequency of first 100 iterations\n",
    "# #             print('min_history')\n",
    "# #             print(min_history)\n",
    "# # Remove max frequncy less than 0.1--\n",
    "#             max_history_last_100 = np.zeros([n, 2]) \n",
    "#             min_history_last_100 = [] \n",
    "#             min_touched_last_100 =[]\n",
    "#         print('Max_Payoff_Matrix')\n",
    "#         print(payoff_matrix)\n",
    "#         (v1, max_opinion, equi_max) = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "#         max_touched = push(max_touched, v1)\n",
    "# #         print('min_touched')\n",
    "# #         print(min_touched)\n",
    "# #         print('max_touched')\n",
    "# #         print(max_touched)\n",
    "# #             print('equi_max')\n",
    "# #             print(equi_max)\n",
    "# #         print(v1, max_opinion, max_pol)\n",
    "#         # cumulate strategy \n",
    "#         max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "\n",
    "#         max_history_last_100[v1,int(max_opinion)] = max_history_last_100[v1,int(max_opinion)] +1\n",
    "# #         print('max_history')\n",
    "# #         print(max_history)\n",
    "# #________________________________________________________________\n",
    "#         max_frequency = max_history/(i+1)  # its frequency \n",
    "# #         print('max_distribution')\n",
    "# #         print(max_frequency)\n",
    "#     #     print(i+1) \n",
    "#         fla_max_fre = max_frequency.flatten() #flaten max_frequency to calculate average payoff\n",
    "#         print('fla_max_fre')\n",
    "#         print(fla_max_fre)\n",
    "#         print('fre_max at spot')\n",
    "#         print(fla_max_fre[column])\n",
    "#         # create payoff matrix for maxmizer\n",
    "#         row = int(row_index(v2, min_opinion))\n",
    "#         column = int(column_index(v1,max_opinion))\n",
    "\n",
    "# # _________________________________________________________________\n",
    "# #         ######################Visualize Maximizer's selection\n",
    "# #         La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "# #         nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "# #         color_map = []\n",
    "# #         for node in nxG:\n",
    "# #             if node == v1:\n",
    "# #                 color_map.append('Red')\n",
    "# #             else: \n",
    "# #                 color_map.append('Grey')  \n",
    "\n",
    "# #         #nxG1 = nx.DiGraph(G)\n",
    "# #         nx.draw(nxG, node_color=color_map, with_labels=True,node_size = 50)\n",
    "# #         plt.figure(figsize=(200, 200))\n",
    "# #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ############################### minimizer play\n",
    "#         (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "# #         print('Min_Payoff_row')\n",
    "# #         print(payoff_row)\n",
    "#         min_touched = push(min_touched, v2)\n",
    "#         min_touched_all.append(v2) \n",
    "#         min_touched_last_100.append(v2)\n",
    "# #         print('min_touched')\n",
    "# #         print(min_touched)\n",
    "# #         print('equi_min')\n",
    "# #         print(equi_min)\n",
    "# #         print('max_touched')\n",
    "# #         print(max_touched)\n",
    "#         #         print(v2, min_opinion, min_pol)\n",
    "#         if (v2,min_opinion) in counter.keys():\n",
    "#             payoff_matrix = payoff_matrix # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "#             print(\"Same history\")\n",
    "#             print((str(v2),str(min_opinion)))\n",
    "#         else:\n",
    "#             payoff_matrix = np.vstack([payoff_matrix, payoff_row]) # if this is a new option, append to previous matrix\n",
    "# #                 print('payoff_row')\n",
    "# #                 print(payoff_row)\n",
    "#         min_history.append((v2,min_opinion))\n",
    "#         min_history_last_100.append((v2,min_opinion))\n",
    "#         #         print('min_history')\n",
    "#         #         print(min_history)\n",
    "#         counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "#         #print(counter)\n",
    "# #         print('counter.keys')\n",
    "# #         print(counter.keys())\n",
    "#         fla_min_fre = np.array(list(counter.values()))/(i+1) #return only frequency of all min options in order\n",
    "# #         print('fla_min_fre')\n",
    "# #         print(fla_min_fre)\n",
    "\n",
    "# #         print('fla_min_fre at the spot')\n",
    "# #         min_counter = dict(counter)\n",
    "# #         print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "#         # create payoff matrix for minimizer\n",
    "#         row = row_index(v2, min_opinion)\n",
    "#         column = column_index(v1,max_opinion)\n",
    "#         #     print('row, column')\n",
    "#         #     print(row, column)\n",
    "\n",
    "#         print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max)) \n",
    "# #         print('min_distribution')\n",
    "# #         print(fla_min_fre)\n",
    "\n",
    "#             ######################Visualize Minimizer selection\n",
    "#     #         La = scipy.sparse.csgraph.laplacian(G1, normed=False)\n",
    "\n",
    "#     #         nxG = nx.from_numpy_matrix(G1)\n",
    "\n",
    "#     #         color_map = []\n",
    "#     #         for node in nxG:\n",
    "#     #             if node == v2:\n",
    "#     #                 color_map.append('Blue')\n",
    "#     #             else: \n",
    "#     #                 color_map.append('Grey')  \n",
    "\n",
    "#     #         #nxG1 = nx.DiGraph(G)\n",
    "#     #         nx.draw(nxG, node_color=color_map, with_labels=True)\n",
    "#     #         plt.figure(figsize=(25, 25))\n",
    "#     #         plt.show()\n",
    "#    ## return (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
