{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network \n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "#%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers the opinion vector around 0\\n\",\n",
    "def mean_center(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return x\n",
    "    \n",
    "# compute number of edges, m\\n\n",
    "def num_edges(L, n):\n",
    "    m = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i > j and L[i,j] < 0:\n",
    "                m += 1            \n",
    "    return m\n",
    "\n",
    "# maximizing polarization only: \\\\bar{z}^T \\\\bar{z}   \n",
    "def obj_polarization(A, L, op, n):\n",
    "    op_mean = mean_center(op, n)\n",
    "    z_mean = np.dot(A, op_mean) \n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "\n",
    "# Calculate innate polarization\n",
    "def obj_innate_polarization(s, n):  \n",
    "#     np.set_printoptions(precision=5)\n",
    "    op_mean = mean_center(s, n)\n",
    "    return np.dot(np.transpose(op_mean), op_mean)[0,0]\n",
    "\n",
    "def make_random_network(n, p1):\n",
    "    np.set_printoptions(precision=4)\n",
    "    ##create two set of weights connected with density 1) inviduals  2) individual & informaton Source\n",
    "    c1 = np.sort(np.random.choice(n, n, replace=False)) #assume (1-r) are individuals\n",
    "    l1 = len(c1)\n",
    "\n",
    "    ### Prepare for create adjacent matrix\n",
    "    pre_weights1 = scipy.sparse.random(1, int(0.5*n*(n - 1)), density=p1).A[0] \n",
    "    weights1 = pre_weights1/25\n",
    "    G = np.zeros((n, n))\n",
    "\n",
    "    ## Assign edges between ind to ind \n",
    "    idx = 0\n",
    "    for i in c1:\n",
    "        for j in c1:\n",
    "                if i == j:\n",
    "                    G[i][j] =0\n",
    "                    continue\n",
    "                elif i < j:\n",
    "                    G[i][j] = weights1[idx]\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    G[i][j] = G[j][i]\n",
    "    return G\n",
    "\n",
    "##create two set of weights connected with density 1) inviduals  2) individual & informaton Source\n",
    "\n",
    "\n",
    "def make_innat_opinions(n): # Make opinion for agents only - no info source is involved\n",
    "    c1 = np.sort(np.random.choice(n, n, replace=False)) #assume (1-r) are individuals\n",
    "    # Make list of ind innate opinion to define info source opinion\n",
    "    innat_s = np.random.uniform(low=0.2, high=0.8, size=int(n))   #individual's innate opinion \n",
    "\n",
    "    s = np.zeros((n, 1))\n",
    "    \n",
    "    idx1 = 0\n",
    "    for i in range(len(s)):\n",
    "        s[i] = innat_s[idx1]  #set innate opinion for ind.\n",
    "        idx1 += 1  \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network\n",
    "### 1. Make Random Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 agents\n",
      "[[0. 1. 1. ... 1. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "G = nx.karate_club_graph()\n",
    "# print(\"Node Degree\")\n",
    "n = 0\n",
    "edges = []\n",
    "\n",
    "for v in G:\n",
    "    a = f\"{G.degree(v):6}\"\n",
    "#     print(f\"{v:4} {G.degree(v):6}\")\n",
    "    edges.append(a)\n",
    "    n = n + 1\n",
    "print('There are ' + str(n) + ' agents')\n",
    "\n",
    "\n",
    "############################ Make Adjacency Matrix #####################################\n",
    "ZKC_graph = nx.karate_club_graph()\n",
    "G = nx.convert_matrix.to_numpy_matrix(ZKC_graph)\n",
    "print(G)\n",
    "\n",
    "# n = 0\n",
    "# for v in ZKC_graph:\n",
    "#     a = f\"{v:4} {G.degree(v):6}\"\n",
    "#     print(f\"{v:4} {G.degree(v):6}\")\n",
    "#     n = n + 1\n",
    "\n",
    "############################ Make Innate Opinion ################################\n",
    "##create two set of weights connected with density 1) inviduals  2) individual & informaton Source\n",
    "c1 = np.sort(np.random.choice(n, n, replace=False)) #assume (1-r) are individuals\n",
    "# print('c1')\n",
    "# print(c1)\n",
    "l1 = len(c1)\n",
    "\n",
    "\n",
    "##### Import Innate opinion\n",
    "df = pd.read_csv ('Karate Innate Opinion.csv')\n",
    "s_1 = np.array(df[df.columns[1]])\n",
    "\n",
    "s = np.reshape(s_1, (34, -1))\n",
    "# print(s)\n",
    "s_use = s.flatten()   # Convert array to a list for later operation\n",
    "s_use = s_use.tolist()\n",
    "new_s = [i * 30 for i in s_use]\n",
    "df = pd.DataFrame(new_s, columns=['Opinion']) #create a datafram with index at column 1, opinion at column 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Karate Club graph\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "\n",
    "# Convert dictionaries to Pandas DataFrames\n",
    "degree_df = pd.DataFrame(list(degree_centrality.values()), columns=['Degree Centrality'])\n",
    "closeness_df = pd.DataFrame(list(closeness_centrality.values()), columns=['Closeness Centrality'])\n",
    "eigenvector_df = pd.DataFrame(list(eigenvector_centrality.values()), columns=['Eigenvalue Centrality'])\n",
    "\n",
    "# Plot histograms of centrality measures in a horizontal layout\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Degree Centrality Histogram\n",
    "axs[0].hist(degree_df['Degree Centrality'], bins=5, edgecolor='black', alpha=0.7)\n",
    "axs[0].set_title('Degree Centrality Distribution')\n",
    "#axs[0].set_xlabel('Degree Centrality')\n",
    "axs[0].set_ylabel('Number of Nodes')\n",
    "\n",
    "# Closeness Centrality Histogram\n",
    "axs[1].hist(closeness_df['Closeness Centrality'], bins=5, edgecolor='black', alpha=0.7)\n",
    "axs[1].set_title('Closeness Centrality Distribution')\n",
    "#axs[1].set_xlabel('Closeness Centrality')\n",
    "axs[1].set_ylabel('Number of Nodes')\n",
    "\n",
    "# Eigenvalue Centrality Histogram\n",
    "axs[2].hist(eigenvector_df['Eigenvalue Centrality'], bins=5, edgecolor='black', alpha=0.7)\n",
    "axs[2].set_title('Eigenvalue Centrality Distribution')\n",
    "#axs[2].set_xlabel('Eigenvalue Centrality')\n",
    "axs[2].set_ylabel('Number of Nodes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12121212121212122\n",
      "0.38372093023255816\n",
      "0.07948057788594247\n"
     ]
    }
   ],
   "source": [
    "node = 6\n",
    "print(degree_centrality[node])\n",
    "print(closeness_centrality[node])\n",
    "print(eigenvector_centrality[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-67716cdd9f35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mnode_sizes\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_edges\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mnode_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mnode_sizes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "# ############################ Make Innate Opinion ################################\n",
    "\n",
    "### Import fixed opinion\n",
    "\n",
    "s_use = s.flatten()   # Convert array to a list for later operation\n",
    "s_use = s_use.tolist()\n",
    "new_s = [i * 30 for i in s_use]\n",
    "df = pd.DataFrame(new_s, columns=['Opinion']) #create a datafram with index at column 1, opinion at column 2\n",
    "\n",
    "\n",
    "def node_edge(G, n):\n",
    "    edges =[]\n",
    "    for v in range(n):\n",
    "        a = np.array(np.nonzero(G[v])[0])\n",
    "        edge = len(a)\n",
    "#         print(edge)\n",
    "        edges.append(edge)\n",
    "        \n",
    "    return edges\n",
    "\n",
    "node_edges = node_edge(G, n)\n",
    "# print(node_edges)\n",
    "\n",
    "node_sizes =[]\n",
    "for i in node_edges:\n",
    "    node_size = 1/i*8000\n",
    "    node_sizes.append(node_size)\n",
    "    \n",
    "# print(node_sizes)\n",
    "\n",
    "######################### Calculate Key Values & Visualization #######################################3\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)  # Return the Laplacian matrix\n",
    "A = np.linalg.inv(np.identity(n) + L)  # A = (I + L)^(-1)\\n  Stanford paper theory\n",
    "m = num_edges(L, n)                    # call the function to calculate the number of edges\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "#print(columnsum_ij)\n",
    "print(n)\n",
    "# what the twitter graph looks like \n",
    "nxG = nx.from_numpy_matrix(G)          \n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Fix seed - fix network shape\n",
    "my_pos = nx.spring_layout(nxG, seed = 2)\n",
    "nx.draw(nxG, pos= my_pos, with_labels=True, node_color=df['Opinion'].astype(int),cmap=plt.cm.Blues, node_size= node_sizes, edge_color='black', width=0.8, font_color='black',font_size=26, font_weight='bold', alpha=0.8)\n",
    "#nx.draw(nxG, pos = my_pos, with_labels=False, node_color=color_map, node_size= node_sizes, edge_color='grey', width=0.5, font_color='white',font_size=9, font_weight='bold')\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin = 0, vmax=1))\n",
    "cbar = plt.colorbar(sm, shrink = 0.5)\n",
    "tick_font_size = 24\n",
    "cbar.ax.tick_params(labelsize=tick_font_size)\n",
    "plt.show()\n",
    "\n",
    "# if we want to customize the color bar range to min/max s\n",
    "# vmin = min(s)\n",
    "# vmax = max(s)\n",
    "# sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin = vmin, vmax=vmax))\n",
    "# sm._A = []\n",
    "# plt.colorbar(sm,shrink=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Agents')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWg0lEQVR4nO3de5gldX3n8fdHEAVEENGAYDti0HgbARuN4oLi5fGyQREWEcWVNU5iVh9ds1mFIGrY4Masbsyut1kjuqyAkdsmxguuCqyJchVHbhNhwAmDI7pquOjK7bt/nBo5M3T3VPfpOqe7fL+e5zx9qk6d+n3rnJ5P1/yq6lepKiRJ/fOASRcgSeqGAS9JPWXAS1JPGfCS1FMGvCT11LaTLmDYbrvtVitWrJh0GZK0bFx22WU/rqpHzPTakgr4FStWcOmll066DElaNpJ8f7bX7KKRpJ4y4CWppwx4SeopA16SesqAl6SeMuAlqac6C/gkT0hyxdDj1iRv66o9SdLmOjsPvqrWAvsCJNkG2ACc01V7kqTNjauL5vnA9VU16wn5kqTFNa4rWY8CTp/phSSrgFUAU1NTYypHku7v8KOPYf2GjWNvd2rP3TnrtFMXfb2dB3yS7YBDgeNmer2qVgOrAaanp729lKSJWb9hIyuPPXns7a455fhO1juOLpqXAJdX1Q/H0JYkqTGOgH81s3TPSJK602nAJ9kBeCFwdpftSJLur9M++Kr6OfDwLtuQJM3MK1klqacMeEnqKQNeknrKgJeknjLgJamnDHhJ6ikDXpJ6yoCXpJ4y4CWppwx4SeopA16SesqAl6SeMuAlqacMeEnqKQNeknrKgJeknjLgJamnDHhJ6ikDXpJ6yoCXpJ7qNOCT7JLkzCTXJrkmybO6bE+SdJ9tO17/h4AvVdURSbYDdui4PUlSo7OAT/JQ4CDg9QBVdSdwZ1ftSZI21+Ue/N7Aj4BTkjwNuAx4a1XdMbxQklXAKoCpqakFN3b40cewfsPGhVe7QFN77s5Zp5069nYlaWu6DPhtgf2Bt1TVRUk+BLwTeNfwQlW1GlgNMD09XQttbP2Gjaw89uQRyl2YNaccP/Y2JamNLg+y3gTcVFUXNdNnMgh8SdIYdBbwVbUR+KckT2hmPR+4uqv2JEmb6/osmrcAn2nOoFkHHNtxe5KkRqcBX1VXANNdtiFJmplXskpSTxnwktRTBrwk9ZQBL0k9ZcBLUk8Z8JLUUwa8JPWUAS9JPWXAS1JPGfCS1FMGvCT1lAEvST1lwEtSTxnwktRTBrwk9ZQBL0k9ZcBLUk8Z8JLUUwa8JPXUvAI+ycOSrOyqGEnS4tlqwCc5P8lDk+wKfAc4JckHuy9NkjSKbVsss3NV3Zrkd4FTqurdSda0WXmSG4HbgHuAu6tqeuGlSpLmo03Ab5tkD+BI4I8X0MbzqurHC3ifJGkEbfrg3wt8Gbiuqi5JsjfwvW7LkiSNqs0e/A+q6lcHVqtq3Tz64As4L0kBH6+q1VsukGQVsApgamqq5Wo1SYcffQzrN2ycSNtTe+7OWaedOpG2peWmTcD/V2D/FvNmcmBV3ZzkkcBXklxbVRcOL9CE/mqA6enparFOTdj6DRtZeezJE2l7zSnHT6RdaTmaNeCTPAt4NvCIJG8feumhwDZtVl5VNzc/b0lyDvAM4MK53yVJWgxz9cFvBzyEwR+BnYYetwJHbG3FSXZMstOm58CLgCtHLViS1M6se/BVdQFwQZJPVdX3F7Du3wDOSbKpndOq6ksLK1OSNF9t+uAflGQ1sGJ4+ao6ZK43VdU64GkjVSdJWrA2Af854GPAJxhcsCRJWgbaBPzdVfXRziuRJC2qNhc6/W2SP0iyR5JdNz06r0ySNJI2e/D/uvn5R0PzCth78cuRJC2WrQZ8VT12HIVIkhZXm+GCd0hyQnMmDUn2SfIvuy9NkjSKNn3wpwB3MriqFeAm4D92VpEkaVG0CfjHVdX7gbsAquoXQDqtSpI0sjYBf2eS7RkcWCXJ44BfdlqVJGlkbc6ieTfwJeDRST4DHAi8vsuiJEmja3MWzVeSXA78NoOumbd6hyZJWvq2GvBJNo37/oPm51SSnYHvV9XdnVUmSRpJmy6ajzC4uccaBnvwT2mePzzJ71fVeR3WJ0laoDYHWW8E9quq6ap6OrAfg3HdXwC8v8PaJEkjaBPwv1VVV22aqKqrGQT+uu7KkiSNqk0XzdokHwXOaKZfBXwvyYNozo2XJC09bfbgXw9cB7wN+HfAOuB1DML9eV0VJkkazVYDvqp+UVUfqKrDquoVwGeBN1fVvVV1e+cVSpIWpM0ePEl2S/KmJBcC5zO436okaQmbtQ8+yU7AYcDRwOOBc4C9q2qvMdUmSRrBXAdZbwEuBk4AvlFVleSw+TaQZBvgUmBDVTnMsCSNyVxdNMcDDwY+ChzXDDK2EG8FrlngeyVJCzRrwFfVf6mqZwKHMriC9VzgUUnekeTxbVaeZC/gZcAnFqFWSdI8tBlsbB3wp8CfJnkq8Grgi0CbPfq/AP4DsNNsCyRZBawCmJqaarFKbXL40cewfsPGsbd7ww03snLsrUqarzYXOv1KVX0X+C6D7ps5Nbf1u6WqLkvy3DnWuRpYDTA9PV3zqefX3foNG1l57Mljb3ftcUeOvU1J89fqNMkFOhA4NMmNDK6CPSTJ/+ywPUnSkM4CvqqOq6q9qmoFcBTwtap6bVftSZI2N2vAJ/lq8/PPxleOJGmxzNUHv0eSgxl0s5zBFjfarqrL2zZSVeczuAJWkjQmcwX8icA7gb2AD27xWgGHdFWUJGl0swZ8VZ0JnJnkXVV10hhrkiQtgjbnwZ+U5FDgoGbW+VX1+W7LkiSNaqtn0SR5H4PhBq5uHm9t5kmSlrA2Fzq9DNi3qu4FSPJp4NvAcV0WJkkaTdvz4HcZer5zB3VIkhZZmz349wHfTvJ1BqdKHoR775K05LU5yHp6kvOBAxgE/DuqavwjXEmS5qXVYGNV9QPgbzquRZK0iLocbEySNEEGvCT11JwBn+QBSa4cVzGSpMUzZ8A3575/J4m3WpKkZabNQdY9gKuSXAzcsWlmVR3aWVWSpJG1Cfj3dl6FJGnRtTkP/oIkjwH2qar/nWQHYJvuS5MkjaLNYGNvBM4EPt7M2hM4t8OaJEmLoM1pkv+WwQ20bwWoqu8Bj+yyKEnS6NoE/C+r6s5NE0m2ZXBHJ0nSEtYm4C9IcjywfZIXAp8D/rbbsiRJo2oT8O8EfgR8F/g94AvACV0WJUkaXZuzaO5tbvJxEYOumbVVtdUumiQPBi4EHtS0c2ZVvXvEeiVJLW014JO8DPgYcD2D4YIfm+T3quqLW3nrL4FDqur2JA8EvpHki1X1rZGrliRtVZsLnT4APK+qrgNI8jjg74A5A77Zy7+9mXxg8/DgrCSNSZuAv2VTuDfWAbe0WXmSbYDLgN8EPlxVF82wzCpgFcDU1PIb8mbd9ddzwMEvnEjbN9xwIysn0vLkTOrzntpzd8467dSxtyuNYtaAT/LK5ulVSb4A/DWDPfB/BVzSZuVVdQ+wb5JdgHOSPKWqrtximdXAaoDp6ellt4d/1z3FymNPnkjba487ciLtTtKkPu81pxw/9jalUc21B/87Q89/CBzcPP8R8LD5NFJVP2tu+/diwOGHJWkMZg34qjp2lBUneQRwVxPu2wMvAP5slHVKktprcxbNY4G3ACuGl28xXPAewKebfvgHAH9dVZ9feKmSpPloc5D1XOCvGFy9em/bFVfVGmC/hZUlSRpVm4D/f1X1l51XIklaVG0C/kNJ3g2cx+DiJQCq6vLOqpIkjaxNwD8VOAY4hPu6aKqZliQtUW0C/jBg7+EhgyVJS1+b0SS/A+zScR2SpEXWZg/+N4Brk1zC5n3wWztNUpI0QW0C3iF+JWkZajMe/AXjKESStLjaXMl6G/cN87sdg2F/76iqh3ZZmCRpNG324Hcank7yCuAZXRUkSVocbc6i2UxVnYvnwEvSktemi+aVQ5MPAKbxzkyStOS1OYtmeFz4u4EbgZd3Uo0kadG06YMfaVx4SdJkzHXLvhPneF9V1Ukd1CNJWiRz7cHfMcO8HYE3AA8HDHhJWsLmumXfBzY9T7IT8FbgWOAM4AOzvU+StDTM2QefZFfg7cBrgE8D+1fVT8dRmCRpNHP1wf858EpgNfDUqrp9bFVJkkY214VOfwg8CjgBuDnJrc3jtiS3jqc8SdJCzdUHP++rXIcleTTwP4DdGdwJanVVfWiUdUqS2mtzodNC3Q38YVVd3hykvSzJV6rq6g7blCQ1RtpLn0tV/WDTjbmr6jbgGmDPrtqTJG2us4AflmQFsB9w0TjakyR120UDQJKHAGcBb6uq+x2cTbIKWAUwNTXVdTnSgqy7/noOOPiFY293as/dOeu0U8fe7iQdfvQxrN+wcSJt33DDjaycSMvd6DTgkzyQQbh/pqrOnmmZqlrN4FRMpqenHaVSS9Jd9xQrjz157O2uOeX4sbc5aes3bJzIZw2w9rgjJ9JuVzrrokkS4K+Aa6rqg121I0maWZd98AcCxwCHJLmieby0w/YkSUM666Kpqm8A6Wr9kqS5jeUsGknS+BnwktRTBrwk9ZQBL0k9ZcBLUk8Z8JLUUwa8JPWUAS9JPWXAS1JPGfCS1FMGvCT1lAEvST1lwEtSTxnwktRTBrwk9ZQBL0k9ZcBLUk8Z8JLUUwa8JPWUAS9JPWXAS1JPdRbwST6Z5JYkV3bVhiRpdl3uwX8KeHGH65ckzaGzgK+qC4GfdLV+SdLctp10AUlWAasApqamJlyNtLSsu/56Djj4hRNpe+PNG9j9UXuOvd0bbriRlWNvtZ8mHvBVtRpYDTA9PV0TLkdaUu66p1h57MkTaXvtcUfyogm0vfa4I8feZl95Fo0k9ZQBL0k91eVpkqcD3wSekOSmJG/oqi1J0v111gdfVa/uat2SpK2zi0aSesqAl6SeMuAlqacMeEnqKQNeknrKgJeknjLgJamnDHhJ6ikDXpJ6yoCXpJ4y4CWppwx4SeopA16SesqAl6SeMuAlqacMeEnqKQNeknrKgJeknjLgJamnDHhJ6ikDXpJ6qtOAT/LiJGuTXJfknV22JUnaXGcBn2Qb4MPAS4AnAa9O8qSu2pMkba7LPfhnANdV1bqquhM4A3h5h+1JkoakqrpZcXIE8OKq+t1m+hjgmVX15i2WWwWsaiafAKxt2cRuwI8XqdylxO1aXtyu5aWP2/WYqnrETC9s22GjmWHe/f6aVNVqYPW8V55cWlXTCylsKXO7lhe3a3np63bNpssumpuARw9N7wXc3GF7kqQhXQb8JcA+SR6bZDvgKOBvOmxPkjSksy6aqro7yZuBLwPbAJ+sqqsWsYl5d+ssE27X8uJ2LS993a4ZdXaQVZI0WV7JKkk9ZcBLUk8t+YDf2nAHSV6TZE3z+IckT5tEnfPVYrte3mzTFUkuTfKcSdQ5X22Hp0hyQJJ7musllrwW39dzk/xz831dkeTESdQ5X22+r2bbrkhyVZILxl3jQrT4vv5o6Lu6svld3HUStXaqqpbsg8HB2euBvYHtgO8AT9pimWcDD2uevwS4aNJ1L9J2PYT7jpGsBK6ddN2LsV1Dy30N+AJwxKTrXqTv67nA5yddawfbtQtwNTDVTD9y0nUvxnZtsfzvAF+bdN1dPJb6HvxWhzuoqn+oqp82k99icL79Utdmu26v5rcP2JEZLhJbgtoOT/EW4CzglnEWN4K+DrvRZruOBs6uqvUAVbUcvrP5fl+vBk4fS2VjttQDfk/gn4amb2rmzeYNwBc7rWhxtNquJIcluRb4O+DfjKm2UWx1u5LsCRwGfGyMdY2q7e/hs5J8J8kXkzx5PKWNpM12PR54WJLzk1yW5HVjq27hWudGkh2AFzPY4eidLocqWAythjsASPI8BgG/HPqq2w7jcA5wTpKDgJOAF3Rd2IjabNdfAO+oqnuSmRZfktps1+UMxgS5PclLgXOBfboubERttmtb4OnA84HtgW8m+VZV/WPXxY2gdW4w6J75+6r6SYf1TMxSD/hWwx0kWQl8AnhJVf3fMdU2inkN41BVFyZ5XJLdqmopD5TUZrumgTOacN8NeGmSu6vq3LFUuDBb3a6qunXo+ReSfKQn39dNwI+r6g7gjiQXAk8DlnLAz+ff11H0tHsGWPIHWbcF1gGP5b6DJU/eYpkp4Drg2ZOud5G36ze57yDr/sCGTdNL9dFmu7ZY/lMsj4Osbb6v3Ye+r2cA6/vwfQFPBL7aLLsDcCXwlEnXPup2NcvtDPwE2HHSNXf1WNJ78DXLcAdJfr95/WPAicDDgY80e4V31xIfLa7ldh0OvC7JXcAvgFdV81u5VLXcrmWn5XYdAbwpyd0Mvq+j+vB9VdU1Sb4ErAHuBT5RVVdOruqtm8fv4WHAeTX430kvOVSBJPXUUj+LRpK0QAa8JPWUAS9JPWXAS1JPGfCS1FMGvDqT5PYO1rlvc6XofN/35CRfS/KPSb6X5F3ZyqW0SR6V5MwW6/5Ckl3mW9MW63hukm9uMW/bJD9Msscc7/n8KO2q3wx4LTf7AvMK+CTbM7gf8H+qqsczuBLz2cAfzPW+qrq5qrY6nHFVvbSqfjafmmZwIbBXkhVD814AXFlVPxhx3fo1ZcCrc82e5vlJzkxybZLPbNp7TnJjkvcmuTzJd5P8VjP/Gc34/t9ufj4hg5u3/wnwqmYc71cl2THJJ5Nc0iw706iBRzMYb+Q8gKr6OfBm4J1NW+9Jcmqzh/+9JG9s5q9IcmXz/PVJzk7ypWaZ9w9t341Jdmuev70ZX/zKJG8bWs81Sf57M6b6ec0fnV+pqnuBzwGvGpp9FHD6TJ/FDJ/xe5L8+6HpKzf9sUjy2iQXN5/Zx5Ns0/a70/JmwGtc9gPeBjyJwTjdBw699uOq2h/4KLAppK4FDqqq/RhcrXxyDYZ+PRH4bFXtW1WfBf6YwVjeBwDPA/48yY5btP1k4LLhGVV1PfCQJA9tZq0EXgY8CzgxyaNm2IZ9GQTwUxn8kRke74QkTweOBZ4J/DbwxiT7NS/vA3y4qp4M/IzBlcpbOp1BqJPkQQz+p3LWTJ/FDO+dUZInNjUfWFX7AvcAr2n7fi1vS3qoAvXKxVV1E0CSK4AVwDea185ufl4GvLJ5vjPw6ST7MBgJ8IGzrPdFwKFDe68PZjA+0TVDy4TZRxPcNP9/VdUvgF8k+TqD8WSu2GLZr1bVPzfbcDXwGDYflvY5wDmbLn1PcjbwLxh0D91QVZvWdxmD7d+8kKpLkjyk2UN/IvCtqvpp84ekzWcxk+czGA3ykuY/TduzfMbh14gMeI3LL4ee38Pmv3u/nGH+ScDXq+qwpqvh/FnWG+Dwqlo7R9tXAQdt9qZkb+D2qrqtCb4t/wDM9Adhrm3YVMtstnzv9rMsdwaDvfgnct8oh20+i7vZ/H/kDx6q6dNVddwctamn7KLRUrUzgxE0AV4/NP82YKeh6S8Dbxnq09+P+/sM8JwkL2iW2R74S+D9Q8u8PMmDkzycwe33LllAzRcCr0iyQ9NNdBjwf+a5jtOB1wKHMNjzh9k/i2E3Mhh1lCT7MxhJEQYjQR6R5JHNa7smecw8a9IyZcBrqXo/8L4kf89gRMBNvg48adNBVgZ7tw8E1jQHRE/ackVN18vLgROSrAW+yyDA/9vQYhczuHPWt4CTqmrW8flnU1WXMxgC+WLgIgYjL357nuu4Gvg5g+MKm0Y5nO2zGHYWsGvT/fUmmvHam/WdAJyXZA3wFWDG0y7VP44mqV97Sd7DoLvmP0+6FmkxuQcvST3lHrwk9ZR78JLUUwa8JPWUAS9JPWXAS1JPGfCS1FP/H23l5KgpFMY7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(s)\n",
    "# plt.title(\"Karate Network Opinion Distribution\")\n",
    "# # Create a histogram\n",
    "plt.hist(s, bins=10, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Innate Opinion Value')\n",
    "plt.ylabel('Number of Agents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Equilibrium & Polarization  - based on derivation\n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\n",
      "0.6590698342810327\n",
      "Equi_polarization:\n",
      "0.4743762518797978\n",
      "Difference:\n",
      "-0.1846935824012349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## s =  make_innat_opinions(n, c1)\n",
    "# print('Innate Opinion')\n",
    "# print(s)\n",
    "# print('Equilibrium Opinion')\n",
    "# print(np.dot(A, s))\n",
    "\n",
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, s, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing players' behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "  \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "    new_op = random.randint(0, 1)  # randomly select an opininon between 0 and 1\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play1(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "#     max_opi_option = random.uniform(0, 1)   # options that maximizer have\n",
    "    \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "#     v = 1\n",
    "    new_op = random.uniform(0, 1)  # randomly select an opininon between 0 and 1\n",
    "    #new_op = 0\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# min_touched =[]\n",
    "# (v1, maxmize_op, innat_equi_por, max_por) = choose_max_vertex(s, n, min_touched)\n",
    "# print(v1, maxmize_op, innat_equi_por, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# (v2, min_opinion, min_pol) = random_play(s,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer_fir_play(s,n,min_touched):    # maxmizer first-time play, greedy algorithm\n",
    "    op = copy.copy(s)\n",
    "\n",
    "    print('Maximizer Play')\n",
    "\n",
    "    max_champion = choose_max_vertex(op, n, min_touched) # The best choice among all opinions and vertexs, function is in \"pure_strategy_selection.ipynb\"\n",
    "    (v1, max_opinion, innate_obj, max_pol) = max_champion # find agent v1, and max_opinion that can maxmize the equi_polarization(max_pol)\n",
    "\n",
    "    if v1 == None:   # if maximizer cannot find one\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "#         print('v1', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(max_champion)\n",
    "\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        ##### change the agent's opinion with best action(agent v1, max_op)\n",
    "        op[v1,0] = max_opinion\n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "# print(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### minimizer first-time play, greedy algorithm\n",
    "def minimizer_fir_play(s,n,max_touched): \n",
    "    \n",
    "    op = copy.copy(s)\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = choose_min_vertex(op, n, max_touched)\n",
    "    (v2, min_opinion, innat_equi_por, min_pol) = min_champion\n",
    "    \n",
    "   #Store innate_op of the min_selected vertex\n",
    "    old_opinion_min = op[v2,0]\n",
    "    \n",
    "    if v2 == None:\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "\n",
    "        ##### change the agent's opinion\n",
    "        op[v2,0] = min_opinion   #-------------------------------------------------> store minimize strategy\n",
    "\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return (v2,min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# print(v2, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing above functions\n",
    "# min_touched=[]\n",
    "# max_touched=[]\n",
    "# # Game start from maximizer random play\n",
    "# print('Maximizer random selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# max_touched.append(v1)\n",
    "# # print('v1, max_opinion, max_pol')\n",
    "# # print(v1, max_opinion, max_pol)\n",
    "# # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Game start from minimizer random play \n",
    "# print('Minimizer random selection')\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# min_touched.append(v2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row are Column are depended on min and max's choice: agent v and opinion \n",
    "def row_index(v2, min_opinion):\n",
    "    row = 11*v2 + min_opinion*10 \n",
    "    return int(row)\n",
    "def column_index(v1,max_opinion):\n",
    "    column = 2*v1 + max_opinion\n",
    "    return int(column)  #the python dataframe index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Strategy Payoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_payoff_row(op1,v2):\n",
    "    payoff_row = np.zeros(2*n)\n",
    "\n",
    "#     print('one opinion changed -min')\n",
    "#     print(op1)\n",
    "    for column in range(2*n):\n",
    "#         print(column)\n",
    "        v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "        max_opinion = column%2\n",
    "#         print(v1, max_opinion)\n",
    "        # update the maximizer's change to the opinion array that has changed by minimizer(op1)\n",
    "        op2 = copy.copy(op1)\n",
    "#         temp = op1[v1]\n",
    "        op2[v1,0] = max_opinion\n",
    "\n",
    "        # calculate the polarization with both max and min's action\n",
    "        payoff_row[column] = obj_polarization(A, L, op2, n)\n",
    "#         op1[v1,0] = temp # restore\n",
    "#         print(op2,payoff_row[column])\n",
    "\n",
    "        ############# CAN DELETE \n",
    "#         if column==33:\n",
    "# #         print('max_opinion')\n",
    "# #         print(v1, max_opinion)\n",
    "#             print('_________________________Payoff row start')\n",
    "#             print('two opinion changed -min +  max')\n",
    "#             print(op2)\n",
    "        \n",
    "    # when v1 == v2, the polarization should be negative for max, infinet for min. \n",
    "    # Replace the the column_index of agent v2 with 0 for max\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "    \n",
    "    return payoff_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.8507    0.7248    0.746     0.6309 -100.     -100.        0.7452\n",
      "    0.6778    0.7674    0.7225    0.8768    0.7506    0.8218    0.6953\n",
      "    0.8079    0.6942    0.7735    0.6947    0.7832    0.6826    0.7835\n",
      "    0.6943    0.7623    0.6737    0.7239    0.6757    0.7744    0.7054\n",
      "    0.8208    0.7314    0.7949    0.677     0.7828    0.7082    0.7929\n",
      "    0.6449    0.8038    0.7076    0.768     0.6796]\n"
     ]
    }
   ],
   "source": [
    "# #(1,0) (2,0.3928571428571428)\n",
    "# op1=copy.copy(s)\n",
    "# print(op1)\n",
    "\n",
    "op1 = copy.copy(s)\n",
    "# print(op1)\n",
    "op1[2,0] = 1  #op1 is the opinion array that updated by minimizer\n",
    "# print(op1)\n",
    "payoff_row_1 = make_payoff_row(op1,2)\n",
    "print(payoff_row_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEEDDDDDDD UPDAE\n",
    "\n",
    "# Calculate polarization of minimizer's Mixed Strategy\n",
    "def mixed_min_polarization(s,v2,weight_op,fla_max_fre):\n",
    "\n",
    "    op1 =  copy.copy(s) # make a copy of the innate opinion array \n",
    "    op1[v2,0] = weight_op # then only updated by minimizer's current change\n",
    "#     print('Min update')\n",
    "#     print(v2, weight_op)\n",
    "    # calculate the polarization with both min(did here) and max's action(in make_payoff_row)\n",
    "    payoff_row = make_payoff_row(op1,v2)  # the vector list out 2*n payoffs after min's action combine with 2*n possible max's actions\n",
    "    #print(payoff_row)\n",
    "\n",
    "    # Replace the the column_index of agent v2 with 100 for min\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = 100\n",
    "    payoff_row[j_2] = 100\n",
    "    \n",
    "#     print('Min Payoff Row')\n",
    "#     print(payoff_row)\n",
    "    #calculate fictitious payoff - equi_min  \n",
    "    payoff_cal = payoff_row * fla_max_fre # fla_max_fre recorded the frequency of each maximizer's action, frequency sum = 1\n",
    "                                             # payoff (2*n array) * maximizer_action_frequency (2*n array)\n",
    "# can DELETE - use to check if function works as expected\n",
    "#     if v2 ==6 and v1==16:\n",
    "#         print('Payoff row')\n",
    "#         column = column_index(16,1)\n",
    "#         print(payoff_row[column],column)\n",
    "#         print('fla_max_fre')\n",
    "#         print(np.nonzero(fla_max_fre))\n",
    "#         print(fla_max_fre [np.nonzero(fla_max_fre)])\n",
    "#         print('compare to: '+str(fla_max_fre[column]))\n",
    "    \n",
    "    mixed_pol = np.sum(payoff_cal) # add up all, calculate average/expected payoff\n",
    "\n",
    "\n",
    "#     print('min_mixed_polarization')\n",
    "#     print(mixed_pol)\n",
    "        # Replace the the column_index of agent v2 with 100 for min\n",
    "\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "\n",
    "    return (mixed_pol,payoff_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # op2=op\n",
    "# # op2[0,0]=1\n",
    "# # min_opinion1 = derivate_s(op2,n,1)\n",
    "# # # print(min_opinion1)\n",
    "# # min_opinion2 = derivate_s1(op2,n,1)\n",
    "# # print(min_opinion2)\n",
    "# v2 = 254\n",
    "# min_opinion = 0\n",
    "# (mixed_pol, payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "# print(np.nonzero(fla_max_fre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_s(op,n,v2):\n",
    "               #op - opinion array that updated by maximizer\n",
    "    c = [1/n] * n\n",
    "#     print(c)\n",
    "    sum_term = 0\n",
    "    j = 0\n",
    "\n",
    "    sum_term = np.dot(np.dot((A-c),(A[v2]-c)),op)  # sum up all terms\n",
    "    \n",
    "    term_out = op[v2]*np.dot((A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "    sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "    s_star = -sum_s/np.dot((A[v2]-c),(A[v2]-c))\n",
    "    s_star = s_star[0] #take value out of array\n",
    "    min_opinion =min(max(0,s_star),1)\n",
    "    \n",
    "#     print('Min opinion-should be unique')\n",
    "#     print(min_opinion)\n",
    "    return min_opinion\n",
    "\n",
    "# def derivate_s1(op,n,v2):\n",
    "#                #op - opinion array that updated by maximizer\n",
    "#     c = [1/n] * n\n",
    "# #     print(c)\n",
    "#     sum_term = 0\n",
    "#     j = 0\n",
    "#     for j in range(0,n):\n",
    "#         term = op[j]*np.dot(np.transpose(A[j]-c),(A[v2]-c))\n",
    "# #             print(A[j])\n",
    "# #             print(A[v])\n",
    "#         sum_term = sum_term + term  # sum up all terms\n",
    "    \n",
    "#     term_out = op[v2]*np.dot(np.transpose(A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "#     sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "#     s_star = -sum_s/np.dot(np.transpose(A[v2]-c),(A[v2]-c))\n",
    "#     s_star = s_star[0] #take value out of array\n",
    "#     min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "#     return min_opinion\n",
    "\n",
    "\n",
    "\n",
    "def min_mixed_opinion(op, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            \n",
    "##             temp = op[v1,0] \n",
    "          \n",
    "##             op[v1,0]= max_opinion #update innate opinion array with max_action  \n",
    "\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "#             print(fla_max_fre[column],min_opinion)\n",
    "            print(min_opinion)\n",
    "            op1 = copy.copy(op)\n",
    "            op1[v2] = min_opinion\n",
    "            min_por = obj_polarization(A, L, op1, n)\n",
    "            #(min_por, row) = mixed_min_polarization(s, v2, min_opinion,fla_max_fre)\n",
    "\n",
    "    \n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "\n",
    "    print('Weighted opinion')\n",
    "    print(weight_op)\n",
    "    \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "    \n",
    "    print('Weighted polarization')\n",
    "    print(mixed_por)\n",
    "    \n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out weighted opinion proved that we need to do this step insetead of min_mixed_opinion - we are weighting\n",
    "# different min_opinion here\n",
    "def min_mixed_opinion_1(s, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            op = copy.copy(s)\n",
    "            op[v1] = max_opinion\n",
    "#             print(op)\n",
    "\n",
    "#             print('Weight')\n",
    "#             print(fla_max_fre[column])\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "\n",
    "\n",
    "            \n",
    "            op1 = copy.copy(op)\n",
    "            op1[v2] = min_opinion   #after max action, update min action on opinion array\n",
    "#             print(min_opinion)\n",
    "            min_por = obj_polarization(A, L, op1, n)\n",
    "            t = 0  \n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "            \n",
    "        #can delete           \n",
    "#             if v2==6 and v1==16:\n",
    "#                 print('Max action')\n",
    "#                 print(v1,max_opinion)\n",
    "#                 print('Weight, Min_opinion')\n",
    "#                 print(fla_max_fre[column],min_opinion)\n",
    "#                 print(op1)\n",
    "            #(min_por, row) = mixed_min_polarization(s, v2, min_opinion,fla_max_fre)\n",
    "\n",
    "#     print(weight_op)\n",
    "\n",
    "#     print('Weighted opinion')\n",
    "#     print(weight_op)\n",
    "    \n",
    "  \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "#     print('Weighted polarization')\n",
    "#     print(mixed_por)\n",
    "#     print('fla_max_fre')\n",
    "#     print(np.nonzero(fla_max_fre))\n",
    "#     print(fla_max_fre [np.nonzero(fla_max_fre)])\n",
    "#         print('compare to: '+str(fla_max_fre[column]))\n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# op=copy.copy(s)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# op[21] = 1\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(op)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # print(21,fla_max_fre)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# v2 = 6\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m (weight_op_1,payoff_row,min_por) \u001b[38;5;241m=\u001b[39m min_mixed_opinion_1(s, n, \u001b[43mv2\u001b[49m, fla_max_fre)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'v2' is not defined"
     ]
    }
   ],
   "source": [
    "# op=copy.copy(s)\n",
    "# op[21] = 1\n",
    "# print(op)\n",
    "# # print(21,fla_max_fre)\n",
    "# v2 = 6\n",
    "(weight_op_1,payoff_row,min_por) = min_mixed_opinion_1(s, n, v2, fla_max_fre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizer search: Go through each agent \n",
    "\n",
    "def mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre):\n",
    "    # current polarization that changed by maximizer, \"innate\" objective that min start with\n",
    "    op = copy.copy(s)\n",
    "    op[v1,0] = max_opinion\n",
    "#     print('Check if op has been updated by Maximizer')\n",
    "#     print(op)\n",
    "    min_por = obj_polarization(A, L, op, n) #min_por- set a standard to compare with pol after min's action\n",
    "    maxup_por = min_por # store innate max updated polarization\n",
    "#     print('check maxup por')\n",
    "#     print(maxup_por)\n",
    "#     payoffs = []    # create an empty list to store all polarizations   \n",
    "    champion = (None, None, 0, None)  # assume the best action is champion\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in max_touched]  # for the vertice that Maximizer has not touched\n",
    "    \n",
    "    for v2 in C1:  \n",
    "#         print('_________________________________')\n",
    "#         print('Min start with agent '+ str(v2) )\n",
    "        (changed_opinion, payoff_row, por) =  min_mixed_opinion_1(s, n, v2, fla_max_fre) # find the best new_op option           \n",
    "#         print('changed opinion, por, Maxup_por')\n",
    "#         print(changed_opinion, por, maxup_por)\n",
    "\n",
    "        if por < min_por:  # if the recent polarization is smaller than the minimum polarization in the history\n",
    "            min_por = por\n",
    "                                 # update the recent option as champion\n",
    "            champion = (v2, changed_opinion, payoff_row, min_por)  \n",
    "#         else:\n",
    "#             print('Innate polarization is smaller than Min action')\n",
    "\n",
    "    return (champion)  # find the best minimizer's action after going through every new_op option of every agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_touched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m      4\u001b[0m max_opinion \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 5\u001b[0m champion \u001b[38;5;241m=\u001b[39m mixed_choose_min_vertex(s, n, v1, max_opinion, \u001b[43mmax_touched\u001b[49m, fla_max_fre)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_touched' is not defined"
     ]
    }
   ],
   "source": [
    "# print('v1,max_opinion')\n",
    "# print(v1,max_opinion)\n",
    "v1 = 16\n",
    "max_opinion = 1\n",
    "champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by maximizer, fla_max_fre includes max's hisotry, so minimizer react to the innate op after that\n",
    "def mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre): \n",
    "\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre)\n",
    "    (v2, min_opinion, payoff_row, min_pol) = min_champion\n",
    "    \n",
    "    if v2 == None:    # if minimizer cannot find a action to minimize polarization after maximizer's action\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "#         print('v2', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(v2, min_opinion, innat_equi_por, min_pol)\n",
    "\n",
    "        # Store innate_op of the min_selected vertex\n",
    "        old_opinion_min = op[v2,0]\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "        print('fla_max_fre')\n",
    "        print(np.nonzero(fla_max_fre))\n",
    "        print(fla_max_fre [np.nonzero(fla_max_fre)])\n",
    "\n",
    "\n",
    "#         print(\"Payoff row\")\n",
    "#         print(payoff_row)\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "    return (v2, payoff_row, min_opinion, min_pol)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_touched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmax_touched\u001b[49m)\n\u001b[0;32m      2\u001b[0m (v2, payoff_row, min_opinion, polarization) \u001b[38;5;241m=\u001b[39m mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_touched' is not defined"
     ]
    }
   ],
   "source": [
    "print(max_touched)\n",
    "(v2, payoff_row, min_opinion, polarization) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "# print('v2, payoff_row, min_opinion, polarization')\n",
    "# print(v2, payoff_row, min_opinion, polarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by minimizer, fla_min_fre includes min's hisotry, so maxmizer react to the innate op after that\n",
    "def mixed_max_polarization(payoff_matrix,v1,max_opinion,fla_min_fre):\n",
    "\n",
    "    # create payoff matrix for maxmizer\n",
    "    column = int(column_index(v1,max_opinion))\n",
    "#     print(payoff_matrix)\n",
    "#     print(\"column\"+str(column))\n",
    "    payoff_vector = payoff_matrix[:,column]\n",
    "    \n",
    "#     print('payoff vector')\n",
    "#     print(payoff_vector)\n",
    "\n",
    "    #calculate fictitious payoff - equi_max   \n",
    "    payoff_cal = payoff_vector * fla_min_fre #payoff * frequency\n",
    "    \n",
    "#     print('max_payoff_calculation')\n",
    "#     print(payoff_cal)\n",
    "    mixed_pol = np.sum(payoff_cal) # add up\n",
    "#     print(\"Max_mixed_polarization\")\n",
    "#     print(mixed_pol)\n",
    "\n",
    "    return mixed_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_pol = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if value of opinion at v should be set to 0 or 1 to maximize equilibrium polarization \n",
    "def max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre):\n",
    "    \n",
    "    por_arr = np.zeros(2)  # create a two_element array to store polarization value of each option\n",
    "\n",
    "\n",
    "    max_opi_option = [0, 1.0]   # Maximizer has two options to change agent v1's opinion\n",
    "    \n",
    "    # objective if set opinion to 0, 1.0\n",
    "    j = 0\n",
    "    for new_op in max_opi_option:\n",
    "#         print('change op to '+ str(i/10))\n",
    "        max_opinion = new_op\n",
    "\n",
    "        por_arr[j] = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)\n",
    "    \n",
    "        j = j + 1   # index increase 1, put the polarization in array\n",
    "\n",
    "#     print('Polarization Options')\n",
    "#     print(por_arr)\n",
    "    \n",
    "    maxmize_op = np.argmax(por_arr)  # the index of maximum polarization = max_opinion --[0,1]\n",
    "    max_por = np.max(por_arr)        # find the maximum polarization in the record\n",
    " \n",
    "#     print('new_op', 'innat_equi_por', 'max_por')\n",
    "#     print(maxmize_op, innat_equi_por, max_por)\n",
    "\n",
    "    return (maxmize_op, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fla_min_fre = [0, 0, 0, 0, 0.65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0, 0, 0]\n",
    "# v1 = 2\n",
    "# champion = max_mixed_opinion(payoff_matrix, n, v1, v2, fla_min_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which agent maximizer should select to maximizer the equilibrium polarization\n",
    "def mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre):\n",
    "#     print('Check if op has been updated by minimizer')\n",
    "#     print(op)\n",
    "    max_por = obj_polarization(A, L, op, n)  # use \"innate\"(after min action) polarization as a comparable standard to find max_por\n",
    "    minup_por = max_por # store innate min_update polarization\n",
    "#     print('check minup por')\n",
    "#     print(minup_por)\n",
    "    champion = (None, None, max_por)  # assume champion is the best action\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in min_touched]  # for the vertice that Minimizer has not touched\n",
    "    for v1 in C1:  \n",
    "#             print('Maximizer start from agent'+str(v1))\n",
    "            (changed_opinion, por) = max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre)\n",
    "#             print('changed_opinion, por, minup_por')\n",
    "#             print(changed_opinion, por,minup_por)\n",
    "            \n",
    "            if por > max_por: # if the polarization of most recent action > maximum polarization of previous actions\n",
    "                max_por = por\n",
    "                champion = (v1, changed_opinion,max_por)   # save the this action as champion    \n",
    "#             else:\n",
    "#                 print('Innate polarization is bigger than max action')\n",
    " \n",
    "    return (champion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'payoff_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpayoff_matrix\u001b[49m)\n\u001b[0;32m      2\u001b[0m champion \u001b[38;5;241m=\u001b[39m mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'payoff_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)\n",
    "champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_touched = []\n",
    "# # payoff_matrix = np.empty((0, 2*n), float)\n",
    "# # fla_min_fre = np.empty((0,n))\n",
    "# # champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)\n",
    "# # print(champion)\n",
    "# print(c1)\n",
    "# vertices = np.where(c1)\n",
    "# print(vertices)\n",
    "# por=0\n",
    "# for i in c1:\n",
    "#     print(i)\n",
    "#     max_por = 0.75\n",
    "#     if por > max_por:\n",
    "#         max_por = por\n",
    "#         print('yes')\n",
    "#     else:\n",
    "#         print('por<max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre): \n",
    "    op = copy.copy(s)   # op is a copy of innate opinion\n",
    "    \n",
    "    #update innat opinion \n",
    "    op[v2,0] = min_opinion  # Op has been updated by minimizer, so maximizer react to the innate op after that\n",
    "    \n",
    "\n",
    "    max_champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre) # The best choice among all opinions and vertexs\n",
    "    (v1, max_opinion, max_pol) = max_champion\n",
    "\n",
    "    if v1 == None:\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        \n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Testing function -- NO NEED TO RUN\n",
    "# min_touched = []\n",
    "# v2 = 0\n",
    "# min_opinion = 0\n",
    "# b = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "# print('v1,max_opinion,max_pol')\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Player's Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\n",
      "0.6590698342810327\n",
      "Equi_polarization:\n",
      "0.4743762518797978\n",
      "Difference:\n",
      "-0.1846935824012349\n"
     ]
    }
   ],
   "source": [
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, op, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = 'Karate'\n",
    "memory = 50\n",
    "\n",
    "\n",
    "# with open('Network_'+str(Network)+'.txt', \"a\") as fi:\n",
    "#     print('Innate Opinion', file=fi)\n",
    "#     print(s, file=fi)\n",
    "#     print('Adjacency Matrix', file=fi)\n",
    "#     print(G,file=fi)\n",
    "\n",
    "# Game Preparation\n",
    "def push(obj, element):\n",
    "    if len(obj) >= memory:\n",
    "        obj.pop(0)\n",
    "        print('pop')\n",
    "    obj.append(element)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameters\n",
    "Game_rounds =301 # Rounds + 1- use for printing data\n",
    "memory = 5\n",
    "def all_fre_limited_touch(s, n):\n",
    "    # Preparation for the game\n",
    "    op = copy.copy(s)\n",
    "    payoff_matrix = np.empty((0, 2*n), float)\n",
    "    max_history = np.zeros([n, 2])  # n*2 matrix, agent i & opinion options\n",
    "    min_history = []  # append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "#     print(type(min_history))\n",
    "\n",
    "    max_history_last_100 = np.zeros([n, 2]) \n",
    "    min_history_last_100= []\n",
    "\n",
    "    max_touched = []\n",
    "    min_touched = []\n",
    "    min_touched_all = []\n",
    "    min_touched_last_100 =[]\n",
    "    print('min_touched')\n",
    "    print(min_touched)\n",
    "    \n",
    "    \n",
    "    # Game start from maximizer random play\n",
    "    print('Maximizer first selection')\n",
    "    (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action \n",
    "    #(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "#     (v1, max_opinion, max_pol) = (11, 1, 0.14833274000237331)\n",
    "    First_max = (v1, max_opinion, max_pol) \n",
    "\n",
    "\n",
    "#     (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    # Maximizer start with greedy play\n",
    "    # (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "    max_touched.append(v1)    # save Maximizer's action history\n",
    "\n",
    "    # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "    max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "    # print('max_history')\n",
    "    # print(max_history)\n",
    "    print('history at spot')\n",
    "    print(max_history[v1,int(max_opinion)])\n",
    "\n",
    "    max_frequency = max_history/1  # its frequency, only played  1 time so far, divided by 1 \n",
    "    # print('fre_max at spot')\n",
    "    # print(max_frequency[v1,int(max_opinion)])\n",
    "\n",
    "    fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "                                            # so we can multiply the freuency (2n*1)with payoff array (1*2n) \n",
    "                                            # to get average payoff of fictitious play\n",
    "    print('fre_max at spot')\n",
    "    print(fla_max_fre)\n",
    "\n",
    "    column = int(column_index(v1,max_opinion))    # the frequency of maximizer's most recent action (v1,max_opinion)\n",
    "\n",
    "    print(fla_max_fre[column])\n",
    "\n",
    "    # print(np.shape(fla_max_fre.shape))\n",
    "\n",
    "\n",
    "    # if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "    print('Minimizer first selection')\n",
    "    (v2, min_opinion, min_pol) = random_play(op,n) \n",
    "    #(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "    \n",
    "#     (v2, min_opinion, min_pol) = (29, 1, 0.5933309600094931)\n",
    "    First_min = (v2, min_opinion, min_pol)\n",
    "\n",
    "    if v1==v2:   # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "        sys.exit()\n",
    "\n",
    "    # Minimizer start with greedy play\n",
    "    # (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    min_touched.append(v2)\n",
    "   \n",
    "\n",
    "    # store minimizer play history\n",
    "    min_history.append((v2,min_opinion))\n",
    "    print('min_history')\n",
    "    print(min_history)\n",
    "\n",
    "\n",
    "    counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/1 #return only frequency of all min options in order\n",
    "#     print('fla_min_fre')\n",
    "#     print(fla_min_fre)\n",
    "\n",
    "\n",
    "    (a,payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "    payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "#     print('Payoff Matrix')\n",
    "#     print(payoff_matrix)\n",
    "    print('fla_min_fre at the spot')\n",
    "    min_counter = dict(counter)\n",
    "    print(min_counter) \n",
    "    print(min_counter[(v2,min_opinion)]) \n",
    "#     print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "\n",
    "    equi_min = min_pol\n",
    "    equi_max = max_pol\n",
    "    # print(equi_min)\n",
    "    # print(equi_max)\n",
    "\n",
    "\n",
    "\n",
    "    Flag = 0\n",
    "\n",
    "    i = 0\n",
    "    while Flag == 0: \n",
    "        i = i + 1\n",
    "        print(\"Game \" + str(i))\n",
    "        print(\"_____________________\")\n",
    "\n",
    "    #     if max_pol == min_pol:\n",
    "        if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "                                # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "            print('min_recent_'+str(memory)+'_touched')# then stop at Game 202\n",
    "            print(min_touched)\n",
    "            print('max_recent_'+str(memory)+'_touched')\n",
    "            print(max_touched)\n",
    "            print('Min last 100 action')\n",
    "            print(min_touched_last_100)\n",
    "\n",
    "            break\n",
    "\n",
    "        elif equi_min == equi_max:\n",
    "            print(\"Reached Nash Equilibrium at game\"+ str(i) + \"and Equi_Por = \" + str(equi_min))\n",
    "            print('max_distribution')\n",
    "            print(max_frequency)\n",
    "            print('min_distribution')\n",
    "            print(fla_min_fre)\n",
    "            Flag = 1\n",
    "            break\n",
    "        ############################## maximizer play  \n",
    "        else:\n",
    "            if i == Game_rounds-100:    #if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "    #             max_touched_100 = max_touched \n",
    "    #             min_touched_100 = min_touched\n",
    "    #             max_fre_100 = max_frequency  # store the max_frequency of first 100 iterataions\n",
    "    #             print('max_history')\n",
    "    #             print(max_history)\n",
    "    #             min_fre_100 = fla_min_fre  # max_frequency of first 100 iterations\n",
    "    #             print('min_history')\n",
    "    #             print(min_history)\n",
    "    # Remove max frequncy less than 0.1--\n",
    "                max_history_last_100 = np.zeros([n, 2]) \n",
    "                min_history_last_100 = [] \n",
    "                min_touched_last_100 =[]\n",
    "\n",
    "            (v1, max_opinion, equi_max) = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "            max_touched = push(max_touched, v1)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "    #             print('equi_max')\n",
    "    #             print(equi_max)\n",
    "    #         print(v1, max_opinion, max_pol)\n",
    "            # cumulate strategy \n",
    "            max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "\n",
    "            max_history_last_100[v1,int(max_opinion)] = max_history_last_100[v1,int(max_opinion)] +1\n",
    "    #         print('max_history')\n",
    "    #         print(max_history)\n",
    "    #________________________________________________________________\n",
    "            max_frequency = max_history/(i+1)  # its frequency \n",
    "    #         print('max_distribution')\n",
    "    #         print(max_frequency)\n",
    "        #     print(i+1) \n",
    "            fla_max_fre = max_frequency.flatten() #flaten max_frequency to calculate average payoff\n",
    "#             print('fla_max_fre')\n",
    "#             print(fla_max_fre)\n",
    "            print('fre_max at spot')\n",
    "            print(fla_max_fre[column])\n",
    "            # create payoff matrix for maxmizer\n",
    "            row = int(row_index(v2, min_opinion))\n",
    "            column = int(column_index(v1,max_opinion))\n",
    "\n",
    "    # _________________________________________________________________\n",
    "    #         ######################Visualize Maximizer's selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v1:\n",
    "    #                 color_map.append('Red')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True,node_size = 50)\n",
    "    #         plt.figure(figsize=(200, 200))\n",
    "    #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    ############################### minimizer play\n",
    "            (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "            min_touched = push(min_touched, v2)\n",
    "            min_touched_all.append(v2) \n",
    "            min_touched_last_100.append(v2)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('equi_min')\n",
    "    #         print(equi_min)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "            #         print(v2, min_opinion, min_pol)\n",
    "            if (v2,min_opinion) in counter.keys():\n",
    "                payoff_matrix = payoff_matrix # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "                print(\"Same history\")\n",
    "                print((str(v2),str(min_opinion)))\n",
    "            else:\n",
    "                payoff_matrix = np.vstack([payoff_matrix, payoff_row]) # if this is a new option, append to previous matrix\n",
    "    #                 print('payoff_row')\n",
    "    #                 print(payoff_row)\n",
    "            min_history.append((v2,min_opinion))\n",
    "            min_history_last_100.append((v2,min_opinion))\n",
    "            #         print('min_history')\n",
    "            #         print(min_history)\n",
    "            counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "            #print(counter)\n",
    "    #         print('counter.keys')\n",
    "    #         print(counter.keys())\n",
    "            fla_min_fre = np.array(list(counter.values()))/(i+1) #return only frequency of all min options in order\n",
    "    #         print('fla_min_fre')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "    #         print('fla_min_fre at the spot')\n",
    "    #         min_counter = dict(counter)\n",
    "    #         print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "            # create payoff matrix for minimizer\n",
    "            row = row_index(v2, min_opinion)\n",
    "            column = column_index(v1,max_opinion)\n",
    "            #     print('row, column')\n",
    "            #     print(row, column)\n",
    "\n",
    "            print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max)) \n",
    "    #         print('min_distribution')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "            ######################Visualize Minimizer selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G1, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G1)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v2:\n",
    "    #                 color_map.append('Blue')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True)\n",
    "    #         plt.figure(figsize=(25, 25))\n",
    "    #         plt.show()\n",
    "    return (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, fla_max_fre, max_history_last_100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.     0.0243 0.     0.     0.     0.     0.     0.0272 0.     0.\n",
      "  0.     0.0385 0.     0.0091 0.     0.     0.     0.016  0.0074 0.0236]\n",
      " [0.0243 0.     0.     0.     0.0345 0.     0.     0.     0.     0.\n",
      "  0.     0.     0.0094 0.     0.     0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.0313 0.0235 0.     0.     0.     0.     0.\n",
      "  0.0189 0.0054 0.     0.     0.     0.0324 0.     0.     0.     0.005 ]\n",
      " [0.     0.     0.0313 0.     0.0288 0.0391 0.0002 0.     0.     0.0051\n",
      "  0.0254 0.     0.0293 0.     0.     0.0071 0.     0.     0.     0.0319]\n",
      " [0.     0.0345 0.0235 0.0288 0.     0.0354 0.     0.     0.0185 0.\n",
      "  0.0368 0.0301 0.     0.0341 0.0006 0.0387 0.0345 0.     0.     0.    ]\n",
      " [0.     0.     0.     0.0391 0.0354 0.     0.     0.     0.     0.0046\n",
      "  0.     0.     0.016  0.03   0.     0.     0.0388 0.     0.     0.    ]\n",
      " [0.     0.     0.     0.0002 0.     0.     0.     0.     0.     0.\n",
      "  0.0026 0.     0.     0.     0.0379 0.     0.0364 0.0067 0.     0.0016]\n",
      " [0.0272 0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  0.0094 0.     0.0068 0.0395 0.     0.0214 0.     0.0126 0.     0.    ]\n",
      " [0.     0.     0.     0.     0.0185 0.     0.     0.     0.     0.0368\n",
      "  0.0363 0.     0.0168 0.022  0.0173 0.0324 0.     0.034  0.0193 0.021 ]\n",
      " [0.     0.     0.     0.0051 0.     0.0046 0.     0.     0.0368 0.\n",
      "  0.     0.0377 0.     0.0089 0.0176 0.     0.     0.     0.0185 0.0354]\n",
      " [0.     0.     0.0189 0.0254 0.0368 0.     0.0026 0.0094 0.0363 0.\n",
      "  0.     0.     0.     0.     0.     0.     0.0144 0.0373 0.     0.0246]\n",
      " [0.0385 0.     0.0054 0.     0.0301 0.     0.     0.     0.     0.0377\n",
      "  0.     0.     0.     0.0344 0.     0.     0.     0.     0.0295 0.    ]\n",
      " [0.     0.0094 0.     0.0293 0.     0.016  0.     0.0068 0.0168 0.\n",
      "  0.     0.     0.     0.     0.     0.0208 0.0266 0.     0.017  0.0167]\n",
      " [0.0091 0.     0.     0.     0.0341 0.03   0.     0.0395 0.022  0.0089\n",
      "  0.     0.0344 0.     0.     0.     0.0123 0.0135 0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.0006 0.     0.0379 0.     0.0173 0.0176\n",
      "  0.     0.     0.     0.     0.     0.     0.0373 0.     0.0237 0.    ]\n",
      " [0.     0.     0.0324 0.0071 0.0387 0.     0.     0.0214 0.0324 0.\n",
      "  0.     0.     0.0208 0.0123 0.     0.     0.     0.0231 0.     0.    ]\n",
      " [0.     0.     0.     0.     0.0345 0.0388 0.0364 0.     0.     0.\n",
      "  0.0144 0.     0.0266 0.0135 0.0373 0.     0.     0.0309 0.0118 0.    ]\n",
      " [0.016  0.     0.     0.     0.     0.     0.0067 0.0126 0.034  0.\n",
      "  0.0373 0.     0.     0.     0.     0.0231 0.0309 0.     0.     0.    ]\n",
      " [0.0074 0.     0.     0.     0.     0.     0.     0.     0.0193 0.0185\n",
      "  0.     0.0295 0.017  0.     0.0237 0.     0.0118 0.     0.     0.0058]\n",
      " [0.0236 0.     0.005  0.0319 0.     0.     0.0016 0.     0.021  0.0354\n",
      "  0.0246 0.     0.0167 0.     0.     0.     0.     0.     0.0058 0.    ]]\n"
     ]
    }
   ],
   "source": [
    "G = make_random_network(n, p1)\n",
    "# G[G.nonzero()] = 1\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################ Make Innate Opinion ################################\n",
    "### Generate innate opinion array\n",
    "s = make_innat_opinions(n) \n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[17] =0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_touched\n",
      "[]\n",
      "Maximizer first selection\n",
      "    Agent0 's opinion 0.20738777069383113 changed to 1\n",
      "Network reaches equilibrium Polarization: 0.6547087520613768\n",
      "history at spot\n",
      "1.0\n",
      "fre_max at spot\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "1.0\n",
      "Minimizer first selection\n",
      "    Agent1 's opinion 0.6355612075286321 changed to 0\n",
      "Network reaches equilibrium Polarization: 0.645381419963736\n",
      "min_history\n",
      "[(1, 0)]\n",
      "Counter({(1, 0): 1})\n",
      "fla_min_fre at the spot\n",
      "{(1, 0): 1}\n",
      "1\n",
      "Game 1\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.47816777367986585\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6299603732791976 and Equi_Max = 0.9196972328332491\n",
      "Game 2\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "fre_max at spot\n",
      "0.6666666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4746482974178964\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.333 0.667]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6450127628675191 and Equi_Max = 0.7974208132523205\n",
      "Game 3\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "fre_max at spot\n",
      "0.75\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.47288855928691165\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.25 0.75]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6525322447548725 and Equi_Max = 0.7566470891546611\n",
      "Game 4\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "fre_max at spot\n",
      "0.8\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.47183271640832086\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.2 0.8]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.657041785757106 and Equi_Max = 0.736256311243527\n",
      "Game 5\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.8333333333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4711288211559269\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.167 0.833]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6600472513710208 and Equi_Max = 0.7240203945089763\n",
      "Game 6\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.8571428571428571\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4706260388327884\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.143 0.857]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6621935741299029 and Equi_Max = 0.7158624603128322\n",
      "Game 7\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.47024895209043455\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.125 0.875]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.663803076452393 and Equi_Max = 0.7100350252250578\n",
      "Game 8\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.8888888888888888\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4699556624019371\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.111 0.889]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6650547695200058 and Equi_Max = 0.7056642562556517\n",
      "Game 9\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4697210306511391\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.1 0.9]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6660560344686719 and Equi_Max = 0.7022646518726404\n",
      "Game 10\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9090909090909091\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46952905921866805\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.091 0.909]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6668751920677133 and Equi_Max = 0.6995448927838734\n",
      "Game 11\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9166666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4693690830249421\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.083 0.917]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.667557782715964 and Equi_Max = 0.6973195845090667\n",
      "Game 12\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9230769230769231\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46923371855332796\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.077 0.923]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6681353305300816 and Equi_Max = 0.6954651255020864\n",
      "Game 13\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9285714285714286\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46911769186337293\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.071 0.929]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.66863035043698 and Equi_Max = 0.693895942418397\n",
      "Game 14\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9333333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46901713539874523\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.067 0.933]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6690593519524082 and Equi_Max = 0.6925509095861455\n",
      "Game 15\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.468929148492196\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.062 0.938]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6694347162910742 and Equi_Max = 0.6913852003388351\n",
      "Game 16\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9411764705882353\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46885151298641725\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.059 0.941]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6697659108280882 and Equi_Max = 0.6903651939088911\n",
      "Game 17\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9444444444444444\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46878250364794727\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.056 0.944]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6700602986595663 and Equi_Max = 0.6894651797810833\n",
      "Game 18\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9473684210526315\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4687207584503688\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.053 0.947]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6703236924644347 and Equi_Max = 0.688665160531824\n",
      "Game 19\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.95\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4686651877725483\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.05 0.95]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.670560742178004 and Equi_Max = 0.6879493484703895\n",
      "Game 20\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9523809523809523\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46861490954023444\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.048 0.952]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6707752118828157 and Equi_Max = 0.687305113263796\n",
      "Game 21\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9545454545454546\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46856920205631275\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.045 0.955]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.670970181171532 and Equi_Max = 0.686722230224864\n",
      "Game 22\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9565217391304348\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46852746913621035\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.043 0.957]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6711481939722125 and Equi_Max = 0.6861923336024852\n",
      "Game 23\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9583333333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46848921395944987\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.042 0.958]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6713113701617339 and Equi_Max = 0.6857085124850006\n",
      "Game 24\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.96\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4684540191968301\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.04 0.96]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6714614903881544 and Equi_Max = 0.6852650077230673\n",
      "Game 25\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9615384615384616\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46842153172364276\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.038 0.962]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6716000613159966 and Equi_Max = 0.6848569815877824\n",
      "Game 26\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9629629629629629\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46839145072995064\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.037 0.963]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6717283663706588 and Equi_Max = 0.6844803405823219\n",
      "Game 27\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9642857142857143\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4683635183786652\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.036 0.964]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6718475056076355 and Equi_Max = 0.6841315976271172\n",
      "Game 28\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9655172413793104\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4683375123964339\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.034 0.966]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6719584273319166 and Equi_Max = 0.6838077637755772\n",
      "Game 29\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9666666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46831324014635134\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.033 0.967]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6720619533927517 and Equi_Max = 0.6835062623325663\n",
      "Game 30\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.967741935483871\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46829053384788705\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.032 0.968]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6721587995820925 and Equi_Max = 0.6832248601485275\n",
      "Game 31\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.96875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4682692466930767\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.031 0.969]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6722495922078948 and Equi_Max = 0.6829616122110193\n",
      "Game 32\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9696969696969697\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.468249249668861\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.03 0.97]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6723348816537718 and Equi_Max = 0.6827148166249626\n",
      "Game 33\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9705882352941176\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46823042894018735\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.029 0.971]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6724151535455102 and Equi_Max = 0.6824829777780063\n",
      "Game 34\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9714285714285714\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46821268368172364\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.029 0.971]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.672490838003136 and Equi_Max = 0.6822647760058976\n",
      "Game 35\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9722222222222222\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4681959242709523\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.028 0.972]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6725623173508124 and Equi_Max = 0.6820590424579022\n",
      "Game 36\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.972972972972973\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4681800707742768\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.027 0.973]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.672629932576365 and Equi_Max = 0.6818647381511793\n",
      "Game 37\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9736842105263158\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46816505167216316\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.026 0.974]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6726939887707851 and Equi_Max = 0.6816809364216405\n",
      "Game 38\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9743589743589743\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46815080278041427\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.026 0.974]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6727547597308108 and Equi_Max = 0.6815068081456345\n",
      "Game 39\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.975\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46813726633325287\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.025 0.975]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6728124918710573 and Equi_Max = 0.6813416092351534\n",
      "Game 40\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.975609756097561\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4681243902005871\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.024 0.976]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6728674075635934 and Equi_Max = 0.681184670008697\n",
      "Game 41\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9761904761904762\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4681121272170959\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.024 0.976]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6729197080003899 and Equi_Max = 0.6810353861175811\n",
      "Game 42\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9767441860465116\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4681004346049299\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.023 0.977]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6729695756563304 and Equi_Max = 0.6808932107684441\n",
      "Game 43\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9772727272727273\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4680892734751351\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.023 0.977]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6730171764163231 and Equi_Max = 0.6807576480309598\n",
      "Game 44\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9777777777777777\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4680786083955534\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.022 0.978]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.673062661418779 and Equi_Max = 0.6806282470581064\n",
      "Game 45\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9782608695652174\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4680684070150839\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.022 0.978]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6731061686586165 and Equi_Max = 0.6805045970770476\n",
      "Game 46\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9787234042553191\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46805863973591094\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.021 0.979]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6731478243856119 and Equi_Max = 0.6803863230333587\n",
      "Game 47\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9791666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4680492794267036\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.021 0.979]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6731877443279434 and Equi_Max = 0.6802730817912956\n",
      "Game 48\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9795918367346939\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4680403011709333\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.02 0.98]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6732260347658996 and Equi_Max = 0.680164558809017\n",
      "Game 49\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.98\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4680316820453938\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.02 0.98]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6732627934767393 and Equi_Max = 0.680060465220913\n",
      "Game 50\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9803921568627451\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46802340092477734\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.02 0.98]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6732981105683775 and Equi_Max = 0.6799605352700451\n",
      "Game 51\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9807692307692307\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46801543830880005\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.019 0.981]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6733320692168757 and Equi_Max = 0.679864524042645\n",
      "Game 52\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9811320754716981\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4680077761688974\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.019 0.981]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6733647463204319 and Equi_Max = 0.6797722054640121\n",
      "Game 53\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9814814814814815\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46800039781195407\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.019 0.981]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6733962130807044 and Equi_Max = 0.6796833705212836\n",
      "Game 54\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9818181818181818\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46799328775889953\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.018 0.982]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6734265355207119 and Equi_Max = 0.6795978256836714\n",
      "Game 55\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9821428571428571\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46798643163631126\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.018 0.982]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6734557749472426 and Equi_Max = 0.6795153914950328\n",
      "Game 56\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9824561403508771\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679798160794279\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.018 0.982]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6734839883645836 and Equi_Max = 0.6794359013172335\n",
      "Game 57\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9827586206896551\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46797342864519564\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.017 0.983]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6735112288454489 and Equi_Max = 0.6793592002057853\n",
      "Game 58\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9830508474576272\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679672577341577\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.017 0.983]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6735375458641849 and Equi_Max = 0.679285143901795\n",
      "Game 59\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9833333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679612925201544\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.017 0.983]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6735629855966513 and Equi_Max = 0.6792135979264242\n",
      "Game 60\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9836065573770492\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46795552288693804\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.016 0.984]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6735875911906049 and Equi_Max = 0.6791444367659027\n",
      "Game 61\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9838709677419355\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679499393709223\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.016 0.984]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6736114030099157 and Equi_Max = 0.6790775431367005\n",
      "Game 62\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9841269841269841\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679445331093831\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.016 0.984]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6736344588555233 and Equi_Max = 0.6790128073218139\n",
      "Game 63\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.984375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46793929579351706\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.016 0.984]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6736567941656754 and Equi_Max = 0.6789501265702573\n",
      "Game 64\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9846153846153847\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46793421962583154\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.015 0.985]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6736784421976854 and Equi_Max = 0.6788894045528562\n",
      "Game 65\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9848484848484849\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679292972814092\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.015 0.985]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6736994341931619 and Equi_Max = 0.6788305508682688\n",
      "Game 66\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9850746268656716\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679245218726412\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.015 0.985]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6737197995284443 and Equi_Max = 0.6787734805939183\n",
      "Game 67\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9852941176470589\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46791988691707237\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.015 0.985]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6737395658517601 and Equi_Max = 0.6787181138771358\n",
      "Game 68\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9855072463768116\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46791538630804175\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6737587592084585 and Equi_Max = 0.6786643755623816\n",
      "Game 69\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9857142857142858\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679110142878406\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6737774041555065 and Equi_Max = 0.6786121948508785\n",
      "Game 70\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9859154929577465\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46790676542313797\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6737955238663104 and Equi_Max = 0.6785615049894201\n",
      "Game 71\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9861111111111112\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4679026345824549\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6738131402268018 and Equi_Max = 0.6785122429854772\n",
      "Game 72\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9863013698630136\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678986169154892\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6738302739236236 and Equi_Max = 0.6784643493460434\n",
      "Game 73\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9864864864864865\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46789470783411713\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.673846944525168 and Equi_Max = 0.6784177678379467\n",
      "Game 74\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9866666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467890902994915\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6738631705561292 and Equi_Max = 0.6783724452675941\n",
      "Game 75\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9868421052631579\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678871982830603\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.673878969566173 and Equi_Max = 0.678328331278335\n",
      "Game 76\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.987012987012987\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678835897974875\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6738943581932503 and Equi_Max = 0.6782853781638258\n",
      "Game 77\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9871794871794872\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46788007383718594\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6739093522220461 and Equi_Max = 0.6782435406959327\n",
      "Game 78\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9873417721518988\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678766468885374\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6739239666379832 and Equi_Max = 0.6782027759658757\n",
      "Game 79\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678733056136052\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.013 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.673938215677181 and Equi_Max = 0.6781630432374364\n",
      "Game 80\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9876543209876543\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678700468392885\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.673952112872709 and Equi_Max = 0.6781243038111748\n",
      "Game 81\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9878048780487805\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678668675472723\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6739656710974579 and Equi_Max = 0.678086520898704\n",
      "Game 82\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9879518072289156\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678637648647022\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6739789026039119 and Equi_Max = 0.678049659506165\n",
      "Game 83\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9880952380952381\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46786073605552675\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Reached Nash Equilibrium at Equi_Min = 0.6739918190610789 and Equi_Max = 0.6780136863261241\n",
      "Game 84\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9882352941176471\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678577785124494\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740044315888133 and Equi_Max = 0.6779785696371907\n",
      "Game 85\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9883720930232558\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46785488974944367\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740167507897453 and Equi_Max = 0.6779442792107245\n",
      "Game 86\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9885057471264368\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678520673947829\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740287867790088 and Equi_Max = 0.6779107862240468\n",
      "Game 87\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9886363636363636\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678493091845463\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740405492119391 and Equi_Max = 0.6778780631796397\n",
      "Game 88\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9887640449438202\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678466129565621\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740520473099075 and Equi_Max = 0.6778460838298558\n",
      "Game 89\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9888888888888889\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46784397664475547\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740632898844293 and Equi_Max = 0.6778148231066984\n",
      "Game 90\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.989010989010989\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46784139827386756\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740742853596854 and Equi_Max = 0.6777842570562894\n",
      "Game 91\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9891304347826086\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678388759545206\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740850417935743 and Equi_Max = 0.6777543627776532\n",
      "Game 92\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.989247311827957\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46783640787860065\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6740955668974041 and Equi_Max = 0.6777251183654953\n",
      "Game 93\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9893617021276596\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678339923149342\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741058680543308 and Equi_Max = 0.6776965028566708\n",
      "Game 94\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9894736842105263\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46783162760523966\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741159523366274 and Equi_Max = 0.677668496180072\n",
      "Game 95\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9895833333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678293121603305\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741258265218792 and Equi_Max = 0.6776410791096784\n",
      "Game 96\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9896907216494846\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678270444565535\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741354971081723 and Equi_Max = 0.6776142332205439\n",
      "Game 97\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9897959183673469\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678248230324454\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741449703283556 and Equi_Max = 0.6775879408475031\n",
      "Game 98\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.98989898989899\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467822646485592\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741542521634365 and Equi_Max = 0.6775621850464087\n",
      "Game 99\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.99\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46782051346967557\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741633483551736 and Equi_Max = 0.6775369495577168\n",
      "Game 100\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9900990099009901\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678184226916982\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741722644179206 and Equi_Max = 0.6775122187722562\n",
      "Game 101\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9901960784313726\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46781637290936745\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674181005649775 and Equi_Max = 0.6774879776990332\n",
      "Game 102\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9902912621359223\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46781436292863526\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741895771430758 and Equi_Max = 0.6774642119349295\n",
      "Game 103\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9903846153846154\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678123916013788\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6741979837942957 and Equi_Max = 0.6774409076361628\n",
      "Game 104\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9904761904761905\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678104578232129\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674206230313369 and Equi_Max = 0.677418051491399\n",
      "Game 105\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9905660377358491\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46780856053142744\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742143212324906 and Equi_Max = 0.6773956306963951\n",
      "Game 106\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9906542056074766\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46780669870303987\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742222609144214 and Equi_Max = 0.6773736329300809\n",
      "Game 107\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9907407407407407\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678048713529557\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742300535603342 and Equi_Max = 0.6773520463319831\n",
      "Game 108\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9908256880733946\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467803077532231\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742377032172246 and Equi_Max = 0.6773308594808984\n",
      "Game 109\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.990909090909091\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4678013163264286\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742452137849202 and Equi_Max = 0.6773100613747459\n",
      "Game 110\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.990990990990991\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677995868540639\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742525890227091 and Equi_Max = 0.6772896414115156\n",
      "Game 111\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9910714285714286\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677978882651344\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674259832555616 and Equi_Max = 0.6772695893712474\n",
      "Game 112\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9911504424778761\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46779621973972574\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742669478803407 and Equi_Max = 0.6772498953989761\n",
      "Game 113\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9912280701754386\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677945804866927\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742739383708893 and Equi_Max = 0.67723054998858\n",
      "Game 114\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.991304347826087\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677929697424081\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742808072839064 and Equi_Max = 0.6772115439674823\n",
      "Game 115\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9913793103448276\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46779138676957654\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6742875577637355 and Equi_Max = 0.6771928684821489\n",
      "Game 116\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9914529914529915\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46778983085610976\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674294192847218 and Equi_Max = 0.6771745149843359\n",
      "Game 117\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9915254237288136\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46778830131405763\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743007154682484 and Equi_Max = 0.6771564752180449\n",
      "Game 118\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9915966386554622\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677867974785946\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743071284620996 and Equi_Max = 0.6771387412071378\n",
      "Game 119\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9916666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677853187070559\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674313434569533 and Equi_Max = 0.6771213052435795\n",
      "Game 120\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9917355371900827\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677838643780221\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743196364407031 and Equi_Max = 0.677104159876267\n",
      "Game 121\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9918032786885246\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677824338904477\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743257366388722 and Equi_Max = 0.6770872979004112\n",
      "Game 122\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.991869918699187\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46778102666283394\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743317376439418 and Equi_Max = 0.6770707123474441\n",
      "Game 123\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9919354838709677\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46777964213243983\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743376418558139 and Equi_Max = 0.6770543964754148\n",
      "Game 124\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.992\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467778279754532\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743434515975917 and Equi_Max = 0.6770383437598526\n",
      "Game 125\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9920634920634921\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677769390016703\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743491691186276 and Equi_Max = 0.6770225478850679\n",
      "Game 126\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9921259842519685\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46777561936302686\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743547965974248 and Equi_Max = 0.6770070027358667\n",
      "Game 127\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9921875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677743203437373\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674360336144408 and Equi_Max = 0.6769917023896577\n",
      "Game 128\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9922480620155039\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677730414642816\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743657898045596 and Equi_Max = 0.6769766411089295\n",
      "Game 129\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9923076923076923\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677717822598945\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743711595599381 and Equi_Max = 0.6769618133340749\n",
      "Game 130\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9923664122137404\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677705422800018\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743764473320791 and Equi_Max = 0.6769472136765509\n",
      "Game 131\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9924242424242424\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677693210876833\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743816549842884 and Equi_Max = 0.676932836912348\n",
      "Game 132\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9924812030075187\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46776811825915904\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743867843238314 and Equi_Max = 0.6769186779757588\n",
      "Game 133\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9925373134328358\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677669333832994\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743918371040237 and Equi_Max = 0.676904731953426\n",
      "Game 134\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9925925925925926\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46776576606115605\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6743968150262287 and Equi_Max = 0.6768909940786589\n",
      "Game 135\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9926470588235294\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46776461590551494\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744017197417698 and Equi_Max = 0.6768774597260008\n",
      "Game 136\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9927007299270073\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46776348254046707\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744065528537536 and Equi_Max = 0.6768641244060356\n",
      "Game 137\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9927536231884058\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677623656009996\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744113159188203 and Equi_Max = 0.6768509837604246\n",
      "Game 138\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9928057553956835\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677612647326036\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744160104488125 and Equi_Max = 0.6768380335571567\n",
      "Game 139\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9928571428571429\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467760179590899\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744206379123763 and Equi_Max = 0.6768252696860035\n",
      "Game 140\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9929078014184397\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677591098412753\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744251997364933 and Equi_Max = 0.6768126881541712\n",
      "Game 141\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9929577464788732\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46775805515854774\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744296973079473 and Equi_Max = 0.6768002850821353\n",
      "Game 142\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.993006993006993\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677570152266275\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744341319747302 and Equi_Max = 0.676788056699653\n",
      "Game 143\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9930555555555556\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677559897382062\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744385050473884 and Equi_Max = 0.6767759993419417\n",
      "Game 144\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.993103448275862\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46775497839445274\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744428178003145 and Equi_Max = 0.6767641094460173\n",
      "Game 145\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9931506849315068\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46775398090472325\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744470714729854 and Equi_Max = 0.6767523835471844\n",
      "Game 146\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9931972789115646\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677529969862827\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744512672711502 and Equi_Max = 0.6767408182756696\n",
      "Game 147\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9932432432432432\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677520263640373\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744554063679687 and Equi_Max = 0.6767294103533933\n",
      "Game 148\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9932885906040269\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46775106877027833\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744594899051068 and Equi_Max = 0.6767181565908722\n",
      "Game 149\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9933333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677501239444362\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744635189937838 and Equi_Max = 0.676707053884245\n",
      "Game 150\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9933774834437086\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677491916328436\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744674947157833 and Equi_Max = 0.6766960992124206\n",
      "Game 151\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.993421052631579\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46774827158850885\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.67447141812442 and Equi_Max = 0.676685289634335\n",
      "Game 152\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9934640522875817\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677473635708974\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744752902454698 and Equi_Max = 0.6766746222863209\n",
      "Game 153\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9935064935064936\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46774646734572256\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744791120780649 and Equi_Max = 0.6766640943795769\n",
      "Game 154\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9935483870967742\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677455826847433\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744828845955523 and Equi_Max = 0.6766537031977373\n",
      "Game 155\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9935897435897436\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46774470936557166\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744866087463216 and Equi_Max = 0.6766434460945361\n",
      "Game 156\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9936305732484076\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677438471714849\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744902854545987 and Equi_Max = 0.6766333204915591\n",
      "Game 157\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9936708860759493\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677429958912474\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744939156212106 and Equi_Max = 0.6766233238760845\n",
      "Game 158\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9937106918238994\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677421553189374\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6744975001243216 and Equi_Max = 0.6766134537990021\n",
      "Game 159\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.99375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46774132525378126\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745010398201421 and Equi_Max = 0.6766037078728148\n",
      "Game 160\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9937888198757764\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677405054999936\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745045355436082 and Equi_Max = 0.6765940837697122\n",
      "Game 161\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9938271604938271\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46773969586662295\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745079881090418 and Equi_Max = 0.6765845792197172\n",
      "Game 162\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9938650306748467\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46773889616740416\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745113983107789 and Equi_Max = 0.6765751920089014\n",
      "Game 163\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9939024390243902\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46773810622061485\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745147669237829 and Equi_Max = 0.6765659199776648\n",
      "Game 164\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9939393939393939\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677373258489381\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745180947042283 and Equi_Max = 0.6765567610190809\n",
      "Game 165\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9939759036144579\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46773655487932986\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745213823900691 and Equi_Max = 0.6765477130772994\n",
      "Game 166\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9940119760479041\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677357931428905\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745246307015834 and Equi_Max = 0.6765387741460084\n",
      "Game 167\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9940476190476191\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46773504047474207\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745278403418987 and Equi_Max = 0.6765299422669504\n",
      "Game 168\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9940828402366864\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467734296713909\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745310119975008 and Equi_Max = 0.6765212155284936\n",
      "Game 169\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9941176470588236\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677335617032034\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745341463387226 and Equi_Max = 0.676512592064251\n",
      "Game 170\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9941520467836257\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46773283528911425\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674537244020215 and Equi_Max = 0.6765040700517506\n",
      "Game 171\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9941860465116279\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46773211732170056\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745403056814062 and Equi_Max = 0.6764956477111504\n",
      "Game 172\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9942196531791907\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677314076544882\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745433319469379 and Equi_Max = 0.6764873233040007\n",
      "Game 173\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9942528735632183\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677307061443702\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745463234270932 and Equi_Max = 0.6764790951320461\n",
      "Game 174\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9942857142857143\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677300126515107\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745492807182051 and Equi_Max = 0.6764709615360712\n",
      "Game 175\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9943181818181818\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677293270392518\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745522044030525 and Equi_Max = 0.6764629208947861\n",
      "Game 176\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9943502824858758\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46772864917402424\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745550950512453 and Equi_Max = 0.6764549716237471\n",
      "Game 177\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9943820224719101\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677279789252598\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745579532195918 and Equi_Max = 0.6764471121743176\n",
      "Game 178\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.994413407821229\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677273161653084\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745607794524582 and Equi_Max = 0.6764393410326612\n",
      "Game 179\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9944444444444445\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46772666076935643\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745635742821131 and Equi_Max = 0.6764316567187704\n",
      "Game 180\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.994475138121547\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46772601261534874\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745663382290616 and Equi_Max = 0.6764240577855257\n",
      "Game 181\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9945054945054945\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677253715839125\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745690718023696 and Equi_Max = 0.676416542817789\n",
      "Game 182\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.994535519125683\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677247375582843\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745717754999754 and Equi_Max = 0.6764091104315231\n",
      "Game 183\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9945652173913043\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46772411042423906\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745744498089923 and Equi_Max = 0.6764017592729428\n",
      "Game 184\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9945945945945946\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46772349007002134\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745770952060008 and Equi_Max = 0.6763944880176934\n",
      "Game 185\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9946236559139785\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677228763862791\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745797121573334 and Equi_Max = 0.6763872953700543\n",
      "Game 186\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9946524064171123\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677222692659992\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745823011193458 and Equi_Max = 0.6763801800621695\n",
      "Game 187\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9946808510638298\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677216686044458\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745848625386848 and Equi_Max = 0.6763731408533029\n",
      "Game 188\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9947089947089947\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677210742990993\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745873968525447 and Equi_Max = 0.6763661765291171\n",
      "Game 189\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9947368421052631\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677204862495986\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745899044889136 and Equi_Max = 0.6763592859009744\n",
      "Game 190\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9947643979057592\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46771990435768424\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745923858668186 and Equi_Max = 0.676352467805261\n",
      "Game 191\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9947916666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467719328527144\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674594841396555 and Equi_Max = 0.6763457211027322\n",
      "Game 192\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9948186528497409\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46771875866375956\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745972714799157 and Equi_Max = 0.6763390446778766\n",
      "Game 193\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9948453608247423\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677181946752555\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6745996765104085 and Equi_Max = 0.6763324374383028\n",
      "Game 194\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9948717948717949\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677176364712488\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746020568734693 and Equi_Max = 0.6763258983141424\n",
      "Game 195\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9948979591836735\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677170839632014\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746044129466676 and Equi_Max = 0.6763194262574729\n",
      "Game 196\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9949238578680203\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677165370643728\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746067450999069 and Equi_Max = 0.6763130202417578\n",
      "Game 197\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9949494949494949\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677159956897747\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746090536956167 and Equi_Max = 0.6763066792613042\n",
      "Game 198\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9949748743718593\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677154597561274\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746113390889427 and Equi_Max = 0.676300402330736\n",
      "Game 199\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.995\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677149291818165\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746136016279262 and Equi_Max = 0.6762941884844829\n",
      "Game 200\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9950248756218906\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677144038868521\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746158416536833 and Equi_Max = 0.6762880367762863\n",
      "Game 201\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.995049504950495\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46771388379282786\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746180595005749 and Equi_Max = 0.6762819462787188\n",
      "Game 202\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9950738916256158\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677133688228826\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746202554963725 and Equi_Max = 0.6762759160827175\n",
      "Game 203\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9950980392156863\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677128589016624\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746224299624225 and Equi_Max = 0.6762699452971321\n",
      "Game 204\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9951219512195122\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677123539552834\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746245832137988 and Equi_Max = 0.6762640330482852\n",
      "Game 205\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9951456310679612\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677118539112964\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746267155594591 and Equi_Max = 0.6762581784795475\n",
      "Game 206\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9951690821256038\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677113586986522\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746288273023899 and Equi_Max = 0.6762523807509223\n",
      "Game 207\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9951923076923077\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677108682476681\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746309187397507 and Equi_Max = 0.6762466390386455\n",
      "Game 208\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9952153110047847\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677103824899948\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746329901630143 and Equi_Max = 0.6762409525347934\n",
      "Game 209\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9952380952380953\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770990135858515\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746350418581009 and Equi_Max = 0.676235320446906\n",
      "Game 210\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.995260663507109\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677094247876627\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674637074105511 and Equi_Max = 0.6762297419976175\n",
      "Game 211\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9952830188679245\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677089527126924\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746390871804523 and Equi_Max = 0.6762242164242978\n",
      "Game 212\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9953051643192489\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770848507035095\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746410813529636 and Equi_Max = 0.676218742978707\n",
      "Game 213\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9953271028037384\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770802179849863\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746430568880374 and Equi_Max = 0.6762133209266563\n",
      "Game 214\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9953488372093023\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770756283615195\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746450140457346 and Equi_Max = 0.6762079495476797\n",
      "Game 215\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9953703703703703\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677071081234566\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746469530813011 and Equi_Max = 0.6762026281347155\n",
      "Game 216\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9953917050691244\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677066576016617\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746488742452774 and Equi_Max = 0.6761973559937955\n",
      "Game 217\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9954128440366973\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770621121309425\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746507777836068 and Equi_Max = 0.6761921324437434\n",
      "Game 218\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9954337899543378\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677057689011347\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746526639377405 and Equi_Max = 0.6761869568158815\n",
      "Game 219\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9954545454545455\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467705330610193\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746545329447396 and Equi_Max = 0.6761818284537453\n",
      "Game 220\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.995475113122172\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677048962856851\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746563850373745 and Equi_Max = 0.6761767467128061\n",
      "Game 221\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9954954954954955\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770446587401066\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746582204442207 and Equi_Max = 0.676171710960201\n",
      "Game 222\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9955156950672646\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677040393225306\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746600393897553 and Equi_Max = 0.6761667205744688\n",
      "Game 223\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9955357142857143\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770361657954596\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746618420944451 and Equi_Max = 0.6761617749452975\n",
      "Game 224\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9955555555555555\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770319759427664\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746636287748383 and Equi_Max = 0.6761568734732714\n",
      "Game 225\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.995575221238938\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770278231684154\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746653996436506 and Equi_Max = 0.6761520155696327\n",
      "Game 226\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9955947136563876\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770237069823856\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746671549098483 and Equi_Max = 0.6761472006560427\n",
      "Game 227\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9956140350877193\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770196269032505\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746688947787324 and Equi_Max = 0.6761424281643549\n",
      "Game 228\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9956331877729258\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770155824579945\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746706194520178 and Equi_Max = 0.6761376975363889\n",
      "Game 229\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9956521739130435\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770115731818274\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746723291279108 and Equi_Max = 0.6761330082237152\n",
      "Game 230\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9956709956709957\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4677007598618008\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746740240011857 and Equi_Max = 0.6761283596874418\n",
      "Game 231\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9956896551724138\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46770036583176705\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746757042632588 and Equi_Max = 0.6761237513980078\n",
      "Game 232\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9957081545064378\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676999751839653\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.67467737010226 and Equi_Max = 0.6761191828349828\n",
      "Game 233\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9957264957264957\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769958787503363\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746790217031037 and Equi_Max = 0.6761146534868712\n",
      "Game 234\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9957446808510638\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769920386234815\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746806592475557 and Equi_Max = 0.6761101628509203\n",
      "Game 235\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9957627118644068\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676988231040075\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746822829143017 and Equi_Max = 0.676105710432935\n",
      "Game 236\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9957805907172996\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769844555881745\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746838928790119 and Equi_Max = 0.6761012957470964\n",
      "Game 237\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9957983193277311\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467698071186276\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746854893144032 and Equi_Max = 0.676096918315785\n",
      "Game 238\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.99581589958159\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769769994655924\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674687072390303 and Equi_Max = 0.6760925776694086\n",
      "Game 239\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9958333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676973318005067\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746886422737067 and Equi_Max = 0.6760882733462341\n",
      "Game 240\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.995850622406639\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769696670960814\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746901991288391 and Equi_Max = 0.676084004892224\n",
      "Game 241\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9958677685950413\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676966046359898\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746917431172097 and Equi_Max = 0.6760797718608764\n",
      "Game 242\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9958847736625515\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769624554240113\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.67469327439767 and Equi_Max = 0.67607557381307\n",
      "Game 243\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9959016393442623\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769588939220264\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674694793126467 and Equi_Max = 0.6760714103169112\n",
      "Game 244\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9959183673469387\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676955361493526\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746962994572968 and Equi_Max = 0.6760672809475874\n",
      "Game 245\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9959349593495935\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769518577839575\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746977935413565 and Equi_Max = 0.676063185287221\n",
      "Game 246\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9959514170040485\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769483824445063\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6746992755273943 and Equi_Max = 0.676059122924729\n",
      "Game 247\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9959677419354839\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769449351319864\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747007455617594 and Equi_Max = 0.6760550934556866\n",
      "Game 248\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9959839357429718\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769415155087235\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747022037884499 and Equi_Max = 0.6760510964821906\n",
      "Game 249\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.996\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769381232424473\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747036503491594 and Equi_Max = 0.6760471316127308\n",
      "Game 250\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9960159362549801\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769347580061804\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747050853833247 and Equi_Max = 0.6760431984620607\n",
      "Game 251\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.996031746031746\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769314194781386\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747065090281684 and Equi_Max = 0.6760392966510733\n",
      "Game 252\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9960474308300395\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676928107341623\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747079214187435 and Equi_Max = 0.676035425806679\n",
      "Game 253\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9960629921259843\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676924821284922\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747093226879762 and Equi_Max = 0.6760315855616869\n",
      "Game 254\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.996078431372549\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769215610012144\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747107129667079 and Equi_Max = 0.6760277755546885\n",
      "Game 255\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.99609375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769183261884734\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747120923837351 and Equi_Max = 0.6760239954299447\n",
      "Game 256\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9961089494163424\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676915116549373\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.67471346106585 and Equi_Max = 0.6760202448372746\n",
      "Game 257\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9961240310077519\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676911931791195\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747148191378788 and Equi_Max = 0.6760165234319475\n",
      "Game 258\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9961389961389961\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676908771625745\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.67471616672272 and Equi_Max = 0.6760128308745774\n",
      "Game 259\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9961538461538462\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769056357692596\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747175039413813 and Equi_Max = 0.6760091668310197\n",
      "Game 260\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9961685823754789\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46769025239423256\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674718830913016 and Equi_Max = 0.6760055309722706\n",
      "Game 261\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9961832061068703\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676899435869797\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747201477549575 and Equi_Max = 0.6760019229743685\n",
      "Game 262\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9961977186311787\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768963712807077\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747214545827561 and Equi_Max = 0.6759983425182974\n",
      "Game 263\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9962121212121212\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768933299082044\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747227515102096 and Equi_Max = 0.6759947892898939\n",
      "Game 264\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9962264150943396\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768903114894544\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747240386493992 and Equi_Max = 0.6759912629797538\n",
      "Game 265\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9962406015037594\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676887315765582\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747253161107203 and Equi_Max = 0.6759877632831437\n",
      "Game 266\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9962546816479401\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768843424815904\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747265840029147 and Equi_Max = 0.6759842898999124\n",
      "Game 267\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.996268656716418\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676881391386284\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747278424331007 and Equi_Max = 0.6759808425344058\n",
      "Game 268\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9962825278810409\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676878462232207\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747290915068039 and Equi_Max = 0.6759774208953817\n",
      "Game 269\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9962962962962963\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676875554775568\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747303313279869 and Equi_Max = 0.6759740246959292\n",
      "Game 270\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.996309963099631\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676872668776173\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747315619990768 and Equi_Max = 0.6759706536533876\n",
      "Game 271\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9963235294117647\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676869803997362\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747327836209958 and Equi_Max = 0.6759673074892687\n",
      "Game 272\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9963369963369964\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768669602059415\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747339962931861 and Equi_Max = 0.6759639859291794\n",
      "Game 273\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9963503649635036\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676864137172122\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747352001136393 and Equi_Max = 0.675960688702747\n",
      "Game 274\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9963636363636363\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676861334669459\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747363951789206 and Equi_Max = 0.6759574155435457\n",
      "Game 275\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9963768115942029\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768585524747847\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747375815841964 and Equi_Max = 0.6759541661890254\n",
      "Game 276\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9963898916967509\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.467685579036816\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747387594232583 and Equi_Max = 0.6759509403804406\n",
      "Game 277\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9964028776978417\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676853048132806\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747399287885492 and Equi_Max = 0.6759477378627825\n",
      "Game 278\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.996415770609319\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676850325555052\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747410897711852 and Equi_Max = 0.6759445583847112\n",
      "Game 279\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9964285714285714\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676847622424282\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747422424609824 and Equi_Max = 0.6759414016984908\n",
      "Game 280\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.99644128113879\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676844938532878\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747433869464771 and Equi_Max = 0.6759382675599235\n",
      "Game 281\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9964539007092199\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676842273676164\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747445233149503 and Equi_Max = 0.6759351557282882\n",
      "Game 282\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9964664310954063\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768396276523594\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747456516524497 and Equi_Max = 0.6759320659662786\n",
      "Game 283\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9964788732394366\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676837000262526\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747467720438103 and Equi_Max = 0.6759289980399417\n",
      "Game 284\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9964912280701754\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768343913105154\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.004 0.996]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747478845726762 and Equi_Max = 0.6759259517186205\n",
      "Game 285\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9965034965034965\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768318006029247\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674748989321523 and Equi_Max = 0.6759229267748945\n",
      "Game 286\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9965156794425087\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768292279490453\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747500863716754 and Equi_Max = 0.6759199229845243\n",
      "Game 287\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9965277777777778\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676826673160818\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747511758033294 and Equi_Max = 0.6759169401263956\n",
      "Game 288\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9965397923875432\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768241360527857\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747522576955707 and Equi_Max = 0.6759139779824652\n",
      "Game 289\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.996551724137931\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676821616442051\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Reached Nash Equilibrium at Equi_Min = 0.674753332126394 and Equi_Max = 0.6759110363377081\n",
      "Game 290\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9965635738831615\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676819114148228\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747543991727223 and Equi_Max = 0.6759081149800646\n",
      "Game 291\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9965753424657534\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676816628993404\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747554589104244 and Equi_Max = 0.6759052137003903\n",
      "Game 292\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9965870307167235\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768141608020936\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747565114143342 and Equi_Max = 0.6759023322924053\n",
      "Game 293\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9965986394557823\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768117094012013\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747575567582657 and Equi_Max = 0.6758994705526464\n",
      "Game 294\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9966101694915255\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676809274619975\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.674758595015034 and Equi_Max = 0.675896628280418\n",
      "Game 295\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9966216216216216\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46768068562899734\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747596262564688 and Equi_Max = 0.6758938052777466\n",
      "Game 296\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9966329966329966\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676804454245023\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747606505534336 and Equi_Max = 0.675891001349333\n",
      "Game 297\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9966442953020134\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676802068321179\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747616679758394 and Equi_Max = 0.675888216302509\n",
      "Game 298\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9966555183946488\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46767996983566923\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747626785926627 and Equi_Max = 0.6758854499471922\n",
      "Game 299\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9966666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.46767973441919686\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747636824719601 and Equi_Max = 0.6758827020958434\n",
      "Game 300\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent5 's opinion 0.5519151463585465 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9966777408637874\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent17 's opinion 0.3293933849667663 changed to 0.4676795005669535\n",
      "fla_max_fre\n",
      "(array([ 1, 11], dtype=int64),)\n",
      "[0.003 0.997]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.6747646796808839 and Equi_Max = 0.6758799725634238\n",
      "Game 301\n",
      "_____________________\n",
      "min_recent_5_touched\n",
      "[17, 17, 17, 17, 17]\n",
      "max_recent_5_touched\n",
      "[5, 5, 5, 5, 5]\n",
      "Min last 100 action\n",
      "[17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]\n"
     ]
    }
   ],
   "source": [
    "Experiment = 10\n",
    "\n",
    "Experiment_note = str('Note: This experiement has initial condition. Game round:'+str(Game_rounds)+'.')\n",
    "(First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, fla_max_fre, max_history_last_100) = all_fre_limited_touch(s, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_distribution\n",
      "[1.]\n",
      "(array([5], dtype=int64), array([1], dtype=int64))\n",
      "Min_distribution_last_100\n",
      "dict_keys([17])\n",
      "fla_min_fre\n",
      "[1.]\n",
      "dict_keys([(1, 0), (17, 0.47816777367986585), (17, 0.4746482974178964), (17, 0.47288855928691165), (17, 0.47183271640832086), (17, 0.4711288211559269), (17, 0.4706260388327884), (17, 0.47024895209043455), (17, 0.4699556624019371), (17, 0.4697210306511391), (17, 0.46952905921866805), (17, 0.4693690830249421), (17, 0.46923371855332796), (17, 0.46911769186337293), (17, 0.46901713539874523), (17, 0.468929148492196), (17, 0.46885151298641725), (17, 0.46878250364794727), (17, 0.4687207584503688), (17, 0.4686651877725483), (17, 0.46861490954023444), (17, 0.46856920205631275), (17, 0.46852746913621035), (17, 0.46848921395944987), (17, 0.4684540191968301), (17, 0.46842153172364276), (17, 0.46839145072995064), (17, 0.4683635183786652), (17, 0.4683375123964339), (17, 0.46831324014635134), (17, 0.46829053384788705), (17, 0.4682692466930767), (17, 0.468249249668861), (17, 0.46823042894018735), (17, 0.46821268368172364), (17, 0.4681959242709523), (17, 0.4681800707742768), (17, 0.46816505167216316), (17, 0.46815080278041427), (17, 0.46813726633325287), (17, 0.4681243902005871), (17, 0.4681121272170959), (17, 0.4681004346049299), (17, 0.4680892734751351), (17, 0.4680786083955534), (17, 0.4680684070150839), (17, 0.46805863973591094), (17, 0.4680492794267036), (17, 0.4680403011709333), (17, 0.4680316820453938), (17, 0.46802340092477734), (17, 0.46801543830880005), (17, 0.4680077761688974), (17, 0.46800039781195407), (17, 0.46799328775889953), (17, 0.46798643163631126), (17, 0.4679798160794279), (17, 0.46797342864519564), (17, 0.4679672577341577), (17, 0.4679612925201544), (17, 0.46795552288693804), (17, 0.4679499393709223), (17, 0.4679445331093831), (17, 0.46793929579351706), (17, 0.46793421962583154), (17, 0.4679292972814092), (17, 0.4679245218726412), (17, 0.46791988691707237), (17, 0.46791538630804175), (17, 0.4679110142878406), (17, 0.46790676542313797), (17, 0.4679026345824549), (17, 0.4678986169154892), (17, 0.46789470783411713), (17, 0.467890902994915), (17, 0.4678871982830603), (17, 0.4678835897974875), (17, 0.46788007383718594), (17, 0.4678766468885374), (17, 0.4678733056136052), (17, 0.4678700468392885), (17, 0.4678668675472723), (17, 0.4678637648647022), (17, 0.46786073605552675), (17, 0.4678577785124494), (17, 0.46785488974944367), (17, 0.4678520673947829), (17, 0.4678493091845463), (17, 0.4678466129565621), (17, 0.46784397664475547), (17, 0.46784139827386756), (17, 0.4678388759545206), (17, 0.46783640787860065), (17, 0.4678339923149342), (17, 0.46783162760523966), (17, 0.4678293121603305), (17, 0.4678270444565535), (17, 0.4678248230324454), (17, 0.467822646485592), (17, 0.46782051346967557), (17, 0.4678184226916982), (17, 0.46781637290936745), (17, 0.46781436292863526), (17, 0.4678123916013788), (17, 0.4678104578232129), (17, 0.46780856053142744), (17, 0.46780669870303987), (17, 0.4678048713529557), (17, 0.467803077532231), (17, 0.4678013163264286), (17, 0.4677995868540639), (17, 0.4677978882651344), (17, 0.46779621973972574), (17, 0.4677945804866927), (17, 0.4677929697424081), (17, 0.46779138676957654), (17, 0.46778983085610976), (17, 0.46778830131405763), (17, 0.4677867974785946), (17, 0.4677853187070559), (17, 0.4677838643780221), (17, 0.4677824338904477), (17, 0.46778102666283394), (17, 0.46777964213243983), (17, 0.467778279754532), (17, 0.4677769390016703), (17, 0.46777561936302686), (17, 0.4677743203437373), (17, 0.4677730414642816), (17, 0.4677717822598945), (17, 0.4677705422800018), (17, 0.4677693210876833), (17, 0.46776811825915904), (17, 0.4677669333832994), (17, 0.46776576606115605), (17, 0.46776461590551494), (17, 0.46776348254046707), (17, 0.4677623656009996), (17, 0.4677612647326036), (17, 0.467760179590899), (17, 0.4677591098412753), (17, 0.46775805515854774), (17, 0.4677570152266275), (17, 0.4677559897382062), (17, 0.46775497839445274), (17, 0.46775398090472325), (17, 0.4677529969862827), (17, 0.4677520263640373), (17, 0.46775106877027833), (17, 0.4677501239444362), (17, 0.4677491916328436), (17, 0.46774827158850885), (17, 0.4677473635708974), (17, 0.46774646734572256), (17, 0.4677455826847433), (17, 0.46774470936557166), (17, 0.4677438471714849), (17, 0.4677429958912474), (17, 0.4677421553189374), (17, 0.46774132525378126), (17, 0.4677405054999936), (17, 0.46773969586662295), (17, 0.46773889616740416), (17, 0.46773810622061485), (17, 0.4677373258489381), (17, 0.46773655487932986), (17, 0.4677357931428905), (17, 0.46773504047474207), (17, 0.467734296713909), (17, 0.4677335617032034), (17, 0.46773283528911425), (17, 0.46773211732170056), (17, 0.4677314076544882), (17, 0.4677307061443702), (17, 0.4677300126515107), (17, 0.4677293270392518), (17, 0.46772864917402424), (17, 0.4677279789252598), (17, 0.4677273161653084), (17, 0.46772666076935643), (17, 0.46772601261534874), (17, 0.4677253715839125), (17, 0.4677247375582843), (17, 0.46772411042423906), (17, 0.46772349007002134), (17, 0.4677228763862791), (17, 0.4677222692659992), (17, 0.4677216686044458), (17, 0.4677210742990993), (17, 0.4677204862495986), (17, 0.46771990435768424), (17, 0.467719328527144), (17, 0.46771875866375956), (17, 0.4677181946752555), (17, 0.4677176364712488), (17, 0.4677170839632014), (17, 0.4677165370643728), (17, 0.4677159956897747), (17, 0.4677154597561274), (17, 0.4677149291818165), (17, 0.4677144038868521), (17, 0.46771388379282786), (17, 0.4677133688228826), (17, 0.4677128589016624), (17, 0.4677123539552834), (17, 0.4677118539112964), (17, 0.4677113586986522), (17, 0.4677108682476681), (17, 0.4677103824899948), (17, 0.46770990135858515), (17, 0.4677094247876627), (17, 0.4677089527126924), (17, 0.46770848507035095), (17, 0.46770802179849863), (17, 0.46770756283615195), (17, 0.4677071081234566), (17, 0.4677066576016617), (17, 0.46770621121309425), (17, 0.4677057689011347), (17, 0.467705330610193), (17, 0.4677048962856851), (17, 0.46770446587401066), (17, 0.4677040393225306), (17, 0.46770361657954596), (17, 0.46770319759427664), (17, 0.46770278231684154), (17, 0.46770237069823856), (17, 0.46770196269032505), (17, 0.46770155824579945), (17, 0.46770115731818274), (17, 0.4677007598618008), (17, 0.46770036583176705), (17, 0.4676999751839653), (17, 0.46769958787503363), (17, 0.46769920386234815), (17, 0.4676988231040075), (17, 0.46769844555881745), (17, 0.467698071186276), (17, 0.46769769994655924), (17, 0.4676973318005067), (17, 0.46769696670960814), (17, 0.4676966046359898), (17, 0.46769624554240113), (17, 0.46769588939220264), (17, 0.4676955361493526), (17, 0.46769518577839575), (17, 0.46769483824445063), (17, 0.46769449351319864), (17, 0.46769415155087235), (17, 0.46769381232424473), (17, 0.46769347580061804), (17, 0.46769314194781386), (17, 0.4676928107341623), (17, 0.4676924821284922), (17, 0.46769215610012144), (17, 0.46769183261884734), (17, 0.4676915116549373), (17, 0.4676911931791195), (17, 0.4676908771625745), (17, 0.46769056357692596), (17, 0.46769025239423256), (17, 0.4676899435869797), (17, 0.46768963712807077), (17, 0.46768933299082044), (17, 0.46768903114894544), (17, 0.4676887315765582), (17, 0.46768843424815904), (17, 0.4676881391386284), (17, 0.4676878462232207), (17, 0.4676875554775568), (17, 0.4676872668776173), (17, 0.4676869803997362), (17, 0.46768669602059415), (17, 0.4676864137172122), (17, 0.4676861334669459), (17, 0.46768585524747847), (17, 0.467685579036816), (17, 0.4676853048132806), (17, 0.4676850325555052), (17, 0.4676847622424282), (17, 0.4676844938532878), (17, 0.4676842273676164), (17, 0.46768396276523594), (17, 0.4676837000262526), (17, 0.46768343913105154), (17, 0.46768318006029247), (17, 0.46768292279490453), (17, 0.4676826673160818), (17, 0.46768241360527857), (17, 0.4676821616442051), (17, 0.4676819114148228), (17, 0.4676816628993404), (17, 0.46768141608020936), (17, 0.46768117094012013), (17, 0.4676809274619975), (17, 0.46768068562899734), (17, 0.4676804454245023), (17, 0.4676802068321179), (17, 0.46767996983566923), (17, 0.46767973441919686), (17, 0.4676795005669535)])\n",
      "Min_distribution_all\n",
      "[0.997]\n",
      "Max_distribution_all\n",
      "[0.003 0.997]\n",
      "[(array([0, 5], dtype=int64), array([1, 1], dtype=int64))]\n"
     ]
    }
   ],
   "source": [
    "# MAXimizer's distribution of LAST 100 iteration \n",
    "print('Max_distribution')  \n",
    "max_l100_fre = max_history_last_100/100\n",
    "print(max_l100_fre [np.nonzero(max_l100_fre)])\n",
    "# print for small network\n",
    "#print(max_history_last_100)\n",
    "# # Print for Large Network\n",
    "print(np.nonzero(max_l100_fre))\n",
    "\n",
    "# MINimizer's Strategy in the last 100 round\n",
    "print('Min_distribution_last_100')\n",
    "counter_h=collections.Counter(min_touched_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_h.keys())\n",
    "counter=collections.Counter(min_touched_last_100)\n",
    "fla_min_fre = np.array(list(counter.values()))/(100) #return only frequency of all min options in order\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)\n",
    "# print(min_touched_last_100)\n",
    "\n",
    "\n",
    "counter_1h=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_1h.keys())\n",
    "counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "# print(counter_1)\n",
    "fla_min_fre_1 = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "print('Min_distribution_all')\n",
    "print(fla_min_fre_1)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "max_fre = max_history/Game_rounds\n",
    "print('Max_distribution_all')\n",
    "print(max_fre[np.nonzero(max_fre)])\n",
    "print([np.nonzero(max_fre)])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________Min Analyze__________________________________________\n",
      "17\n",
      "0\n",
      "{0: 0.20738777069383113, 8: 0.24272274993289872, 9: 0.2437569052037651, 6: 0.2543059654403661, 15: 0.26744380515632193, 12: 0.31775214730228263, 14: 0.3186368506482441, 13: 0.346609103825711, 18: 0.4515598070614181, 3: 0.5409266275725597, 4: 0.5461173513670674, 5: 0.5519151463585465, 19: 0.5876798368598868, 11: 0.5901839082988651, 10: 0.6316911101556144, 1: 0.6355612075286321, 16: 0.643511457683655, 2: 0.6646651201512839, 7: 0.7112627748447193, 17: 0.8}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"___________________Min Analyze__________________________________________\")\n",
    "print(np.argmax(s))\n",
    "print(np.argmin(s))\n",
    "\n",
    "s_aa = s[:, 0]\n",
    "my_dict = {index: value for index, value in enumerate(s_aa)}\n",
    "sorting_s = sorted(my_dict.items(), key=lambda x:x[1])\n",
    "sorted_S = dict(sorting_s)\n",
    "print(sorted_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Analyze Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxG = nx.from_numpy_matrix(G) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________Degree Centrality_____________________________\n",
      "{1: 0.15789473684210525, 2: 0.3157894736842105, 5: 0.3157894736842105, 6: 0.3157894736842105, 7: 0.3157894736842105, 11: 0.3157894736842105, 14: 0.3157894736842105, 0: 0.3684210526315789, 17: 0.3684210526315789, 9: 0.42105263157894735, 15: 0.42105263157894735, 18: 0.42105263157894735, 3: 0.47368421052631576, 10: 0.47368421052631576, 12: 0.47368421052631576, 13: 0.47368421052631576, 16: 0.47368421052631576, 19: 0.47368421052631576, 8: 0.5263157894736842, 4: 0.5789473684210527}\n",
      "                           \n",
      "_______________Closeness Rank_____________________________\n",
      "{1: 0.5135135135135135, 6: 0.5588235294117647, 7: 0.5757575757575758, 11: 0.5757575757575758, 14: 0.5757575757575758, 2: 0.59375, 5: 0.59375, 0: 0.6129032258064516, 9: 0.6129032258064516, 17: 0.6129032258064516, 15: 0.6333333333333333, 18: 0.6333333333333333, 3: 0.6551724137931034, 10: 0.6551724137931034, 12: 0.6551724137931034, 13: 0.6551724137931034, 16: 0.6551724137931034, 19: 0.6551724137931034, 8: 0.6785714285714286, 4: 0.7037037037037037}\n",
      "                           \n",
      "_______________Page Rank_____________________________\n",
      "{1: 0.08931851925340188, 0: 0.1697998505165263, 11: 0.16989682506357492, 7: 0.17043350371206906, 6: 0.17641601517441413, 14: 0.18507153574556626, 2: 0.1852867563918859, 5: 0.192503619700419, 17: 0.19582020857395122, 18: 0.22610842612300855, 9: 0.22981399136924033, 15: 0.2382007095492152, 12: 0.2478851086976829, 13: 0.2522610589371633, 16: 0.2540612614196749, 19: 0.25657624704224746, 3: 0.26037823694377343, 10: 0.2615287590132112, 4: 0.29792603356090835, 8: 0.2984542952714921}\n",
      "                           \n",
      "{18: array([0.026]), 3: array([0.063]), 4: array([0.068]), 5: array([0.074]), 19: array([0.11]), 11: array([0.112]), 13: array([0.131]), 10: array([0.154]), 1: array([0.158]), 14: array([0.159]), 12: array([0.16]), 16: array([0.166]), 2: array([0.187]), 15: array([0.21]), 6: array([0.223]), 7: array([0.234]), 9: array([0.234]), 8: array([0.235]), 0: array([0.27]), 17: array([0.322])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nxG = nx.from_numpy_matrix(G) \n",
    "# G = nx.karate_club_graph()\n",
    "print(\"_______________Degree Centrality_____________________________\")  \n",
    "plt.figure(figsize =(15, 15))\n",
    "deg_centrality = nx.degree_centrality(nxG)\n",
    "sortedDict = sorted(deg_centrality.items(), key=lambda x:x[1])\n",
    "converted_dict = dict(sortedDict)\n",
    "print(converted_dict)\n",
    "print(\"                           \")\n",
    "print(\"_______________Closeness Rank_____________________________\")\n",
    "close_centrality = nx.closeness_centrality(nxG)\n",
    "sortedDict1 = sorted(close_centrality.items(), key=lambda x:x[1])\n",
    "converted_dict1 = dict(sortedDict1)\n",
    "print(converted_dict1)\n",
    "print(\"                           \")\n",
    "print(\"_______________Page Rank_____________________________\")\n",
    "pr = nx.eigenvector_centrality(nxG)\n",
    "sortedDict3 = sorted(pr.items(), key=lambda x:x[1])\n",
    "converted_dict3 = dict(sortedDict3)\n",
    "print(converted_dict3)\n",
    "\n",
    "print(\"                           \")\n",
    "\n",
    "def gap(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return abs(x)\n",
    "\n",
    "gap = gap(s,n)\n",
    "my_gap = {index: value for index, value in enumerate(gap)}\n",
    "sorting_gap = sorted(my_gap.items(), key=lambda x:x[1])\n",
    "sorted_gap = dict(sorting_gap)\n",
    "print(sorted_gap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.111 0.151 0.12  ... 0.106 0.135 0.098]\n",
      " [0.115 0.157 0.109 ... 0.112 0.138 0.104]\n",
      " [0.115 0.157 0.109 ... 0.112 0.138 0.105]\n",
      " ...\n",
      " [0.114 0.157 0.109 ... 0.111 0.138 0.104]\n",
      " [0.114 0.157 0.109 ... 0.111 0.138 0.104]\n",
      " [0.114 0.157 0.109 ... 0.111 0.138 0.104]]\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment = '11'\n",
    "pd.DataFrame(payoff_matrix).to_csv('Karate Payoff Matrix'+ str(Experiment)+'.csv')\n",
    "with open('Result'+str(Network)+'.'+str(Experiment)+'Pure.txt', \"a\") as f:\n",
    "#     print(Experiment_note, file=f)\n",
    "    print('Initial Condition -(agent, opinion, pol)', file=f)\n",
    "    print('Innate op'+str(s),file=f)\n",
    "    print('Adjacency matrix'+ str(G), file=f)\n",
    "    print('Max:'+ str(First_max), file=f)\n",
    "    print('Min' + str(First_min), file=f)\n",
    "\n",
    "    print(\"In the Last 100 Rounds\", file=f) \n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # MAX distribution of LAST 100 iteration \n",
    "    print('Max_distribution', file=f)  \n",
    "    max_l100_fre = max_history_last_100/100\n",
    "    print(max_l100_fre [np.nonzero(max_l100_fre)], file=f)\n",
    "    # print for small network\n",
    "    #print(max_history_last_100, file=f)\n",
    "    # # Print for Large Network\n",
    "    print(np.nonzero(max_l100_fre),file=f)\n",
    "\n",
    "    # MIN Strategy in the last 100 round\n",
    "    counter=collections.Counter(min_touched_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    # print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/100 #return only frequency of all min options in order\n",
    "#     print('Min_frequency', file=f)\n",
    "#     print(list(counter.keys()), file=f)\n",
    "    print('Min_distribution_last_100', file=f)\n",
    "    print(fla_min_fre, file=f)\n",
    "    counter_h=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_h, file=f)\n",
    "    \n",
    "    print('min_recent_'+str(memory)+'_touched', file=f)# then stop at Game 202\n",
    "    print(min_touched, file=f)\n",
    "    print('max_recent_'+str(memory)+'_touched', file=f)\n",
    "    print(max_touched, file=f)\n",
    "    \n",
    "    print('In Overall'+str(Game_rounds)+' Rounds', file=f)\n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # Max action Overall \n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    max_fre = max_history/Game_rounds\n",
    "#     print('Max_frequency', file=f)\n",
    "#     print(max_history, file=f)\n",
    "    print('Max_distribution', file=f)\n",
    "    print(max_fre [np.nonzero(max_fre)], file=f)\n",
    "    print(np.nonzero(max_fre),file=f)\n",
    "\n",
    "\n",
    "    # Min Strategy in the Overall    \n",
    "    counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "    fla_min_fre_all = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "    print('Min_dist_all', file=f)\n",
    "    print(fla_min_fre_all, file=f)\n",
    "    print('Min_distribution', file=f)\n",
    "    counter_a=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_a, file=f)\n",
    "#     print(payoff_matrix, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(6, 0.0): 358, (29, 1.0): 37, (29, 0.9999999999999999): 5, (13, 1): 1})\n",
      "fla_min_fre\n",
      "[0.002 0.893 0.092 0.012]\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(min_history) \n",
    "print(counter)\n",
    "fla_min_fre = np.array(list(counter.values()))/Game_rounds\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
