{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date 08/02/2024 Xilin Zhang\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "#%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nashpy as nash\n",
    "#=================================== Synthetic Network =============================================\n",
    "# name = \"Karate\"\n",
    "# filename = \"%s.ipynb\" % name\n",
    "# print(filename)\n",
    "# %run filename\n",
    "#%run sync_net4.ipynb\n",
    "#%run Karate.ipynb\n",
    "#%run Twitter.ipynb\n",
    "#%run Reddit.ipynb\n",
    "n=20\n",
    "%run Synthetic.ipynb\n",
    "%run NZS_MaxMin.ipynb # NZS_MaxMin and NZS_MinMax cannot be run at the same time. Clean the memory before running another one\n",
    "#%run NZS_MinMax.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_payoff_row(op1,v2, A, n, dynamic):\n",
    "    payoff_row = np.zeros(2*n)\n",
    "\n",
    "    for column in range(2*n):\n",
    "#         print(column)\n",
    "        v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "        max_opinion = column%2\n",
    "#         print(v1, max_opinion)\n",
    "        # update the maximizer's change to the opinion array that has been changed by the minimizer(op1)\n",
    "        op2 = copy.copy(op1)\n",
    "        op2[v1,0] = max_opinion\n",
    "        # calculate the polarization with both max and min's action\n",
    "        if dynamic == 1: #opinion dynamics\n",
    "            payoff_row[column] = obj_polarization(A,op2, n)\n",
    "        elif dynamic == 2:# no opinion dynamics\n",
    "            payoff_row[column] = obj_innate_polarization(op2, n) \n",
    "    \n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    \n",
    "    if dynamic == 1: #opinion dynamics\n",
    "        O_P = obj_polarization(A, s, n)\n",
    "    elif dynamic == 2:  # no opinion dynamics\n",
    "        obj_innate_polarization(s, n) \n",
    "        \n",
    "    payoff_row[j_1] = O_P\n",
    "    payoff_row[j_2] = O_P\n",
    "    \n",
    "    return payoff_row\n",
    "\n",
    "def make_payoff_matrix(s,n, A, dynamics): #opinion dynamics = 1, no opinion dynamics = 0\n",
    "    payoff_matrix = np.empty((0, 2*n), float)\n",
    "    C1 = list(range(n))    # for all agent \n",
    "    for v2 in C1:         \n",
    "            #print('Minimizer start from agent'+str(v2))\n",
    "            min_opi_option = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] #Min has two options to change agent v2's opinion\n",
    "            for min_op in min_opi_option:\n",
    "                     op = copy.copy(s)\n",
    "                     op[v2] = min_op\n",
    "                     payoff_row = make_payoff_row(op,v2, A, n, dynamics) \n",
    "#                      print(payoff_row)\n",
    "                     payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "    return payoff_matrix\n",
    "\n",
    "\n",
    "\n",
    "def find_NE(matrix): #using the Lemke-Howson algorithm\n",
    "    \n",
    "    ###################################### Zero-sum Game\n",
    "    # Take innate steady state polarization p0\n",
    "    p0 = matrix[0][0]\n",
    "    # Calculate the change in polarization\n",
    "    delta_polarization =  matrix - p0\n",
    "    # print(delta_polarization)\n",
    "    \n",
    "    # Define payoff matrices for both players\n",
    "    payoff_matrix_Max = matrix - p0  # maximizer\n",
    "    payoff_matrix_Min = p0- matrix # minimier \n",
    "    # print(payoff_matrix_Max[payoff_matrix_Max<0])\n",
    "    # print('-----------------')\n",
    "    # print(payoff_matrix_Min[payoff_matrix_Min<0])\n",
    "    ####################################### Non-zero-sum\n",
    "    #IF GAME == 'NZS':\n",
    "        ##Replace 0 with -10000\n",
    "    payoff_matrix_Max[payoff_matrix_Max == 0] = -10000\n",
    "    payoff_matrix_Min[payoff_matrix_Min == 0] = -10000\n",
    "        # print(payoff_matrix_2)\n",
    "    \n",
    "    # Creat the game\n",
    "    game = nash.Game(payoff_matrix_Min,payoff_matrix_Max)\n",
    "    # Find Nash Equilibrium using the Lemke-Howson algorithm\n",
    "    equilibrium = game.lemke_howson(initial_dropped_label=0)\n",
    "    \n",
    "    # Find non-zero indices for Min\n",
    "    non_zero_indices_1 = np.nonzero(equilibrium[0])[0]\n",
    "    # Find non-zero indices for Max\n",
    "    non_zero_indices_2 = np.nonzero(equilibrium[1])[0]\n",
    "    Min_agents = []\n",
    "    Max_agents = []\n",
    "    # Print the indices and corresponding probabilities for each player\n",
    "    #print(\"Min Strategy Indices and Probabilities:\")\n",
    "    for index in non_zero_indices_1:\n",
    "        #print(f\"Strategy {index}: Probability {equilibrium[0][index]}\")\n",
    "        Min_agent = index//11\n",
    "        Min_agents.append(Min_agent)\n",
    "    #print(\"Max Strategy Indices and Probabilities:\")\n",
    "    for index in non_zero_indices_2:\n",
    "        #print(f\"Strategy {index}: Probability {equilibrium[1][index]}\")\n",
    "        Max_agent = index//2\n",
    "        Max_agents.append(Max_agent)\n",
    "    \n",
    "    return(Min_agents, Max_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_Max(max_agents, G, k):\n",
    "   \n",
    "    (deg_cent, clo_cent, eigen_cent, MinPaths_sort, op_extre, cc) = metrics(G,0)\n",
    "    first_k_deg = list(deg_cent.keys())[:k] \n",
    "    #print(first_k_deg)\n",
    "    first_k_clo = list(clo_cent.keys())[:k]\n",
    "    first_k_eigen = list(eigen_cent.keys())[:k]\n",
    "   # first_k_path = list(MinPaths_sort.keys())[:k]\n",
    "    first_k_op = list(op_extre.keys())[-k:]  # select top k neutral opinion for maximizer\n",
    "    #first_k_op = list(op_extre.keys())[:k]  # select top k neutral opinion for maximizer\n",
    "    \n",
    "    #print(op_extre)[-k:]\n",
    "    first_k_cc = list(cc.keys())[:k]\n",
    "\n",
    "\n",
    "    deg_hit = 1 if any(element in max_agents for element in first_k_deg) else 0\n",
    "    clo_hit = 1 if any(element in max_agents for element in first_k_clo) else 0\n",
    "    eigen_hit = 1 if any(element in max_agents for element in first_k_eigen) else 0\n",
    "    op_hit = 1 if any(element in max_agents for element in first_k_op) else 0\n",
    "    cc_hit = 1 if any(element in max_agents for element in first_k_cc) else 0\n",
    "\n",
    "\n",
    "    return deg_hit,clo_hit, eigen_hit,op_hit, cc_hit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top K on sythetic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deg_hit_sums clo_hit_sums eigen_hit_sums\n",
      "[88, 96, 98, 99, 99, 99, 99, 99, 99, 0] [84, 96, 98, 98, 99, 99, 99, 99, 99, 0] [89, 97, 97, 98, 98, 99, 99, 99, 99, 0]\n",
      "K= 1\n",
      "Top 1 deg hit rate: 0.88\n",
      "Top 1 clo hit rate: 0.84\n",
      "Top 1 eigen hit rate: 0.89\n",
      "Top 1 op_extre hit rate: 0.04\n",
      "Top 1 cc hit rate: 0.55\n",
      "K= 2\n",
      "Top 2 deg hit rate: 0.96\n",
      "Top 2 clo hit rate: 0.96\n",
      "Top 2 eigen hit rate: 0.97\n",
      "Top 2 op_extre hit rate: 0.08\n",
      "Top 2 cc hit rate: 0.62\n",
      "K= 3\n",
      "Top 3 deg hit rate: 0.98\n",
      "Top 3 clo hit rate: 0.98\n",
      "Top 3 eigen hit rate: 0.97\n",
      "Top 3 op_extre hit rate: 0.15\n",
      "Top 3 cc hit rate: 0.62\n",
      "K= 4\n",
      "Top 4 deg hit rate: 0.99\n",
      "Top 4 clo hit rate: 0.98\n",
      "Top 4 eigen hit rate: 0.98\n",
      "Top 4 op_extre hit rate: 0.19\n",
      "Top 4 cc hit rate: 0.65\n",
      "K= 5\n",
      "Top 5 deg hit rate: 0.99\n",
      "Top 5 clo hit rate: 0.99\n",
      "Top 5 eigen hit rate: 0.98\n",
      "Top 5 op_extre hit rate: 0.24\n",
      "Top 5 cc hit rate: 0.65\n",
      "K= 6\n",
      "Top 6 deg hit rate: 0.99\n",
      "Top 6 clo hit rate: 0.99\n",
      "Top 6 eigen hit rate: 0.99\n",
      "Top 6 op_extre hit rate: 0.24\n",
      "Top 6 cc hit rate: 0.67\n",
      "K= 7\n",
      "Top 7 deg hit rate: 0.99\n",
      "Top 7 clo hit rate: 0.99\n",
      "Top 7 eigen hit rate: 0.99\n",
      "Top 7 op_extre hit rate: 0.27\n",
      "Top 7 cc hit rate: 0.67\n",
      "K= 8\n",
      "Top 8 deg hit rate: 0.99\n",
      "Top 8 clo hit rate: 0.99\n",
      "Top 8 eigen hit rate: 0.99\n",
      "Top 8 op_extre hit rate: 0.31\n",
      "Top 8 cc hit rate: 0.67\n",
      "K= 9\n",
      "Top 9 deg hit rate: 0.99\n",
      "Top 9 clo hit rate: 0.99\n",
      "Top 9 eigen hit rate: 0.99\n",
      "Top 9 op_extre hit rate: 0.36\n",
      "Top 9 cc hit rate: 0.68\n"
     ]
    }
   ],
   "source": [
    "#################################### Set Parameters ####################################3\n",
    "Game = 1     # 1-MaxMin, 2- MinMax, 3- NZS\n",
    "dynamics = 1   # 1- opinion dynamics, 2- no op dynamics\n",
    "K = 10  # number of Top nodes\n",
    "rounds = 100 # number of synthetic networks\n",
    "\n",
    "# Define variables for storing total hits for each k value\n",
    "deg_hit_sums = [0] * K  # to store degree hits for k=1 to k=4\n",
    "clo_hit_sums = [0] * K  # to store closeness hits for k=1 to k=4\n",
    "eigen_hit_sums = [0] * K  # to store eigenvector hits for k=1 to k=4\n",
    "op_extre_sums = [0] * K  # to store opinion extremity hits for k=1 to k=4\n",
    "cc_hit_sums = [0] * K  # to store clustering coefficient hits for k=1 to k=4\n",
    "\n",
    "for i in range(1, rounds):\n",
    "    #print('rounds', i)\n",
    "    s, G, A = make_sync_network(n)\n",
    "    payoff_matrix = make_payoff_matrix(s, n, A, dynamics)\n",
    "    if Game == 1:\n",
    "    ######################## MaxMin Game\n",
    "        (v1, v2, max_opinion, min_opinion, max_pol) = MaxMin_play(s, n, G)\n",
    "        Max_agents = [v1]\n",
    "        Min_agents = [v2]\n",
    "    elif Game == 2:\n",
    "    ######################## Minmax Game\n",
    "        (v1, v2, min_opinion, min_pol) = MinMax_play(s,n)\n",
    "        Max_agents = [v1]\n",
    "        Min_agents = [v2]\n",
    "    elif Game == 3:\n",
    "    ######################## Non-zero-sum game\n",
    "        (Min_agents, Max_agents) = find_NE(payoff_matrix)\n",
    "   # print(Min_agents, Max_agents)\n",
    "    for k in range(1, K):\n",
    "        #print(f'Top {k} hit rate')\n",
    "        deg_hit, clo_hit, eigen_hit, op_hit, cc_hit = top_k_Max(Min_agents, G, k)#top_k_Max(Max_agents, G, k)\n",
    "    \n",
    "        # Summing up hits for each k-1 index (since k starts from 1)\n",
    "        deg_hit_sums[k-1] += deg_hit\n",
    "        clo_hit_sums[k-1] += clo_hit\n",
    "        eigen_hit_sums[k-1] += eigen_hit\n",
    "        op_extre_sums[k-1] += op_hit\n",
    "        cc_hit_sums[k-1] += cc_hit\n",
    "print('deg_hit_sums', 'clo_hit_sums','eigen_hit_sums')\n",
    "print(deg_hit_sums, clo_hit_sums,eigen_hit_sums)\n",
    "\n",
    "            \n",
    "# Calculate and print averages outside the k loop after all rounds are completed\n",
    "for k in range(1,K):\n",
    "    print('K=',k)\n",
    "    print(f'Top {k} deg hit rate:', deg_hit_sums[k-1] / rounds)\n",
    "    print(f'Top {k} clo hit rate:', clo_hit_sums[k-1] / rounds)\n",
    "    print(f'Top {k} eigen hit rate:', eigen_hit_sums[k-1] / rounds)\n",
    "    print(f'Top {k} op_extre hit rate:', op_extre_sums[k-1] / rounds)\n",
    "    print(f'Top {k} cc hit rate:', cc_hit_sums[k-1] / rounds)\n",
    "\n",
    "\n",
    "# max_agents = [1,2,3]\n",
    "# first_k_deg = [3,4,5]\n",
    "# deg_hit = 1 if any(element in max_agents for element in first_k_deg) else 0\n",
    "# print(deg_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes: Lemke-Howson algorithm Test on one Real Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #=================================================== Test on Real Networks ========================================================\n",
    "# import pandas as pd\n",
    "# import pandas as pd\n",
    "# import nashpy as nash\n",
    "\n",
    "# ##################################### Import data\n",
    "# # Properly formatted file path for Windows\n",
    "# #file_path = r'C:\\Users\\xz47\\OneDrive\\Misinfo Paper\\One Node - Final\\12092023_PoGame\\12092023_PoGame\\Reddit Discrete Payoff Matrix.csv'\n",
    "# #r'C:\\Users\\xz47\\OneDrive\\Misinfo Paper\\One Node - Final\\12092023_PoGame\\12092023_PoGame\\Karate NoOpDyn Discrete Payoff Matrix1.csv'\n",
    "# #r'C:\\Users\\xz47\\OneDrive\\Misinfo Paper\\One Node - Final\\12092023_PoGame\\12092023_PoGame\\Karate NoOpDyn Discrete Payoff Matrix1.csv'\n",
    "\n",
    "# # # Import data\n",
    "# # df = pd.read_csv(file_path,header=None)\n",
    "# # matrix = df.to_numpy()\n",
    "# # print(matrix.shape)\n",
    "# ###################################### Zero-sum Game\n",
    "\n",
    "# # Take innate steady state polarization p0\n",
    "# p0 = matrix[0][0]\n",
    "# # Calculate the change in polarization\n",
    "# delta_polarization =  matrix - p0\n",
    "# # print(delta_polarization)\n",
    "\n",
    "# # Define payoff matrices for both players\n",
    "# payoff_matrix_Max = matrix - p0  # maximizer\n",
    "# payoff_matrix_Min = p0- matrix # minimier \n",
    "# print(payoff_matrix_1[payoff_matrix_1<0])\n",
    "# print('-----------------')\n",
    "# print(payoff_matrix_2[payoff_matrix_2<0])\n",
    "\n",
    "# ####################################### Non-zero-sum\n",
    "# #IF GAME == 'NZS':\n",
    "#     ##Replace 0 with -10000\n",
    "# payoff_matrix_1[payoff_matrix_1 == 0] = -10000\n",
    "# payoff_matrix_2[payoff_matrix_2 == 0] = -10000\n",
    "#     # print(payoff_matrix_2)\n",
    "#     # # Create the game\n",
    "\n",
    "# ###################################### Find Nash equilibrium through Lemke-Howson algorithm\n",
    "# # def lemke_howson( A: npt.NDArray, B: npt.NDArray, initial_dropped_label: int = 0, lexicographic: bool = True,)\n",
    "# # ) -> Tuple[npt.NDArray, npt.NDArray]:\n",
    "#     # ----------\n",
    "#     # Parameters\n",
    "#     # A : array\n",
    "#     #     The row player payoff matrix\n",
    "#     # B : array\n",
    "#     #     The column player payoff matrix\n",
    "#     # -------\n",
    "# game = nash.Game(payoff_matrix_Min,payoff_matrix_Max)\n",
    "# # Find Nash Equilibrium using the Lemke-Howson algorithm\n",
    "# equilibrium = game.lemke_howson(initial_dropped_label=0)\n",
    "# print(\"Nash Equilibrium:\", equilibrium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #============================================= Print NE result =========================================\n",
    "# # Identify significant strategies for maximizer\n",
    "# for i in range(0, len(equilibrium[0])):  # Step through each agent's actions\n",
    "#     agent_index = i // 11\n",
    "#     action_0_prob = equilibrium[0][i]\n",
    "#     chosen_action = i%11\n",
    "#     # action_1_prob = equilibrium[0][i+1]\n",
    "#     if action_0_prob>0:\n",
    "#         print(f\"Minimizer Agent {agent_index} chooses Action {chosen_action} with probability {action_0_prob}\")\n",
    "# print('=================================================')\n",
    "# # Identify significant strategies for minimizer\n",
    "# for i in range(0, len(equilibrium[1])):  # Step through each agent's actions\n",
    "#     #print(i)\n",
    "#     agent_index = i // 2\n",
    "#     action_probabilities = equilibrium[1][i]\n",
    "#     chosen_action = i%2\n",
    "#     if action_probabilities>0:\n",
    "#         print(f\"Maximizer Agent {agent_index} chooses Action {chosen_action} with probability {action_probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "125d6e56934ce2649cb3782cc815dc11821615614828859fcaee5b1c4840aa60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
